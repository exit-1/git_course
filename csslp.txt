Version 0.2
-----------

Once you have passed the exam for an ISC2 certification, you must be endorsed by another ISC2-certified professional. This requirement applies to all credentials, even if you already hold an ISC2 certification, as the requirements for each credential vary. You must also pay your first Annual Maintenance Fee (AMF) before the credential can be awarded.

An endorser can be anyone who:

Is an active ISC2 credential holder
Can attest to your assertions regarding professional experience and education (if applicable) and that you are in good standing within the cybersecurity industry
Notes:

If you do not know an ISC2-certified professional, you may request ISC2 to endorse your application.
Although you can start and save a draft application, you must pass the exam for the selected certification before you can submit your application for endorsement.
If you do not yet possess the education and/or experience required for the certification, you can request to be an Associate of ISC2, which requires only that you pass the credential exam.
Applying for a CISSP Concentration requires only that you already hold a CISSP credential, have passed the exam, and possess the required domain work experience.
Following your acceptance of your endorsement application, you will be required to pay your first AMF to become fully certified. If you already an ISC2 member or associate, this additional fee does not apply.


Steps to Endorsement
Get Started - Click New Certification Application.
Certification Type - Select the certification that you are requesting for endorsement.
Membership Type - Choose your ISC2 membership type based on experience.
Request Endorsement - Make sure to have your endorser's ISC2 Member Number.
Experience Waiver - If applicable, request an Experience Waiver.
Job History - Detail your experience.
Review Application - Review all information before submitting the Endorsement Application.


CSSLP - Certified Secure Software Lifecycle Professional
========================================================

Revisit:
- Controls to Prevent Common Web Application Vulnerabilities
- Production Data Reuse
- Verification and Validation Testing
- Vulnerability Management
* common backup methods for performing system backups include
 
DOMAIN 1: Secure Software Concepts
----------------------------------

- framework for security policies, procedures and security objectives throughout SDLC

- Confidentiality (vs. Disclosure)
  -- preserving authorized restrictions on information access and closure including the means (security controls) for it
  -- data at rest
  -- data in transit
  -- data in use (by application)

  -- Security Controls
    * encryption: limit data comprehension to authorized parties while deterring adversaries by creating a high work factor to overcome a protective measure
      - not a solution to all problems - other confidentiality methods (anonymization, tokenization, masking,...) may offer acceptable alternatives depending on the goals and objectives

  -- Access Controls
    * “least privilege” by allowing access only to authorized individuals, processes, or systems, and only on a need-to-know basis

    * anything that needs to be trusted has an identity, which can have identifiers such as
      - email addresses
      - IP addresses
      - public key

    * IAM: people, processes and systems used to manage access to resources by assuring
      - the identity of an entity is verified
      - granting the correct level of access based on the protected resource, which assures identity and other contextual information

    * federated IAM provides the policies, processes and mechanisms to manage identity and trusted access to systems across organizations
      - federated authentication = yhdistetty todennus
      - technology of federation
        -- much like that of Kerberos within an Active Directry domain, where a user logs on once to a domain controller, is ultimately granted an access token, and uses that token to gain access to systems for which the user has authorization
        -- the difference is that while Kerberos works well in a single domain, federated identities allow for the generation of tokens (authentication!) in one domain and the consumption of these tokens (authorization!) in another domain

      - in federated environment, there is an identity provider and a relying party
        -- identity provider holds the identities and generates a token for known users
        -- relying party is the service provider and consumes these tokens

      - identification
        -- identity providers increasingly adopt OpenID, Security Assertion Markup Language (SAML) and OAuth as standard protocols
        -- this includes big-name social media services. Microsoft’s AD is a prevalent example. Relevant standard protocols include SAML and WS-Federation

      - authentication
        -- process of establishing with adequate certainty the identity of an entity
        -- function of the identity provider
        -- done through factors such as passwords, key generators and biometrics
        -- MFA often advised for high-risk roles such as administrative functions

      - authorization
        -- process of granting access to resources
        -- based on identities, attributes of identities such as role, and contextual information such as location and time of day
        -- enforced near the relevant resource, at the policy enforcement point
        -- in a federated identity model, this is typically at the relying party

    * examples of access decisions questions include
      - can a device be allowed to receive an IP address on the local network?
      - can a web server communicate with a particular database server?
      - can a user access a certain application, a function within an application or data within an application?
      - can an application access data from another application?

      - access decisions can be enforced at various policy enforcement points with various technologies
      - individual policies are controlled at the policy decision point
      - communicating these policies can be done through standard protocols such as Extensible Access Control Markup Language (XACML)

    * federation standards
      - SAML (Security Assertion Markup Language)
        * XML-based framework for describing and exchanging security information between online business relationships
      - OAuth (Open Authorization Framework)
        * RFC 6749 defines OAuth to enable 3rd party application for obtaining limited access to an HTTP service, either
          -- on behalf of a resource owner by orchestrating an approval interaction between the resource owner and the HTTP service or
          -- by allowing the third-party application to obtain access on its own behalf
      - OpenID Connect
        * interoperable authentication protocol based on the OAuth, makes use of JSON/REST to send and receive messages with the intent of simplification and uncomplication

    * biometrics
      - for example, touch ID or face ID
      - starts by making baseline measurements of the required characteristics and storing them as part of the profile associated with that user’s identity in the system
      - these readings are then cryptographically hashed so they can then be placed onto a smart card or other physical security token

    * steganography
      - generally designed to conceal the presence of information
      - an example would be hiding a message inside an image by manipulating the RGB values of the pixels or the file header metadata
      - security practitioners understand steganography conceptually while acknowledging security delivered through obscurity is never adequate security


  -- confidentiality can be compromized in many ways
    * network sniffing
    * shoulder surffing
    * social engineering
    * hacking
    * masquerading
    * unprotected downloaded files
    * trojan
    * clear-text data

  -- safeguarding confidentiality
    * multiple layers of defence
    * non-technical methods
      - security awareness training
      - retention policies
      - limited physical access
      - policies for secure disposal of servers and media




- Integrity (vs. Alteration)
  -- preventing unauthorized (intentional or unintentional) modification of data and information since it was created, transmitted or stored
  -- modifications are NOT made by unauthorized subjects
  -- unauthorized modifications are NOT made by authorized subjects
  -- data is internally and externally consistent

  -- technical controls for integrity (to mitigate the risk of unauthorized alteration)

    * hashing works by using cryptographic hash functions to create a fixed-size digest from variable length message (input)
      - deterministic and always produce the same digest from the same input

    * digital signature
      - binds to identity
      - electronic signature
      - mathematical algorithm that validates the authenticity and integrity of emails, credit card transactions, and digital documents through a unique virtual fingerprint that identifies users and protects information
      - for example, the email content itself becomes part of the digital signature
      - significantly more secure than other forms of electronic signatures
      - binding a signature to an identity provides assurance of integrity

      - hash vs. digital signature
        -- hash verifies the message integrity only
          * if the message changes, the hash value changes
          * hash does NOT authenticate the sender
        -- digital signature verifies message integrity AND message authenticity
          * digital signature guarantees the a known source generated the message (non-repudiation)
          * how?: the message hash is encrypted by using the sender's private key (recipient decrypts the hash by using sender's public key)

    * source code version control
      - changes are identified and tracked through revision indicators using letter codes, numbers, and other symbols
      - changes are time stamped and the subjects who make those changes are tracked
      - revisions are compared, restored or merged
      - for example: CVS, Subversion, GIT, and others

  -- integrity compromises include
    * unauthorized access and actions taken by unauthorized individuals (e.g., hackers)
    * authorized individuals such as staff (whether with malicious intentions or unintentionally)

    * subject: requester of resources, object: resource being requested
    * mechanisms used to combat corruption or modification to data
      - access control mechanisms
      - intrusion detection and prevention
      - digital signatures
      - code signing

    * Trusted Computing Base (TCB): collective of all hw, sw, fw, processes and resources critical to security and allow for the information security policy to be enforced on a system
      - responsible for maintaining the confidentiality and integrity of data
      - if TCB integrity is compromised (i.e., if malware is running), it can be assumed that security policy can no longer be enforced
      - Trusted Platform Module (TPM), dedicated microcontroller, is an example of a cryptographic key that can be used to attest (todistaa) to the integrity of a boot process
        -- Microsoft BitLocker would be an example that takes advantage of the TPM chip

  -- safeguarding integrity
    * three basic principles relevant to establishing integrity controls:

      - need-to-know
        -- subjects should be granted access only to those files and programs that they absolutely need to perform their assigned job functions

      - separation of duties
        -- ensure that no single subject has control of a transaction from beginning to end; two or more subjects should be responsible for performing it

      - rotation of duties
        -- subjects in critical or financial roles should be alternated periodically so that nefarious (ilkeä) activities cannot be conducted across time without collusion
        -- includes other advantages such as succession planning (minimizing loss of knowledge after losing a key employee) and availability of backup personnel




- Availability (vs. Destruction)
  -- ensuring reliable and timely access to the data and computing resources
  -- hw, sw, network, data, utilities
  -- resource availability is defined based on business needs

  -- security controls for availability
    * system or software should be designed and implemented to recover from disruptions in a secure and quick manner to avoid negative impacts to productivity and business continuity
    * there are several concepts that can be used to achieve availability:
      - redundancy
      - failover
      - fault tolerance
      - RAID (Redundant Array of Independent Disks)
        -- data storage virtualization technology that combines multiple physical disk-drive components into a single logical unit for the purposes of data redundancy, performance improvement, or both
      - HA (failover) clusters
      - replication

  -- compromising availability
    * DOS/DDOS attacks
    * disasters based on geography are more common disruptors to availability (hurricanes, tornados, flooding)

    * contingency (satunnaisuus) planning
      - minimize the time that business capabilities, including software systems, remain unavailable

  -- safeguarding availability
    * sw services often represent mission-critical components within organizational infrastructure
      --> organizational business continuity and disaster recovery efforts must include considerations for mission-critical app recovery or reconstitution including systems, sw and data

  -- disaster recovery
    * organizational risk tolerance for interruption of mission-critical services and data loss will ultimately drive the disaster recovery strategy
    * effective disaster recovery is interdependent (toisistaan riippuvaiset) on internal policies and external mandates
      - Recovery Time Objective (RTO) and Recovery Point Objective (RPO) requirements would dictate disaster recovery requirements and strategies
        -- RTO: time frame within which an asset (product, service, network, etc.) must come back online if it goes down
        -- RPO: acceptable amount of data (measured by time) a company is willing to lose in case of an incident
    * when using CSP (Cloud Service Provider), common for an organization to have its production environment in one region and its disaster recovery (DR) in a different region

  -- contingency planning
    * could involve business resumption planning, alternative site processing, or simply disaster recovery planning
    * provides an alternative means of processing, thereby ensuring availability
    * physical, technical, and administrative controls (safeguards and countermeasures) are important aspects of security initiatives that address availability
    * risk associated with a single point of failure or compromise should be considered
      - many times these single points of failure are introduced by the architectural and design decisions that we make
      - elimination of failure points could involve a significant level of effort

  -- privacy
    * information security and data privacy are closely related
      - however, there are distinct differences that security practitioners should understand
      - think about a scenario where an organization’s layers of defense (access control, encryption, data retention,...) have successfully secured data, but have simultaneously violated privacy law by collecting user data without explicit consent

      - globally, numerous laws, regulations, and other legal requirements for organizations and entities to protect security and privacy of digital and other information assets
      - privacy laws and regulations require the implementation of measures to adequately protect personal and personally identifiable information (PII)
        -- includes protection from unauthorized access, modification, loss, amendment, or alteration
      - failure to protect PII potentially results in legal challenges, fines, and imposed actions (including restrictions around the processing and collection of personal information)
      - when these consequences are added to the possibility of nonlegal impacts such as reputational damage, loss of consumer and customer confidence, and competitive disadvantages, they exert significant pressure on entities to display compliance and due care




- Elements of security design
  -- accountability
    * ability to determine the actions and behaviors of a subject within a system and to identify that particular subject
    * related to the concept of nonrepudiation (kiistämättömyys), wherein a subject cannot successfully deny the performance of an action
    * auditing and logging contribute to accountability

  -- auditing
    * answers the question: “Who (subject) did what (action) when (timestamp) and where (object)?”
    * one-time or periodic event to evaluate security, or an ongoing activity that continuously examines the system/users
    * audit trails are sets of records that collectively provide documentary evidence of processing

  -- logging
    * to provide documentary evidence regarding the sequence of events, and as such, auditing may rely on it
    * enable a vast number of stakeholders to perform their job duties (administrators, developers, support staff, security, privacy, and compliance personnel)
    * broad range of subjects interested in logs underscores the need to identify these stakeholders and capture their requirements during the early stages of sdlc
      - frequently overlooked, much to the detriment of decisions made without it
    * logs in general may, and likely will, contain sensitive information captured intentionally or accidentally, which can result in serious issues if exposed

    * SIEM (Security Information and Event Management)
      - logs come from diverse sources and increase in size rapidly
      - reviewing the logs for the purpose of identifying anomalies (signs of compromise) without specialized solutions and technologies would be no easy task, and that is where SIEMs come into the picture
        -- use cases include security monitoring, incident investigation and response, and advanced threat detection among others

    * nonrepudiation
      - protects against an individual falsely denying having performed a particular action
        -- determine whether a given individual took a particular action such as creating information, sending a message, approving information, and receiving a message
      - ensures that actions taken (intentional or unintentional) cannot be denied
      - mechanisms relevant to this objective include digital signatures and blockchain technology among others

    * blockchain
      - enables global visibility of a shared transaction accounting ledger (tapahtumakirjanpito)
      - important business value can be delivered by its inherent capabilities
      - involves storing a time-stamped sequence of immutable (muuttumaton) records (blocks) in public databases connected as peers in a network (chains)
      - features of blockchain that have led to the expansion of the possibilities and the applications:
        -- protection against unauthorized changes, as new blocks may only be added to the end of the chain, and once added they can’t be removed
        -- redundancy and elimination of single point of failure, as independent and autonomous nodes store redundant copies of a blockchain
        -- transparency, as blockchains are publicly available for viewing and secure audit trails

    * digital signatures
      - creating a digital signature includes the following steps:
        1) using a one-way cryptographic hash function to create a digest from the input
        2) encrypting the digest (using an asymmetric algorithm) that was created from the previous step, using the private key of the signer

                hash algo                     encryption
        data  ------------>   hash value   ------------------->    digitally signed document
                                              private key


- Governance, Risk management, and Compliance (GRC)
  -- discipline method to correlate business objectives and IT infrastructure while controlling risks by addressing regulations
  -- has sw-centered considerations since facilitation often includes software tools and processes to centralize organizational activities
  -- each aspect is defined briefly below:
    * Governance establishes policies and procedures for achieving business goals
    * Risk management identifies and then reduces risk to an acceptable level
    * Compliance is ensuring that the organization identifies and satisfies any applicable laws and regulations

  -- example regulations:
    * US
      IoT Cybersecurity Improvement Act of 2020
      California Consumer Privacy Act (CCPA) of 2018
      Consumer Privacy Protection Act (CPPA) of 2017
      Federal Cybersecurity Laws of 2014
      Health Insurance Technology for Economic and Clinical Health Act (HITECH) of 2009
      Sarbanes-Oxley Act (SOX) of 2002
      Federal Information Security Management Act (FISMA) of 2002
      Children’s Online Privacy Protection Act (COPPA) of 1998
      National Information Infrastructure Protection Act of 1996
      Health Insurance Portability and Accountability Act (HIPAA) of 1996
      Payment Card Industry Data Security Standard (PCI DSS) of 1996
      Communications Assistance for Law Enforcement Act (CALEA) of 1994

    * non-US
      EU Cyber Solidarity Act proposed 2023
      EU Cyber Resilience Act (CRA) proposed 2022
      EU Cybersecurity Act of 2019
      EU General Data Protection Regulation (GDPR) of 2018
      India Information Technology (IT) Act of 2000

  -- laws apply within internet infrastructure and cloud
  -- in some extreme cases, laws may conflict between jurisdictions (toimivalta)
  -- although the Internet lacks physically defined boundaries, applications are still bound to geographical and jurisdictional boundaries in which they operate

  -- security practitioners need to safeguard the operational security of development environments while ensuring that sw developed is secure to an acceptable level
  -- security practitioners leverage sw tools to aid with these objectives, which may include GRC
  -- organizations must leverage automation and software tools to centralize and streamline processes




- Security Design Principles
  -- system and architecture level
  -- principles are abstracted away from platform and programming language specifics
  -- aiming to to establish adequate security
    * intentional trade-offs to reach a state of security that supports the business function of the system or sw

  -- secure systems typically exhibit the following characteristics (NIST):
    * enables delivery of system capability despite all manner of adversity
    * constrains function of the system to desired behaviors based on system required capabilities
    * constraints are enforced via predefined rulesets to ensure only authorized interactions and operations are allowed

  -- Economy of Mechanism
    * demonstrates the principle, “keep it simple, stupid”
    * the more complex the design of a system/software, the higher the likelihood that a given vulnerability may go unnoticed

    * principle of Build Security In (BSI):
      - one factor in evaluating a system's security is its complexity
      - if the design, implementation, or security mechanisms are highly complex, the likelihood of security vulns increases

    * password vaults
    * resource efficiency
      -- resources must be allocated and de-allocated properly
      -- weakness in sw development practices may lower the cyber resiliency of the entire hosting system, resulting in degradation of organizationally defined levels of CIA
        - examples: Out-of-bounds Write, Out-of-bounds Read, Integer Overflow or Wraparound, NULL Pointer Dereference

  -- Complete Mediation (täydellinen sovittelu)
    * assurance that every request by a subject for accessing an object (especially for security-critical objects) must be vetted (tarkastettu)
    * system that relies on checking the permissions of a subject to an object only once can potentially invite attackers to exploit that system
    * caching permissions can increase the performance of a system, but at the cost of allowing secured objects to be accessed

  -- Open Design
    * concept of open design dictates that the security of a system should not be dependent on secrecy of its design, its implementation or its components
    * example: publishing an encryption algorithm for review by experts can in fact result in the identification of weaknesses in the algorithm which in turn results in increased security
    * "security through obscurity" NOT
    * Kerckhoff's principle: as long as secrecy of the encryption key is maintained, the security of a cryptographic system should also be maintained, even if everything about the system is public knowledge

    * Modular Open Systems Approach (MOSA)
      - previously known as Open Systems Architecture or Open Systems Approach
      - technical and business strategy for designing adaptable systems including software
      - requires that major interface points within systems are modular and embrace widely supported standards
      - must support interoperability, scalability, and portability
      - key to building a system capable of information sharing across domains, with interchangeable hw and sw components

    * Open-Source Software (OSS)
      - not inherently more or less secure just because the source code is freely available
      - often the security of OSS depends on the community rigor (kurinalaisuus) and bandwidth to provide updates throughout the life cycle of the source code

    * Collaborative Design
      - collaborative effort to be successful, its culture must exist, and collaborative sw development is no different
      - from a security perspective, the ability to tap subjects from all over the world with different skills, experiences, knowledge bases and thoughts can be invaluable
        -- this has been evident from a number of well-known software products that are the result of crowdsourcing

  -- Least Common Mechanism
    * concept of least common mechanisms involves minimizing the number of protection mechanisms that are common to multiple subjects
      - example: the ability for the web server process to access a back-end db, for instance, should not also enable other applications on the system to access the back-end db
    * different mechanism (or instantiation of a mechanism) for each subject or class of subjects can provide flexibility of access control among various users and prevent potential security violations that would otherwise occur if only one mechanism was implemented

    * Compartmentalization
      - in the context of the least common mechanism principle, restriction of user roles for the purpose of compartmentalization or isolation of the code/function is recommended
      - execution of different functions based on user roles is recommended in contrast to having a single function used by all roles
    * Allow / Accept Lists
      - aka whitelisting, is the action of explicitly allowing access to a particular resource
      - one method to address access control for subjects that may need to access an object
      - a particular object is accessible while access to everything else is denied

  -- Psychological Acceptability
    * aims at maximizing the usage and adoption of the security functionality in sw by ensuring that it is easy to use and transparent to the user
    * password complexity
      - sw password-complexity policy should not be so difficult that users must write passwords down, thus defeating the purpose of increased security
      - CWE-655: Insufficient Psychological Acceptability states that sw has protection mechanism that's too difficult or inconvenient to use, encouraging nonmalicious users to disable or bypass the mechanism, whether by accident or on purpose
      - password entropy
        -- way to describe the complexity of a password mathematically
        -- determines how difficult guessing a password can be by calculating 50 % chance of guessing a password based on password length and possible characters
        -- LOG2(x) = y  <=>  2**y = x

        -- Number of Possible Combinations = S**L
          * S = size of the pool of unique possible characters
          * L = password length (number of places in the password)
        -- Entropy = log2(Number of Possible Combinations)

        -- example:
          * 4 characters consisting of: Letters of the same case
          * Length: 4
          * Possible Symbols: 26
          * Possible Combinations: 26**4 = 456,976
          * Bits of Entropy: log2(456976) ~ 18.8
          * Strength: Very Weak (way under 50)

      - passwordless authentication
        -- it's possible to provide comparable security with more convenience by authenticating using one or more factors that do not include passwords
        -- users can authenticate using access to applications such as email, devices such as phones, one-time pass codes, or even biometrics
        -- passwordless multifactor is possible by implementing combinations of something you have and something you are, for example

    * Completely Automated Public Turing test to tell Computers and Humans Apart (CAPTCHA)

    * Component Reuse
      - promoting reusability of existing components, so that no newer vulnerabilities are introduced, and the attack surface is not increased
      - sw design should consider leveraging existing components
      - one may also argue that centralizing functionality into business components that can be leveraged repeatedly is akin to putting all eggs in one basket
        --> however, proper defense-in-depth and layered security measures can mitigate this


Terms and Definitions
---------------------
Adequate Security - Security commensurate with the risk and the magnitude of harm resulting from the loss, misuse, or unauthorized access to or modification of information.
Availability - Ensuring timely and reliable access to and use of information by authorized users.
Cloud Service Providers - A service provider who offers customers storage or software solutions available via a public network, usually the internet.
Denial of Service (DoS)- The prevention of authorized access to resources or the delaying of time-critical operations.
Digital Signature - A cryptographic operation which, when implemented correctly, can provide assurance for data integrity, origin, and non-repudiation. This normally requires the use of a Digital Certificate.
Disaster Recovery (DR) - In information systems terms, the activities necessary to restore IT and communications services to an organization during and after an outage, disruption, or disturbance of any kind or scale.
Distributed Denial of Service (DDoS) - A type of DoS attack that uses many sources of attack traffic. A DoS attack uses a single source of attack traffic. Attackers often use botnets to carry out DDoS attacks.
Economy of Mechanism - Demonstrates the principle that overly complex approaches will not necessarily enhance security as opposed to relatively straightforward and simple approaches.
Failover - Switching to a redundant or standby computer server, system, hardware component, or network upon the failure or abnormal termination of the previously active application, server, system, hardware component, or network.
Fault Tolerance - The system remains working as expected even when some of its components are failing.
Hashing - A form of one-way encryption that uses a mathematical function to create a fixed length binary output from a variable length binary input.
High-Availability Clusters - Groups of computers that support server applications that can be reliably utilized with a minimum of downtime. They operate by using high-availability software to harness redundant computers in groups or clusters that provide continued service when system components fail.
Modulary Open Systems Approach (MOSA) - A technical and business strategy for designing adaptable systems including software. MOSA requires that major interface points within systems are modular and embrace widely supported standards.
Need-to-Know - Primarily associated with organizations that assign clearance levels to all users and classification levels to all assets; restricts users with the same clearance level from sharing information unless they are working on the same effort.
Nonrepudiation - Protects against an individual falsely denying having performed a particular action, including the capability to determine whether a given individual took a particular action such as creating information, sending a message, approving information, and receiving a message.
Open-Source Software - Software whose source code and other design information is made publicly available for inspection, testing, assessment, and use.
Password Entropy - A way to describe the complexity of a password mathematically. Password entropy determines how difficult guessing a password can be by calculating a 50-percent chance of guessing a password based on password length and possible characters.
Recovery Point Objective (RPO) - A measure of how much data the organization can lose before the organization is no longer viable.
Recovery Time Objective (RTO) - The target time set for recovering from any interruption.
Redundancy - Continuing the running of an organization even with the absence/failure of one important component.
Redundant Array of Independent Disks (RAID) - A data storage virtualization technology that combines multiple physical disk-drive components into a single logical unit for the purposes of data redundancy, performance improvement, or both.
Replication - The process of storing data in more than one site or node.
Rotation of Duties - The periodical rotation of employees in critical or financial roles to prevent nefarious activities from taking place across time without collusion.
Separation of Duties - The practice of ensuring that no organizational process can be completed by a single person; forces collusion as a means to reduce insider threats.
Software Development Life Cycle (SDLC) - A formal or informal methodology for designing, creating, and maintaining software (including code built in hardware). Source: NIST SP 800-128
Transport Layer Security - Protocols used to secure communications in a wide variety of online transactions, such as financial transactions (e.g., banking, trading stocks, e-commerce), healthcare transactions (e.g., viewing medical records or scheduling medical appointments), and social transactions (e.g., email or social networking). Source: NIST Special Publication 800-52
Work Factor - The amount of effort necessary to break a cryptographic system, usually measured in total elapsed time.
Zero Trust - A collection of concepts and ideas designed to minimize uncertainty in enforcing accurate, least privilege per-request access decisions in information systems and services in the face of a network viewed as compromised.





Quiz

Question 1    1 / 1 point
With what is the confidentiality security objective MOST directly concerned? (D1.1, L1.2)
A) Protection of information from unauthorized destruction
--> B) Protection of information from unauthorized disclosure
C) Protection of information from unauthorized modification
D) Protection of information from destruction, disclosure, and modification
Correct. The confidentiality security objective is about protection of information from unauthorized disclosure.


Question 2    1 / 1 point
Which of the following concepts can be used to achieve availability in software security? (D1.1, L1.2)
--> A) Redundancy
B) Data encryption
C) Intrusion detection
D) Code obfuscation
Correct. Redundancy is a concept used to ensure availability by continuing the operation of an organization even in the absence or failure of an important component.


Question 3    1 / 1 point
What mechanism(s) is BEST suited to mitigate risks to confidentiality for TCP/IP network-based software? (D1.1, L1.2)
A) Deployment of technologies to encrypt sensitive data at rest
--> B) Deployment of technologies to encrypt sensitive data in transit
C) Deployment of any single-sign-on (SSO) technology
D) Deployment of steganography
Correct. Sensitive data in transit across the network should be encrypted in transit.


Question 4    1 / 1 point
Software security practitioners seek to maintain the CIA of systems and software based on business needs. Which aspect of CIA is focused on guaranteeing that authorized subjects are granted uninterrupted access to objects in a timely fashion? (D1.1, L1.2)
A) Confidentiality
B) Consistency
C) Integrity
--> D) Availability
Correct. Security practitioners should know and apply CIA concepts with ease. Availability ensures systems and applications are available.


Question 5    1 / 1 point
Roger Innovation Center, commonly referred to as RIC, chooses to design and develop its own cryptographic algorithms without a substantial and compelling business or technical reason to do so. This may be interpreted as a violation of which of the following design principles? (D1.2, L1.6)
A) Least common mechanisms
B) Psychological acceptability
C) Least privilege
--> D) Open design
Correct. Open design refers to the principle of making the specifications, designs, and mechanisms of a system accessible and transparent to the public. It promotes scrutiny, peer review, and external auditing to enhance security and trust.


Question 6    0 / 1 point
Services R'Us is a supplier of mission-critical software services to numerous large organizations. One of its main selling points is the capability of the software services to be restored to normal business operations near real-time without disruption. Which concept below BEST describes the Services R'Us capability? (D1.1, L1.1)
A) Reliability
--> B) Recoverability
C) Resiliency
D) Exploitability
Reliability in this context is a reference to the software functioning as expected. Resiliency is a reference to software's ability to withstand misuse and attacks. Exploitability is not a relevant answer.


Question 7    1 / 1 point
Which of the following BEST describes a technology that supports non-repudiation? (D1.1, L1.3)
A) Hashing
B) Symmetric encryption algorithms
--> C) Digital signatures
D) Data loss prevention (DLP)
Correct. Digital signatures rely on properties of cryptographic hash functions and asymmetric encryption algorithms to provide for nonrepudiation.


Question 8    1 / 1 point
Which approach recommends executing different functions based on user roles for enhanced security? (D1.2, L1.7)
A) Least Common Mechanism
--> B) Compartmentalization
C) Collaborative Design
D) Allow/Accept lists
Correct. Compartmentalization is the approach of isolating code/functions based on user roles to enhance security.


Question 9    1 / 1 point
When a software system can withstand misuse, abuse, and attacks, it is said to be what? (D1.1, L1.1)
A) Reliable
B) Recoverable
C) Reputable
--> D) Resilient (kimmoisa)
Correct. Resiliency implies that software should be able to withstand misuse and attack.


Question 10   0 / 1 point
Which of the following statements is accurate? (D1.1, L1.2)
A) Good hashing function will produce the same digest every time given the same input
B) Good hashing function will produce a different digest every time given a different input
--> C) Both A and B
D) Neither A nor B
When two different inputs produce the same hash, it is called a hash collision.


Question 11   1 / 1 point
A software developer should not be able to check-in source code changes to the repository and deploy them to the production environment without human or machine-automated intervention.Which of the following terms best describes this scenario? (D1.1, L1.2)
--> A) Separation of Duties
B) Rotation of Duties
C) Need-to-know
D) Dual control
Correct. Separation of duties ensures that no single subject has control of a transaction from beginning to end.


Question 12   1 / 1 point
Which of the following does NOT describe a characteristic that will result in adequate security based on security design principles? (D1.2, L1.5)
A) Enable delivery of system capability despite all manner of adversity
--> B) Enable delivery of security capability despite all manner of business requirements
C) Constrains functions to desired behaviors based on system required capabilities
D) Constraints are enforced via predefined rulesets to ensure only authorized interactions and operations are allowed
Correct. The question is asking about adequate security, but the correct answer is best identified by understanding security only exists to support business missions. Businesses do not exist to support security.


Question 13   1 / 1 point
A banking website requires a PIN that is exactly eight digits in length and contains only numbers. What is the possible number of PINs for the banking website? (D1.2, L1.7)
A) 65
B) 75
--> C) 80
D) 96
Correct. Number of possible combinations = SL = 8 x 10 = 80 where S= [0,1,2,3,4,5,6,7,8,9].
10**8 = 100 000 000


Question 14   1 / 1 point
Nhung was quickly writing notes in a network security lecture regarding Zero Trust (ZT) implementations. Which of the following is MOST likely an erroneous note? (D1.2, L1.7)
A) ZT assumes all data and computing services are considered resources
B) ZT assumes all communication is secured regardless of network location
--> C) ZT assume internal users are more trustworthy than remote or external users
D) ZT assumes all resource authentication and authorization are dynamic and strictly enforced
Correct. All users are assumed to be untrustworthy. No preferences are given. Review NIST SP 800-207 Section 2.1 outlines the Tenets of Zero Trust.


Question 15   1 / 1 point
Which of the following statements best describes the principle of least privilege? (D1.2, L 1.5)
A) Each component should be allocated extensive privileges to ensure maximum functionality.
B) Elevated privileges should be granted to all components to simplify security analysis.
--> C) Sufficient privileges should be allocated to each component, but no more than necessary.
D) The principle of least privilege only applies to middleware servers.
Correct. The principle of least privilege suggests that each component should be granted only the necessary privileges to perform its designated functions.


Question 16   0 / 1 point
Which of the following statements best describes the concept of separation of duties? (D1.2, L1.5)
A) All critical functions should be assigned to a single subject for efficiency.
B) Subjects from different departments should perform similar functions and duties.
--> C) No subject should have enough privileges to misuse the system on their own.
D) The encryption process and key management process should be combined.
The concept of separation of duties states that no subject should have enough privileges to misuse the system on their own. This principle aims to mitigate the risk of fraud by dividing critical functions among multiple subjects.


Question 17   1 / 1 point
Multifactor authentication involves using two or more instances of different authentication factors. Which of the following is NOT considered a widely accepted factor for authentication? (D1.1, L1.1)
A) Something you have
--> B) Somewhere you are
C) Something you know
D) Something you are
Correct. Somewhere (i.e., a location) is not an accepted authentication factor.


Question 18   1 / 1 point
Which principle emphasizes the importance of simplicity in enhancing security? (D1.2, L1.7)
A) Complexity is key (CIK)
B) Simplicity is security (SIS)
--> C) Keep it simple, stupid (KISS)
D) Security through complexity (STC)
Correct. The principle "Keep it simple, stupid (KISS)" emphasizes the importance of simplicity in enhancing security.


Question 19   1 / 1 point
What is the potential drawback of caching permissions in a system? (D1.2, L1.5)
A) Increased performance and efficiency
--> B) Higher likelihood of security vulnerabilities
C) Enhanced security and access control
D) Improved scalability and resource management
Correct. Caching permissions in a system can increase performance, but it may also lead to a higher likelihood of security vulnerabilities.


Question 20   0 / 1 point
Which principle advocates that the security of a system should not rely on the secrecy of its design or components? (D1.2, L1.7)
A) Security through obscurity
B) Open-Source Software (OSS)
--> C) Kerckhoff's principle
D) Modular Open Systems Approach (MOSA)
Security through obscurity suggests that the security of a system relies on keeping its design or components secret, which contradicts the principle mentioned. Open-Source Software is not directly related to the principle described. Modular Open Systems Approach (MOSA) is a strategy for designing adaptable systems, but it is not specifically related to the principle discussed.




DOMAIN 2: Secure Software Life Cycle Management
-----------------------------------------------

Security in Predictive Methodologies

- security in sw development requires a different approach in predictive planning vs. adaptive planning
- security considerations within various SDLC methodologies will differ based on core business requirements, including:

  -- governance (hallinto) for the evaluation and management of risk (risk in architecture and design)
  -- identification of sw security requirements
  -- inclusion of security verification in sw reviews, assessments, analyses, tests, and evaluations
  -- leveraging sw configuration management/version control and change control processes
  -- implementation of adequate physical and logical security controls in various environments including development, testing, staging, and preproduction

- example of a predictive methodology: Waterfall
  -- sequential (noniterative) approach used in sw development processes
  -- phases:
    * requirements
      - define the required functionality and operation of the final package
    * design
      - overall system architecture followed by a detailed design of the system's modules and interfaces
    * implementation
      - program units are developed according to the detailed design, often functionality tested as a unit before integration into the complete system
    * testing
      - one of the most important phases of a system's development
      - takes the form of testing the functionality of each component and then testing the complete integrated system
    * deployment & maintenance
      - final steps in the process but are often two of the hardest
      - badly designed and implemented system can be very difficult to maintain

  -- was used to successfully develop sw projects, but it is not best suited for rapid development where requirement changes occur frequently
  -- has largely been replaced with Agile development methodologies (still embraced within some organizations due to contractual constraints or organizational culture)
  -- straightforward methodology with well-defined project phases to integrate security considerations (security is considered at project initiation)
    * internal projects: might be as simple as including cybersecurity professionals as stakeholders
    * external projects: clear and testable security requirements should be incorporated into contractual language
      --> further, security considerations can also be integrated into each gate or phase of the sw project beyond the requirements phase

  -- NIST (National Institute of Standards and Technology) provides guidance (SPs - Special Publications) on the management of sw security in SDLC
    * NIST SP 800-160 Volume 1 (Engineering Trustworthy Secure Systems)
    * NIST SP 800-160 Volume 2 (Developing Cyber-Resilient Systems)
      - provides security considerations that can inform cyber considerations for the Design phase of Waterfall within an organization

    * NIST SP 800-160 Volume 2 (Table D-6: Strategic Cyber Resiliency Design Principles)
      - defines five strategic cyber-resilient design principles and maps them to design principles for other disciplines
      - security practitioner could tailor the guidance from D-6 for the organization’s sw project and include a mechanism to test for compliance at the end of the design phase

      - one principle from D-6 is “Focus on Common Critical Assets”
        -- suppose the sw project includes an API that is considered critical
        -- security practitioner may help the org develop design guidance for the API that includes fail-safe communication redundancy and layered defense mechanisms, which bolster the cyber resiliency of the sw project
        -- in another phase of Waterfall such as Testing, the organization could leverage best practices for sw assurance from NIST SP 800-218 (Secure SW Development Framework (SSDF)) to describe the gated criteria to move to the Deployment phase

- Microsoft Security Development Lifecycle (SDL)
  -- 12 processes that follow the product life cycle from early stages (e.g., training) to advanced stages (e.g., incident response)
  -- introduces security and privacy considerations in every phase of the development process

  -- to reduce the number of security-related defects and the severity of the impact of any residual defects, SDL highlights the need for the integration of tasks and checkpoints in the development process

  -- MS SDL Practices can be summarized as:
    1) Providing training
    2) Defining security requirements
    3) Defining metrics and compliance reporting
    4) Performing threat modeling
    5) Establishing design requirements
    6) Defining and using cryptography standards
    7) Managing the security risk of using third-party
    8) Using approved tools
    9) Performing static analysis security testing (SAST)
   10) Performing dynamic analysis security testing (DAST)
   11) Performing penetration testing
   12) Establishing a standard incident response process


- The Seven Touchpoints of Secure Software
  -- simply a set of security-related best practices
  -- can be integrated into an existing SDLC process to provide a way of adding security content and considerations to development artifacts already being produced
  -- prioritized touchpoints in order of greatest (code review) to least (security operations) impact in improving software security:

    touchpoint                       phase
    ------------------------------------------
    1) Code review                   Code
    2) Risk analysis                 Test, Requirements & use cases, Architecture & design
    3) Penetration testing           Test, Field feedback
    4) Risk-based security tests     Test plans
    5) Abuse cases                   Requirements & use cases
    6) Security requirements         Requirements & use cases
    7) Security operations           Field feedback


- Security in Adaptive/Iterative Methodologies
  -- in contrast to Waterfall, an adaptive sw development approach relies on short iterations
  -- project is deconstructed into small components that may be started and completed during short development iterations
  -- incremental nature of adaptive planning and execution provides the ability to remain flexible as project requirements and scope evolve throughout the SDLC

  -- addressing security in fast-paced, dynamic sw environments that are typically characterized by unfamiliar territories, uncertain outcomes, and short time-boxed iterations (e.g., one or two weeks) may present challenges to incorporating security activities into the SDLC
    * this is especially true when an organization or security practitioner is transitioning from a predictive environment to adaptive
    * however, implementing good security within adaptive SDLC is achievable

- Agile methods
  -- key characteristics:
    * minimizing upfront planning
    * quickly generating working code
    * making design decisions and adjusting code as the system is being implemented and tested

  -- requirements for one sw feature may be drafted based on the outcomes of a recently completed iteration rather than creating all project requirements at a sw project initiation
  -- typically, each Agile iteration is referred to as a sprint

  -- examples: SCRUM, XP (extreme programming)

  -- SCRUM
    * involves a small team, usually with a team leader called a Scrum Master
    * team’s main purpose is to clear away obstacles, also called impediments or blockers, so that a task can be completed quickly

    * team
      - 5-10 people
      - cross-functional
      - can be shared
      - bigger projects use multiple scrum teams working in parallel

    * product backlog
      - requirements list created by Product Owner
      - ranked by importance
      - product Owner can change items between Sprints
      - release plan is built in

    * sprint backlog
      - team selects, starting from top of list
      - team then breaks down into tasks
      - 4-6 hours of work per person per day
      - commitment is fixed for the duration of the Sprint
      - no items are added until next Sprint

    * stakeholders review
      - at least: functionality that can be demonstrated
      - ideally: shippable code

      - retrospective (takautuva)
        -- identify areas of improvement
        -- define action items for next sprint

  -- XP
    * lightweight methodology best suited for developing software when the requirements are vague or change frequently
    * relies on one or more roles with associated responsibilities assigned to each member of the team, including programmer, customer, tester, coach roles, etc...

    * practices
      - pair programming
        -- relies on two programmers at one machine for all production code
        -- one programmer attends to coding and the other evaluates and improves the code

      - refactoring
        -- entails (aiheuttaa) simplifying the program when implementing the feature

      - collective ownership
        -- places the responsibility for the whole of the system on the team, as opposed to individuals
        -- any team member can change code anywhere, at any time


- Security in Agile Implementations
  -- requires tight integration of security discipline into organizational Agile development processes
    --> ensures that security is considered in every sprint, whether tasking consists of building new software or updating existing functionality

  -- when a sprint ends, any tasks marked done are considered complete shippable code
  -- if we want security, quality, or safety considered part of done code, those processes must fit within the context of a sprint

  -- to fit within sprints, costly and time-consuming security activities, such as code reviews, static analysis, and penetration testing, need to become less expensive, tightly scoped, and often automated so that they can be performed earlier and more frequently

  -- secure SCRUM
    * encourages identification of security concerns during initial backlog planning, later refinements, and sprint planning
    * user stories related to security concerns are clearly marked to ensure that security concerns will be addressed in the sprint that addresses the user story
    * risk analysis of each user story is performed to quantify potential losses resulting from sw compromises in monetary terms
    * permits 3rd-party security experts to be involved in
      - planning (for training of the Scrum team and identifying security concerns)
      - during sprints (to provide turnkey solutions to security issues)

    * scrum masters
      - object to the inclusion of outsiders on a scrum team, insisting that security must be addressed by the scrum team itself as a part of normal unit testing, continuous integration, continuous delivery, and the SDLC’s other compact, continuous feedback loops
      - insist that any security tools must be understandable by developers and must support how they already work and think


  -- Microsoft SDL/agile
    * decomposes the set of security practices from Microsoft SDL into smaller steps that can be levied by Agile development teams within the context of a sprint iteration


- DevOps + Security = DevSecOps
  -- often described as a logical extension of Agile, pioneered by lean-thinking organizations seeking solutions for rapid and frequent delivery of sw
  -- has been adopted for a variety of reasons, most notably because businesses need to build, test, and release sw faster and more reliably to stay competitive

  -- while Agile implements shorter development iterations, DevOps has the goal of shortening the SDLC while improving the reliability of incremental source changes such as feature enhancements, bug fixes, and updates

  -- tech aspects of DevOps are primarily about CI/CD practices, relying on the automation of much of the routine work of transforming code changes into verified production sw
  -- several tools and tech have been introduced for this purpose, but successful implementation of DevOps requires special attention to people and processes in addition to technology

  -- Development, Security, and Operations (DevSecOps)
    * everything that DevOps is but adds an additional focus on software assurance
    * bridges the gap between security and DevOps

    * security should ultimately remain focused on minimizing enterprise risk, and at the same time allow development to deliver more secure code at the rate dictated by business needs
    * traditional resource-intensive and heavyweight security activities throughout the SDLC must also change from being prescribed by nature to adaptive
    * security must be injected into the existing development workflow in DevOps life cycle


- Security Operations vs Secure Software
  -- secure practitioners need to understand the difference between
    * security operations for software development teams and
    * producing security-hardened software and
    * CIA considerations for both

  -- operational security
    * development teams producing software using the DevSecOps CI/CD pipeline
    * source code repository and IDE used should ensure that the sw source is available during the sprint
      - only accessible by development (availability)
      - when being stored in the repository (confidential)
      - and no developer can deploy production changes without leveraging the quality and security checks built into the pipeline (integrity)

    --> this is an oversimplified example that illustrates Security Operations within the SDLC
    --> note that all the mentioned operational security does not directly relate to the end sw product being more secure


  -- software hardening
    * sw assurance processes to make organizational sw secure and resilient
    * focuses on producing sw capable of satisfying the organizations defined levels of CIA for the end sw product
    * these efforts might include static analysis
    * might also include dynamic analysis

    * CNSSI 4009-2015: we must ensure that
      - the process for developing sw is secure from an operational perspective
      - delivering sw that functions as intended
      - sw is free of vulnerabilities intentionally or unintentionally designed or inserted as part of the sw throughout SDLC



- Secure Configuration
  -- organizations relying on technology and maintain an IT infrastructure need to establish, document, and maintain security configuration standards for all their critical sw
    * may be the sole responsibility of the organization or of the cloud service provider, or a shared responsibility

  -- it is necessary to begin with a baseline
    * defined in NIST Special Publication 800-128 (Guide for Security-Focused Configuration Management of Information Systems) as:
      - “set of specifications for a system, or Configuration Item within a system, that has been formally reviewed and agreed on at a given point in time, and which can be changed only through change control procedures. The baseline configuration is used as a basis for future builds, releases, and/or changes.”

  -- security hardening and locking down a system is a delicate activity that requires a balance between functionality and security



- Inventory of Hardware and Software Assets
  -- to ensure that the required adjustments to the system configuration do not adversely affect the security of the system or interfere with the organization's operation of the system, a well-defined configuration management process that integrates information security is needed

  -- to address sw weaknesses, organizations need to establish a known baseline configuration that includes hw and sw
  -- IT assets must be inventoried
  -- changes to configuration baselines must be controlled
  -- typically, organizations maintain a physical asset management system that can identify the location of a computer
  -- asset management systems generally contain more information about assets including operating system versions, patching, and sw installations
  -- goal is to provide an enterprise picture of what, where, and how assets are being used



- Benchmark

  -- CIS (Center for Internet Security)
    * CIS benchmarks are best practices for the secure configuration of a target system
    * two different configuration profiles
      - level 1
        -- base recommendation that can be implemented promptly and is designed to not have an extensive performance impact
        -- the intent is to lower the attack surface of your organization while keeping machines usable and not hindering business functionality
      - level 2
        -- defense in depth
        -- intended for environments where security is paramount (tärkeintä)
        -- recommendations can have an adverse effect (haittavaikutuksia) on an organization if not implemented appropriately or without due care

    * testing should be conducted to determine potential impacts before implementing production changes


  -- STIG (Security Technical Implementation Guide)
    * US DoD specific
    * DoD releases STIG reports periodically for general consumption, providing guidance
    * three category levels (each category indicates the severity of the risk of failing to address a particular sw weakness)
      - category I
        -- includes any vuln that WILL result in the loss of CIA and may result in loss of life, facility, or mission
        -- essentially a mission failure
      - category II
        -- includes any vulnerability that MAY result in the loss of CIA
        -- may also result in personal injury or equipment damage
        -- essentially mission degradation
      - category III
        -- includes any vulnerability that degrades security measures protecting CIA


  -- SCAP (Security Content Automation Protocol)
    * widely adopted within the security community
    * originated as part of an initiative from the U.S. government for automation and standardization of vulnerability management
    * widespread support through major vendors in the space of vulnerability assessment/management
    * way of automating STIG checking (it would not be feasible to check all STIG guidance without the help of automation)


  -- SACM (Security Automation and Continuous Managment)
    * working group by Internet Engineering Task Force (IETF)
    * attempts to mature standards for security automation and monitoring
    * seeks to advance SCAP specifications into international standards and by ameliorating (parantaen) security gaps


  -- SWID (Software Identification)
    * SWID XML tags are another promising development within sw security standards
    * some organizations leverage it to record unique information about sw applications and metadata, including name and version
    * could aid next-generation IT configuration management sw in providing consistent granular change and configuration management information


  -- CC (Common Criteria)
    * allows for the comparability of results between independent evaluations
    * useful as a guide for the development, evaluation, and procurement of IT products with security functionality


  -- FIPS (Federal Information Processing Standards)
    * set of standards developed by the U.S. government for nonmilitary government agencies, government contractors and vendors who work with federal information systems and sw
    * establishes requirements to ensure computer security and interoperability


  -- ISO/IEC 15288 “Systems and software engineering — System life cycle processes”
    * describes the processes and lifecycle stages for system and application engineering
    * describes fourteen technical processes (involving integration, verification, validation, disposal)
      - Business or mission analysis process (clause 6.4.1)
      - Stakeholder needs and requirements definition process (clause 6.4.2)
      - System requirements definition process (clause 6.4.3)
      - Architecture definition process (clause 6.4.4)
      - Design definition process (clause 6.4.5)
      - System analysis process (clause 6.4.6)
      - Implementation process (clause 6.4.7)
      - Integration process (clause 6.4.8)
      - Verification process (clause 6.4.9)
      - Transition process (clause 6.4.10)
      - Validation process (clause 6.4.11)
      - Operation process (clause 6.4.12)
      - Maintenance process (clause 6.4.13)
      - Disposal process (clause 6.4.14)


    * designed to accommodate the development of any type of system or sw whether the effort will result in mass-produced systems, modular components, or one-off developments
    * flexible in that it supports stand-alone and highly integrated systems



- Software Weakness and Vulnerability Standards
  -- OVAL (Open Vulnerability and Assessment Language) by MITRE
    * defines system configuration information, machine state, and results reporting formats
    * specifies test procedures using a chronological checklist

  -- defining STIG guidance programmatically in SCAP using OVAL is only part of the equation
    * reporting formats are not enough
    * sw issues must also be well defined
      - MITRE CVE program provides a list of vulns containing an identification number, description, and public references to a known issue in production sw application
      - CVEs are contained in NVD and referenced by SCAP
      - each CVE includes a CVSS score that represents a severity score on a scale from 0 to 10
      - CVSS scores may be used to prioritize which CVEs should be addressed first by an organization

  -- NIST defines a vulnerability as a “weakness in an information system, system security procedures, internal controls, or implementation that could be exploited or triggered by a threat source.”
    * this implies that a CVE is a disclosed exploitable weakness in a particular production sw version such as Mozilla Firefox 104 or Adobe Reader 23
    * in contrast there is also the standard CWE that describes a taxonomy for identifying the common sources of sw flaws (e.g., buffer overflows, failure to check input data) related to known poor coding practices
    * like CVSS providing severity scores for CVE, CWSS quantifies the severity of CWE


- Security Focused Configuration Management
  -- secure configuration management process consists of four steps

    1) planning
      - involves the development of policy and procedures to incorporate secure configuration management into existing information technology and security programs and then disseminating the policy throughout the organization
      - the policy addresses many different areas, such as
        -- implementation of secure CM plans
        -- integration into existing security program plans
        -- configuration control boards (CCBs)
        -- configuration change control processes
        -- tools and technology
        -- use of common secure configurations and baseline configurations, monitoring, and metrics for compliance with established secure CM policy and procedures


    2) identifying & implementing configurations
      - secure baseline configuration for the information system is developed, reviewed, approved, and implemented
      - maintain the baseline under the configuration management control
      - baseline configurations = documented, formally reviewed and agreed-upon sets of specifications for information systems or configuration items within those systems
        -- serve as a basis for future builds, releases, and/or changes to information systems
        -- may address
          * configuration settings
          * sw loads
          * patch levels
          * how the information system is physically or logically arranged
          * how various security controls are implemented
          * documentation

    3) controlling configuration changes
      - emphasis is put on the management of change to maintain the secure, approved baseline of the information system
      - ensures that changes are formally identified, proposed, reviewed, analyzed for security impact, tested, and approved prior to implementation
      - as part of the configuration change control effort, organizations can employ a variety of access restrictions for change including
        -- access controls
        -- process automation
        -- abstract layers
        -- change windows
        -- verification and audit activities to limit unauthorized/undocumented changes to the information system

    4) monitoring
      - mechanism within secure CM to validate that the information system is adhering to organizational policies, procedures, and the approved secure baseline configuration
      - identifies
        -- undiscovered/undocumented system components
        -- misconfigurations
        -- vulnerabilities
        -- unauthorized changes which can expose organizations to increased risk


- Software Security Maturity Models: SAMM, DSOMM, BSIMM
  -- framework and maturity models can assist organizations of any size with their formulation and implementation of strategy for sw security
  -- CLASP (Lightweight Application Security Process) was well received in the early 2000's, but is now obsolete

  -- SAMM (Software Assurance Maturgity Model) by OWASP
    * open framework for implementing srategy for sw security
    * may be used to meet several objectives
      - valuating an organization’s existing sw security practices
      - building a balanced sw security program in well-defined iterations
      - demonstrating concrete improvements to a security assurance program
      - defining and measuring security-related activities within an organization

    * designed to be flexible so that it may be applied to any style of development
    * built upon the principles of
      - striking a balance between short-term gains and long-term goals through specification of the sw security program in small iterations
      - providing flexibility in the sw security framework to allow tailoring of choices based on organizations’ risk tolerance and other factors
      - taking simple, well-defined and measurable steps in building and assessing an assurance program

    * core business functions (governance, construction, verification, operations) provide the foundation of the model and security practices

      - governance
        -- concerned with managing the overall sw development activities
        1) policy & compliance
          * setting up a security, compliance, and audit control framework throughout an organization to achieve increased assurance in software under construction and in operation
        2) strategy & metrics
          * overall strategic direction of the software assurance program and instrumentation of processes and activities to collect metrics about an organization’s security posture
        3) education & guidance
          * increasing security knowledge amongst personnel in software development through training and guidance on security topics relevant to individual job functions

      - construction
        -- concerned with product mgmt, requirements gathering, high-level architecture specification, detailed design, and implementation
        4) security requirements
          * promoting the inclusion of security-related requirements during the software development process to specify correct functionality from inception
        5) threat assessment
          * accurately identifying and characterizing potential attacks upon an organization’s software to better understand the risks and facilitate risk management
        6) secure architecture
          * bolstering the design process with activities to promote secure-by-default designs and control over technologies and frameworks upon which software is built

      - verification
        -- concerned with quality assurance work such as testing, review, and evaluation
        7) implemention review
          * assessment of an organization’s source code to aid vulnerability discovery and related mitigation activities as well as establish a baseline for secure coding expectations
        8) design review
          * inspection of the artifacts created from the design process to ensure provision of adequate security mechanisms, and adherence to an organization’s expectations for security
        9) security testing
          * testing the organization’s software in its runtime environment to discover vulnerabilities and to establish a minimum standard for software releases

      - operations
        -- concerned with product deployment and the normal operations of sw in production
        10) environment hardening
          * implementing controls for the operating environment surrounding an organization’s software to bolster the security posture of applications that have been deployed
        11) issue management
          * establishing consistent processes for managing internal and external vulnerability reports to limit exposure and gather data to enhance the security assurance program
        12) operational enablement
          * identifying and capturing security-relevant information needed by an operator to properly configure, deploy, and run an organization’s software

    * each of the 12 security practices has three defined maturity levels and an implicit starting point at zero
    * the details for each level differ between the practices, but they generally represent:
      - 0: Implicit starting point representing unfulfilled security practices
      - 1: Initial understanding and ad hoc provision of security practice
      - 2: Increased efficiency and/or effectiveness of the security practice
      - 3: Comprehensive mastery of the security practice at scale

  -- DSOMM (DevSecOps Maturity Model) by OWASP
    * DevSecOps was defined as the sum of DevOps and security
    * organizations can leverage DevSecOps to mature software security capabilities while utilizing SAMM
    * DSOMM is a recent OWASP project to mature sw development operations and ensure operational sw security is sufficiently matured

    * often the security and safety practices utilized within production environments are present in the sw development pipeline prior to integration and release
    * DSOMM describes security measures that should be implemented and aids in prioritizing
      - phases, and subphases of DevSecOps as dimensions and subdimensions respectively
      - for each dimension DSOMM describes development pipeline maturity for five prescribed levels of maturation:
        -- level 1: basic understanding of security practices
        -- level 2: adoption of basic security practices
        -- level 3: high adoption of security practices
        -- level 4: very high adoption of security practices
        -- level 5: advanced deployment of security practices at scale

  -- BSIMM (Building Security in Maturity Model)
    * framework, introduced based on the research and data gathered from the Synopsys SIG (formerly Cigital), a consortium of organizations that practice sw security
    * consists of practices across four domains, and 119 activities to assess initiatives
    * member organizations come from many different business sectors, including financial services, healthcare, technology, and independent sw vendors, among others
    * four domains of the BSIMM SSF (Software Security Framework) are

      - governance
        -- strategy & metrics
        -- compliance & policy
        -- training

      - intelligence
        -- attack models
        -- security features & design (SFD)
        -- standards & requirements

      - SSDL Touchpoints
        -- architecture analysis
        -- code review
        -- security testing

      - deployment
        -- pen testing
        -- sw environment
        -- configuration management & vuln management

- CMMC (Cybersecurity Maturity Model Certification)
  -- program developed by the U.S. DoD
  -- designed to enforce protection and sensitive unclassified information that is shared by the Department with its contractors and subcontractors
  -- focuses on the protection of federal contract information and other sensitive information not suitable for public release
  -- if development of sw applications includes the use of sensitive government information it is likely CMMC will apply

  -- CMMC 2.0 adopts tiered compliance model where contractors must implement security safeguards at progressively advancing levels based on the information processed
  -- levels of compliance are:

    * level 1 (Foundational)
      - 15  practices, annual self-assesement & affirmation
    * level 2 (Advanced)
      - 110 practices (based on NIST SP-800-171), triennial (every 3rd year) 3rd party assessments for national security information, self-assessments for select programs
    * level 3 (Expert)
      - 110+ practices (based on 171 and 172), triennial government-led assessments



- Governance & strategy
  -- governance is described by SAMM (and BSIMM) as being centered on the processes and activities related to how an organization manages overall sw development activities
    * includes concerns that affect cross-functional groups involved in development, as well as business processes that are established at the organization level
  -- governance involves:
    * strategy and metrics
    * policy and compliance
    * education and guidance

  -- governance includes those practices that help organize, manage, and measure a sw security initiative
  -- staff development is also a central governance practice
  -- practices under governance are described as strategy and metrics, compliance and policy, and training
  -- these practices encompass (kattavat):
    * planning
    * assigning roles and responsibilities
    * identifying sw security goals
    * determining budgets
    * identifying metrics and gates
    * identifying controls for compliance regimens
    * developing contractual controls such as service-level agreements (SLAs)
    * setting organizational sw security policy
    * auditing against that policy
    * training for critical roles in software security



- Functional Milestones and Security Milestones
  -- in sw development, they mark important events in project tasks and help the project stakeholders stay informed about the project status
  -- for the same fundamental reason that functional milestones are established, security milestones should also be established

  -- when describing milestones, "description", "project event", and "goal" are usually among the attributes that are captured
    * example:
      - milestone name: “preliminary design review”
      - project event: “the approval of the preliminary design by the stakeholders"  <----- GOAL

    * milestone goal (project event) could be used as the criterion applied to determine whether the milestone has been reached



- Security Strategy
  -- first part of addressing sw security considerations is to figure out where the organization presently stands
  -- you must answer two questions:
    * what is the level of security maturity for the end application (the product being developed)?
    * what is the level of security maturity for the organization's development efforts (how the product is being developed)?

  -- maturity models can provide a standardized way to describe the present state and future state of organizational sw security

  -- once the present state of the organization is understood, the next effort is to get business buy-in for the desired level of maturity
    * this is not determined based on the security practitioner's feelings or desires
    * the desired state is based on reducing organizational risk to support the business
    * the sw security practitioner must work with management to achieve this understanding

    * delta between present and future state of sw security can become a path bridged by milestones and checkpoints describing the major changes needed to achieve sw security maturation
    * milestone planning can assist with organizational budgeting considerations and checkpoints can provide a means to validate and verify positive security maturation progress



- Security Documentation
  -- one security practitioner might perform a task completely different from one another, given the same business goal and no direction
  -- successful security documentation provides a flexible approach using guidelines while ensuring consistent results by leveraging procedures
  -- policies ensure that security team members and the business at large are uniform in approach to implementing and maintaining sw security throughout the life cycle

  -- formal System Security Plan (SSP)
    * provides an overview of the security requirements for an information system
    * describes the security controls that are either already in place or are planned to meet those requirements

  -- security documentation for a sw system must accurately capture/reflect security-relevant details on activities throughout various phases of the life cycle
    * where and how these aspects and details are documented will be determined by different factors such as the prescribed formality in the adopted methodology

  -- NIST SP 800-18 (Guide for Developing Security Plans for Federal Information Systems)
    * “the objective of system security planning is to improve protection of information system resources
    * all federal systems have some level of sensitivity and require protection as part of good management practice
    * the protection of a system must be documented in a system security plan”

  -- according to NIST SP 800-64 (Security Considerations in the System Development Life Cycle)
    * primary security documentation is the SSP
    * other documentation supporting it may include:
      - Configuration Management (CM) plan
      - Business Continuity plan (BCP) including a Business Impact Analysis (BIA)
      - Continuity of Operations (COOP) plan
      - Continuous Monitoring plan
      - Security Awareness, Training, and Education plan
      - Incident Response (IR) and Disaster Recovery (DR) plan
      - Privacy Impact Assessment (PIA)


- System Development Documentation
  -- sw security documentation should be created and updated throughout the SDLC
  -- security-relevant artifacts that are created in one phase of the life cycle may be needed and used in subsequent phases

    * requirements phase
      - sw security doc could include functional and nonfunctional security requirements and misuse/abuse cases, or any subject-object matrix used to generate the misuse/abuse cases
      - misuse/abuse cases will be discussed in detail in future domains

    * design phase
      - during this phase, sw security documentation could include the documentation of the threat model
      - additional documentation is produced, including the most common usage scenarios, various user roles, dependencies, assumptions, and threat scenarios
      - this documentation can be useful for those who perform code reviews, and for those who create security test cases, among other possible uses in subsequent phases of the life cycle

    * implementation, testing and subsequent life cycle phases
      - during these phases, similar documentation should be expected
      - simplest way to relate to these documentation needs is to consider the security activities in each phase
      - some examples of security-relevant activities related to the implementation, testing, deployment, operations, and maintenance of sw that may require documentation include:
        -- attack surface analysis
        -- static analysis of application code
        -- manual security code reviews
        -- penetration test
        -- secure distribution and deployment
        -- secure configuration
        -- continuous monitoring


- Software Security Documentation
  -- organizations create policies, procedures, and guidelines while leveraging standards to inform business operations
  -- security-centric documentation, including sw security, follows the same conventions
  -- sw security policies describe the objectives and constraints for organizational sw security
    * policies are high level and provide the “what” and “why” (for example: activities to ensure new code is relatively free from defects)
    * procedures deal step-by-step with the “how” (for example: steps to execute static and dynamic analysis)
  -- sw security policies are normally technology independent

  -- policies will inform the standards to inherit, as well as which procedures must be created to ensure the organization’s operational and development security
  -- not every aspect of an organization can or should be described step-by-step
    * security guidelines can help bridge the gap between strategic policy goals and the direct tasking offered by procedures

                             guidelines
  -- strategy ---> policies ------------> procedures



- Security Metrics
  -- metrics are measurement information that must be clearly defined before collection begins
  -- two common categories of general security metrics include

    * key performance indicator (KPI)
      - used by organizations to measure progress towards an organizational goal or objective
      - if assuming security is part of organizationally defined goals then security metrics would be included in organizational KPIs
      - focus on business goal performance

    * key risk indicator (KRI)
      - metrics that focus on monitoring risk
      - predefined threshold is established and once that threshold is reached, relevant parties are alerted
      - thresholds are defined largely by organizational risk tolerance
      - aid in effectively reporting risk management results to stakeholders so to inform organizational actions
      - focus on measuring risk management efforts

  -- characteristics of good metrics include
    * consistent: there should be no significant deviation between each measurement
    * contextually specific: the metrics should be expressed in more than one unit of measurement
    * inexpensive: the metrics should be generated automatically instead of manually
    * objective: the findings should be indicative of the actual state of security in sw or system, and not the opinion of the person conducting the measurements or the stakeholder
    * quantitative: the metrics should be precise and expressed in terms of cardinal numbers or as a percentage


- General Software Development Metrics
  -- DevSecOps methodology includes considerations for metrics related to both the sw development processes and the quality and security of the sw itself
  -- metrics are important for situational awareness as processes and products evolve

  -- some possible operations-focused metrics that can benefit both the business and sw stakeholders:
    * deployment frequency
    * lead time (läpimenoaika) from development to production
    * ratio of failed deployments
    * average amount of time to complete a feature
    * average amount of time to fix sw defect based on priority

  -- sw assurance metrics practitioners seek to understand and describe the security flaws or technical debt of the sw product
    * as defined by NIST, sw assurance is the level of confidence that sw is free from vulnerabilities, either intentionally designed into sw or accidentally inserted at any time during its life cycle, and that the sw functions as intended

    * SAMATE (Software Assurance Metrics and Tool Evaluation) project
      - seeks to standardize sw security metrics reporting for various categories of sw security assessment tools
      - for example, there are many vulnerability scanning tools that report the severities or criticalities of defects differently
        -- without standardization of tool output, aggregating and comparing vulnerability scanning tool results is a challenging problem


- Cyclomatic Complexity
  -- complexity almost always increases risk for projects and processes, and sw development is no exception
  -- cyclomatic complexity directly describes
    * the density of a code section (e.g., function block)
    * quantitatively the number of possible linearly independent execution paths
      - "How many independent execution paths does a function contain?"

  -- code section that contains no branching logic would have a cyclomatic complexity of 1
  -- if the code section has two paths for execution, then the cyclomatic complexity is 2
  -- guide for understanding the cyclomatic complexity consideration for a code section:
    * 1-10: Simple (low risk)
    * 1-20: Not Simple (moderate risk)
    * 2-50: Complex (high risk)
    * > 50: Untestable (very high risk)


- Severity Levels
  -- CVSS:

         2.0                         3.0

  severity   base score       severity   base score
  --------------------------------------------------
                              NONE       0.0
  LOW        0.0-3.9          LOW        0.1-3.9
  MEDIUM     4.0-6.9          MEDIUM     4.0-6.9
  HIGH       7.0-10.0         HIGH       7.0-8.9
                              CRITICAL   9.0-10.0


 -- CWSS:
   * describes poor coding practices (before production) within a range of 0 to 100


- Measuring Software Security Through Attack Surface Evaluation
  -- attack surface evaluation will be covered in detail in other domains
  -- one method of measuring sw security is a technique called attack surface evaluation
    * helps to understand and manage application security risks as an application is designed, implemented, and changed

  -- the larger the attack surface, the less secure the application
  -- as an app evolves over time and new features are added to it, its attack surface will increase accordingly
    * relative attack surface quotient (RASQ) may be used to measure the attack surface of an application and track changes to the attack surface over time
    * this method makes it possible to calculate an overall attack surface score for the system and measure this score as changes are made to the system and to how it is deployed



- End-of-Life Policies
  -- generic guidance
    * EOL period for a sw version begins when the developer of that sw sends an EOL announcement
    * EOL announcements should be made approximately six months prior to the EOL date and contain an official timeline describing discontinuation plans, including an end-of-support date
    * if a direct replacement product is offered, the developer may shorten the EOL period
    * support contracts should not extend past the EOL date of sw versions

  -- sw specific guidance
    * executable code, help files, etc., should be deinstalled and disposed of
    * secure deinstallation techniques and tools should be employed
    * secure deletion of all executable, support, data, and documentation files associated with the sw should take place
      - may include application-specific data stores, temp files, setup files, configuration files, metadata files, read-me files, and user documentation
    * deletion of all directory structures associated with the sw should occur
    * secure archiving of source code and documentation should take place
    * documenting, date/time stamping, digital signing, and the secure storing of accurate records of the decommissioning process is essential


- Credential Removal, Configuration Removal and License Cancellation
  -- removal of credentials, configuration files, and sensitive user data must be considered
  -- possible 3rd-party component licenses must also be identified and canceled if no longer needed

  -- most effective approach is to have a series of best practices that can be followed, including the following
    * ensure
      -- that there is an effective policy for sw licensing, configuration, and credential management
      -- such a policy should address what sw is authorized, approved, and supported
      -- applies to proprietary, open source, or cloud-based software, regardless of whether it is commercial, freeware, or shareware
      -- this policy should also articulate when obsolete sw is no longer supported
      -- communicate the policies about license violation and penalties and educate the users

    * develop
      -- and maintain an inventory of all authorized sw through manual, automated, or mixed processes
        * identify seldom-used licenses and any obsolete sw used by personnel
        * develop a blacklist of sw that should be prevented from installation (e.g., unauthorized sw), and enforce it through processes and appropriate technologies

    * retire
      -- old applications and decrease IT complexities
        * reduce the risk of sw that is no longer supported and for which security patches may not be developed

    * collect
      -- records of all sw licenses in a central repository
        * include commercial, open-source, shareware, freeware, and public domain sw
        * use the repository to manage the purchases and support information
        * run reports and match licenses to installed sw


- Assessing Risks and Applying Destructive Techniques
  -- for storage devices containing magnetic media, a single overwrite pass with a fixed pattern, such as binary zeros, typically hinders recovery of data, even if state-of-the-art laboratory techniques are applied to attempt a retrieval
    * major drawback of relying solely upon the native read/write interface for performing the overwrite procedure is that areas not currently mapped to active logical block addressing (LBA) addresses are not dealt with
    * dedicated sanitize commands support addressing these areas more effectively
      - use of such commands results in a trade-off because, although they should more thoroughly address all areas of the media, using these commands also requires trust and assurance from the vendor that the commands have been implemented as expected

  -- relying on overwrite techniques on magnetic media may expose data to increased risk of unintentional disclosure, becaue media types have evolved (e.g., flash memory-based devices)
    * although the host interface (Advanced Technology Attachment [ATA], Small Computer System Interface [SCSI]) may be the same or similar across devices with varying underlying media types, it is critical that the sanitization techniques be carefully matched to the media


  -- destructive techniques for some media types may become more difficult or even impossible to apply in the future
    * traditional techniques, such as degaussing magnetic media, become more complicated as the media evolves, because some emerging variations of magnetic recording technologies incorporate media with higher coercivity (magnetic force)
    * as a result, existing degaussers may not have sufficient force to effectively degauss such media
    * it's also becoming more challenging as the necessary particle size for commonly applied grinding techniques goes down proportionally to any increases in flash memory storage density
      - flash memory chips already present challenges as they occasionally damage grinders due to the hardness of the component materials, and this problem will get worse as grinders attempt to grind the chips into even smaller pieces

  -- cryptographic erase (CE) is an emerging sanitization technique that can be used in some situations when data is encrypted as it is stored on media
    * media sanitization is performed by sanitizing the cryptographic keys used to encrypt the data, as opposed to sanitizing the storage locations on media containing the encrypted data itself
    * CE techniques are typically capable of sanitizing media quickly and could support partial sanitization, a technique where a subset of storage media is affected
    * sometimes referred to as selective sanitization, partial sanitization has potential applications in cloud computing and mobile devices

    * operational use of CE today presents some challenges
      - it may be difficult to verify that CE has effectively sanitized media
      - if verification cannot be performed, organizations should use alternative sanitization methods that can be verified, or they should use CE in combination with a verifiable sanitization technique

  -- for additional guidance on sanitization, NIST SP 800-88 (Guidelines for Media Sanitization) offers guidance for the disposal of data that can be mapped to sw security considerations



- Organizational Security Reporting
  -- there are two overarching (kattava) ways to separate organizational reporting on security
    * software security reporting through organizationally defined development processes
    * operational security reporting, which deals more with various levels of reporting within an organizational structure

  -- there are several ways to define an organizational structure for reporting and other purposes
    * as an example, NIST SP 800-39 (Managing Information Security Risk) describes a multitiered security reporting structure of three tiers:
      - organizational
      - business process
      - information systems

    * the organizational tier provides the strategy and context for the entire organization, which includes lower-level reports

                            strategic reporting
                          |                    /\
                          |     organization    |
                          |                     |
                          |   business process  |
                        1.|                     | 2.
                          |  information system |
                          |                     |
                         \ /                    |
                          `                     |
                            tactical reporting

    1. Information gets more granular and quantitative. Higher levels dictate lower-level reporting requirements
    2. Information gets more qualitative as reporting information is aggregated. Lower-level reporting feeds higher levels.

    organization
      - this level of the organization includes stakeholders such as the C-suite and the board of directors, as reporting must provide information that can be actioned at a high level
      - it would be unusual to present a 42-page report listing all high and moderate CVEs for currently deployed software in the enterprise
      - instead, a leader from security might present the total quantity for high and moderate CVEs, and three potential approaches for addressing them that include costs

    business process
      - might be synonymous within some organizations with a department (e.g., IT department, security operations center)
      - reporting is likely consolidated (ydistetty, koottu) from each system, sw, or network appliance into dashboards and other visual methods that allow for situational awareness within the mission or operation of the organization

    information system
      - involves information systems: end points
      - reporting is detailed and quantitative, with system logs, network logs, configuration baselines, and CVEs for particular sw installed on hw within the organization



- Software Development Security Reporting
  -- SDLC provides many opportunities for reporting the status of activities related to the maturation or enhancement of an organization’s source code
  -- reporting mechanisms throughout the software life cycle:

    * plan
      - opportunities and risks can be captured and recorded within the risk register
      - if using Agile then the backlog could provide context for reporting, for example: do user stories have acceptance criteria that include security considerations?

    * code/build/test
      - some IDEs can provide developers trustworthiness of 3rd-party libraries or linter suggestions based on language style guides
      - often before source code can be committed to a repository it must pass a gated check-in where unit, integration, and/or static analysis tests review the source code for defects
      - if the source code is found deficient (puutteellinen), it is often shelved (i.e., shelveset) instead of committed into the code base as a changeset
      - there is often reporting on the CWEs or style issues and emails sent about failing to commit the change

    * release/deploy/operate/monitor
      - once the source is branched into an official release there will likely be a final statement regarding the sw risks and mitigations
      - organization or authorizing official will use a document such as a system authorization package (SAP), to decide whether the sw system is authorized to operate
      - once sw is deployed and operating, it is monitored
      - monitoring and reporting mechanisms are established at the various tiers of the organization such as in the example given on organizational security reporting:
        -- at the information system level, business level, and strategic tier


- Continuous Improvement
  -- feedback
    * CI leads to higher levels of maturity
    * both BSIMM and SAMM define maturity levels for respective security practices
    * SAMM defines three maturity levels as objectives
    * each level within a security practice is characterized by a successively more sophisticated objective defined by specific activities, with more stringent success metrics than the previous level
    * additionally, each security practice can be improved independently, though related activities can lead to optimizations

  -- retrospective (takautuva/taaksepäin katsova)
    * CI in Agile dev envs hinges on (riippuu) creating the opportunity for teams to regularly and frequently reflect on what was done well, and to identify what can be improved upon
    * in fact, incorporating security in this type of environment relies upon these retrospectives that can help teams uncover opportunities for continuous and incremental improvements

    * frequency of running these retrospectives is up to the team; they could be scheduled at the end of each sprint, or less frequently as the organization sees fit
    * regardless of the frequency, for these retrospectives to be effective, a key requirement is to embrace a positive spirit and open mindset to encourage team members to step forward and share what they believe can result in improvements for the team

  -- lessons learned
    * major lesson learned in sw development is that the quality of the processes used to build sw is usually reflected in the quality of the sw product itself
    * failure to make improvements to processes despite the attempt could be due to various reasons, including:
      - over-emphasis on assessments and insufficient commitments to making improvements
      - starting quickly and not planning properly for the process improvement
      - poorly managing the implementation of the improvements


- Importance of Feedback Loops
  -- there is a similarity between a) security reporting information flowing from one tier of an organization to another and b) feedback loops integrated into processes and systems
  -- feedback loops are integrally important to ensure stability within development processes

  -- continuing the thread on gated check-in:
    * suppose Jeramie was a developer at your organization, and he attempted to commit source code that contained several critical security defects
    * as a result, the build and test portion of the pipeline would hopefully reject his changes
    * this would be good for the overall status of the branch of sw he was attempting to advance with a commit
    * however, Jeramie needs to know his changes were not committed -> he needs feedback
    * perhaps, the team needs to assist with remediation; in that case, they would require notification also

  -- feedback loops help ensure corrective actions are taken before a process can progress
    * in dynamic and automatic systems, output from the system action is directly input into the system to inform the next action
    * feedback can be positive or negative
      - positive: accelerate or advance a process
      - negative: reduce processing to preserve a stable state, much like the gated check-in slowed Jeramie’s ability to commit code with defects but with the result being more secure sw
    * in the most general sense, feedback loops also provide the opportunity for improving processes in the future


- Risk Management
  -- process of identifying risk, assessing risk and steps to reducing the risk to acceptable level

  -- risk is a function of the likelihood of a given threat source exercising a particular potential vulnerability and the resulting impact of that adverse event on the organization
    * to determine the likelihood of future adverse event, threats to an IT system must be analyzed in conjunction with the potential vulns and the controls in place for the IT system
    * impact refers to the magnitude of harm that could be caused by a threat’s exercise of a vulnerability
    * level of impact is governed by the potential mission impacts and in turn produces a relative value for the IT assets and resources affected

    * addressing security risk

      - mitigate
        -- controls (safeguards and countermeasures) can be implemented to reduce or mitigate risk
        -- given time and resource availability, this is the best course of action when addressing risk
        -- ISO/IEC 27005:2018 rebranding:
          ** modification: course of action that implements controls that are technical, environmental, or cultural

      - accept
        -- when the cost of implementing controls outweighs the risk itself, risk can be accepted if the acceptance is properly documented and formally performed
        -- acceptance with an exception to policy may be necessary when the sw has unfixable vulns due to some legacy components in sw that are no longer supported
        -- risk acceptance must be performed only after the residual risk (jäännösriski) is known and there are contingency plans (valmiussuunnitelma) in place to address the risk as soon as possible
        -- ISO/IEC 27005:2018 rebranding:
          ** retention (säilyttäminen): retaining the risk without further action

      - transfer
        -- when the cost of implementing controls outweighs the risk itself, risk can also be transferred by buying insurance, but this is not common
        -- another risk transference mechanism that is prevalent in the world of sw is the use of disclaimers (“as is” clauses) and end-user licensing agreements (EULAs), which are instruments that can be used to transfer the risk to the end user
        -- ISO/IEC 27005:2018 rebranding:
          ** sharing: risk is shared with another party (e.g., contractual, subcontractual, insurance).

      - avoid
        -- risk can be avoided by discontinuing the use of the vulnerable sw and/or replacing or upgrading it with a more secure version
        -- it is important to note that avoiding risk is not the same as allowing risk to be ignored (left unhandled) - the latter is never a good practice
        -- ISO/IEC 27005:2018 rebranding:
          ** avoidance: activity or condition that precipitates the risk is avoided


  -- steps of risk management methodology:
    1) risk assessment
      * determine extent of potential threat, vulnerabilities and risk associated with IT systems
      * analysis can be qualitative or quantitative or both
      * output: helps identify proper controls reducing or eliminating risk during risk mitigation process

    2) risk mitigation
      * involves prioritize, evaluate, implement risk reducing controls as recommended by risk assessment process
      * must be fully integrated in SDLC

  -- risk monitoring enables
    * verifying compliance, determining effectiveness of risk reponse measures, identifying risk-impacting changes
    * maintaining awareness of the risk being incurred (aiheutua)
    * highlighting need to revisit other steps in the risk management process
    * initiating process improvement activities



  -- Technical Risk vs. Business Risk
    * technical risk
      - pertains (liittyä/kuulua jhkn) to information and technology limits or constraints associated with preventing an organization from achieving its technical missions and goals
      - includes poor hw design, improper implementation of sw components, and limits in system resources

    * business risk
      - associated with preventing an organization from achieving its business missions and goals
      - can pertain to the infrastructure needed to support the technical team and communicate progress, issues, and needs to sponsors and stakeholders
      - include not being able to fill leadership/staff positions, inadequate requirements derivation, poor organizational communication, conflicts in budget, and schedule constraints


  -- Regulations and Compliance
    * regulatory compliance is defined as adhering to the applicable legal, regulatory, contractual, strategic, and policy requirements
    * as the number of regulations with which organizations need to be compliant increases, organizations are increasingly adopting the use of consolidated and harmonized sets of compliance controls (e.g., COBIT, NIST)
      - this approach is used to ensure that all necessary governance requirements can be met without unnecessary duplication of effort and activities
      - these sets of compliance controls help ensure compliance with standards such as ISO/IEC 27002, PCI DSS, GLBA, FISMA, and HIPAA, among others

    * managing regulatory requirements can be a resource-intensive task for many organizations
    * regulatory changes globally magnify the challenges of remaining compliant
    * good example would be compliance changes that were introduced when the General Data Protection Regulation (GDPR) took effect in 2018
    * another example would be New York State Department of Financial Services Cybersecurity Requirements for Financial Services, which also took effect in 2018

  -- Legal
    * working knowledge of legal concepts related to information technology and information security is required to fully understand the nontechnical threats and risks inherent with insecure sw

    * intellectual property
      - IP rights allow creators or owners of patents, trademarks, and copyrighted works to benefit from their own work or investment in a creation
      - like other property rights, IP rights can be sold, transferred, or inherited

    * World Intellectual Property Organization (WIPO) administers the Patent Cooperation Treaty (PCT), which provides for the filing of a single international patent application that has the same effect as national applications filed in the designated countries

    * copyright covers literary works (such as novels, poems, and plays), films, music, artistic works (e.g., drawings, paintings, photographs, and sculptures) and architectural design
    * trademark is a distinctive sign that identifies certain goods or services produced or provided by an individual or a company
      -> ensures that the owners of marks have the exclusive right to use them to identify goods or services, or to authorize others to use them in return for payment



  -- Standards and Guidelines
    * standard is a commonly recognized and agreed-upon model for conforming to an established or accepted measurement or value
      - when organizations adopt standards, they are usually mandatory for the entire organization

    * guidelines tend to be recommendations and are usually optional, however, can provide the necessary information from which to develop standards

    * examples of commonly adopted standards and guidelines are the following:

      - PCI DSS (Payment Card Industry Data Security Standard)
        -- standard for organizations that handle credit card holder information to reduce credit card fraud, and it is mandated and administered by the PCI Security Standards Council

      - OWASP (Open Web Application Security Project):
        -- online community that creates methodologies, documentation, tools and technologies for web application security

      - SAFECode (Software Assurance Forum for Excellence in Code):
        -- provides guidelines and suggestions that assist businesses with sw product purchase decisions

      - ISO 27000 series:
        -- developed by the International Standards Organization
        -- provides an information security framework that can be applied to all types and sizes of organizations
        -- two most used components are
          * ISO 27001, which defines the requirements for the program
          * ISO 27002, which defines the operational steps necessary in an information security program

      - Special Publication 800 series:
        -- NIST has created a collection of information security standards and best practices documentation known as the Special Publication 800 series
        -- primarily for U.S. government agency use, could be applied in any industry to build an information security program


  -- Architectural Risk Assessment
    * not every security problem stems from insecure coding practices
    * many sw defects that result in security issues are due to flaws in a system’s architecture and design
    * described by US-CERT, ARA process entails identification of flaws in sw architecture and determination of risks to information assets due to those flaws

    * through the process of ARA:
      - flaws are found that expose information assets to risk
      - risks are prioritized based on their impact to the business
      - mitigations for those risks are developed and implemented
      - sw is reassessed to determine the efficacy of the mitigations

    * studies vuls and threats that may be malicious or non-malicious in nature
    * whether the vulns are exploited intentionally (malicious) or unintentionally (non-malicious), the net result is that CI and/or A of the organization’s assets may be harmed
    * risk management and risk transfer instruments deal with unmitigated vulnerabilities

    * strength of conducting risk assessment at the architectural level is the ability to see the relationships and impacts at a system level
      - this means assessing vulns not just at a component or function level but also at interaction points
      - use case models help to illustrate the relationships among system components
      - ARA should factor these relationships into the vulns analysis and consider vulns that may emerge from these combinations


  -- ARA Activities

    * known vuln analysis
      - there are many known vulns documented throughout sw security literature
      - they range from the obvious (failure to authenticate) to the subtle (symmetric key management)
      - static code checkers, runtime code checkers, profiling tools, pen testing tools, stress test tools, and app scanning tools can find some security bugs in code, but they do not address architectural problems
      - when performing a known vuln analysis, consider the architecture as it has been described in the artifacts that were reviewed for asset identification

    * ambiguity (moniselitteisyys/hämäryys) analysis
      - ambiguity is a rich source of vulns when it exists between requirements or specifications and development
      - architecture’s role is to eliminate the potential misunderstandings between business reqs for sw and the developers’ implementation of the sw's actions
      - it is vital to acquire business statements such as marketing literature, business goal statements, and requirements-phase artifacts such as use cases, user stories, and reqs
      - these pre-req and req artifacts must be contrasted with development artifacts, such as code, low-level design, and API doc, and then compared to intermediate architecture doc

    * underlying platform vuln analysis
      - architectural risk assessment must include an analysis of the vulns associated with the app execution environment
      - includes OS vulns, network vulns, platform vulns, and interaction vulns resulting from the interaction of components
      - goal is to develop list of app or system vulns that could be accidentally triggered or intentionally exploited resulting in security breach or violation of system security policy
      - when credible threats can be combined with the vulns uncovered in this exercise, a risk exists that needs further analysis and mitigation




- Secure Operation Processes

  -- Deployment Environment
    * technology for sw app deployment has changed over time and when technology evolves it can often change security considerations throughout the life cycle
    * security practitioners must understand SDLC and security impacts from operational process and technology shifts

    * deployment environment is the collective consisting of various components including servers, operating systems, databases, middleware, and any other component that is configured to collaborate and provide an environment for hosting a sw application

    * deployment of sw is not free of risk, whether it takes place in on-premises data centers or in the cloud
    * cloud service providers (CSP) have given organizations the opp to rent virtualized servers and associated service for running existing apps or developing and testing new ones
    * organization may have its development, QA, staging, and production environment all as separate instances in the cloud as part of minimizing their on-premises IT footprint
    * regardless of whom the hosting infrastructure belongs to, security controls of various types and nature must be assured to protect against the threats to, and mitigate the risks of, disclosure, tampering, and destruction of information assets

    * maintaining the security of the deployment/production environment is crucial
    * risks to the environment must be identified and analyzed, including those risks that may stem from:
      - lack of redundancy mechanisms in place to address the availability requirements
      - lack of secure configuration standards for various infrastructure components that make up the production environment
      - excessive physical access to the data centers or logical access to production environment
      - improper patch/update mechanisms that could cause disruptions
      - improper segregation of production environment from nonproduction environments
      - flawed change management processes
      - insufficient logging and auditing
      - insufficient documentation of the production environment


  -- Virtualization and Containerization
    * operational sw can be deployed using virtualization and/or containerization

    * virtualization is a methodology for emulation or abstraction of hw resources that enables complete execution stacks, including sw apps, to run on it
      - traditional hw can be allocated to multiple virtual machines (VMs) using an operating system called a hypervisor
      - there are two types of hypervisors:
        -- type I hypervisor runs directly on computing hardware
        -- type II relies on sw to make a virtual layer
    * VM is essentially an entire virtual computer: OS, CPU, memory, storage, etc.

    * containerization is a lighter weight packaging of the minimum dependencies and apps required to run sw: frameworks, libraries, and sw
      - this minimum packaging is isolated and modular by design

    * VMs can be moved across hypervisors and underlying resources can generally be scaled up or down as required
    * containers may be moved across platforms and infrastructures provided the container orchestration sw is available


  -- Container Technology Life Cycle Security Considerations
    * NIST SP 800-190 (Application Container Security Guide) primarily describes
      - core components of containerization
      - major risks
      - corresponding mitigation techniques

    * however, there is also guidance on the container
      - life cycle—creation
      - testing
      - accreditation (valtuutus)

    * initiation
      - security practitioners should consider container impact on security policies and posture and make adjustments to account for security gaps
      - incident response and forensic analysis polices will undoubtably be impacted
      - vuln management and assessment will need to be tailored for containers
      - new processes and training can be considered to address any potential culture shock introduced by the technology change

    * planning & design
      - containers are immutable (muuttumaton) by nature, which can improve forensic analysis when accounted for properly within an organization’s continuity and response efforts
      - this opportunity is directly related to a greater separation between the OS and app sw, which simplifies efforts

    * implementation
      - after container is designed it should be tested prior to being released for production
      - NIST SP 800-125 (Guide to Security for Full Virtualization Technologies) outlines several infrastructure assessment considerations that should occur before production containerization deployment:
        -- authentication
        -- connectivity
        -- networking
        -- app functionality
        -- management
        -- performance
        -- technology security (including isolation capabilities)

      - new infrastructure without sunsetting preexisting infrastructure is correlated with an increased organizational attack surface
      - new security tools may be required to properly analyze the attack surface
      - security controls and technologies will also need to be adjusted (e.g., event logging, network logging)
      - to reduce potential negative impacts while minimizing risk use a phased deployment of containers within the organization

    * operations & maintenance
      - operational security processes are equally important to app security for containerization
      - organizational container technology should be updated and assessed regularly
      - policies and procedures must be updated in conjunction with technology maintenance
      - container monitoring should be integrated with preexisting security infrastructure such as SIEM tools to ensure reporting mechanisms are effective

      - NIST SP 800-61 (Computer Security Incident Handling Guide) applies to containerization and should be considered by security practitioners
      - incident response teams must be able to know the roles, owners, and sensitivity levels of containers, and be able to integrate this data into processes
      - regardless, predefined procedures should describe response steps for container incidents
      - updating incident response plans is an integral part of maintenance

    * disposition
      - containers can be instantiated and deprovisioned automatically based on organizational needs
      - this is a strength for resource utilization and a challenge for record retention and forensics
      - organizations must ensure mechanisms satisfy data retention policies
      - data storage devices for containers must be accounted for in organizational disposal plans


  -- System Integration
    * systems seldom run in silos and without dependencies on other sw systems and processes upstream and downstream
    * system integration testing must assure the verification of proper execution and functionality of sw components and interfaces between modules within the solution

    * there are various areas of concern when it comes to system integration
      - one example would be integration with legacy apps developed using older technologies/frameworks that may not support certain features and capabilities (SSO) resulting in the expansion of the attack surface and presenting a new attack vector
      - another example would be encrypted data exchanged to/from downstream/upstream processes that would require exchange of encryption keys; this could also introduce a weakness related to the key management processes
      - many more examples can be listed, but ultimately all such risks must be identified and analyzed for proper handling


  -- Continuous Improvement (CI)
    * quality, security, and resiliency of sw products is closely related to the overall sw development process
    * this is evident in Capability Maturity Model Integration (CMMI) by SEI, the multipart standard from the ISO, and ISO/IEC 27034, among many more examples
    * what is common in all such frameworks or standards is the emphasis on the need for the continuous process improvement

    * continuous improvements in SDLC processes are recommended regardless of the sw development processes used by the organization
    * actual implementation of how these improvements are made is affected by various factors, such as development methodology that is adopted and the development team’s culture
      - one implementation method for continuous and incremental improvements would be “Retrospectives” in Agile environments


  -- Continuous Monitoring (CM)
    * information security continuous monitoring (ISCM), commonly referred to as CONMON
      - defined by NIST as “Maintaining ongoing awareness of information security, vulnerabilities, and threats to support organizational risk management decisions”

    * the terms “continuous” and “ongoing” in this context mean that security controls and organizational risks are assessed and analyzed at a frequency sufficient to support risk-based security decisions to adequately protect organization information

    * NIST SP 800-137 Rev1, describes this as disciplined and structured process that integrates information security and risk management activities into system-development life cycle
    * ongoing monitoring is a critical part of that risk management process
    * in addition, an organization’s overall security architecture and accompanying security program are monitored to ensure that organization-wide operations remain within an acceptable level of risk, despite any changes that occur
    * timely, relevant, and accurate information is vital, particularly when resources are limited, and organizations must prioritize their efforts

    * process of implementing ISCM is described follows:
      - define the ISCM strategy
      - establish an ISCM program
      - implement the ISCM program
      - analyze and report findings
      - respond to findings
      - review and update ISCM strategy and program


  -- Verification and Validation
    * NIST SP 800-160 vol. 1 defines verification as the process of producing objective evidence that sufficiently demonstrates that the system satisfies its security requirements and security characteristics with the level of assurance that applies to the system

    * validation is the confirmation that requirements for a specific intended use or application have been fulfilled and that the system, while in use, fulfills its mission or business objectives while being able to provide adequate protection for stakeholder and mission or business assets, minimize or contain asset loss and associated consequences, and achieve its intended use in its intended operational environment with the desired level of trustworthiness

    * trustworthy sw is achieved by leveraging security throughout the life cycle both operationally and from a secure sw development perspective
    * these efforts demonstrate security controls and assessment activities that aid in the risk management framework (RMF) authorization process and support general engineering best practices


  -- Assessment and Authorization (A&A)
    * NIST SP 800-37 (Risk Management Framework for Information Systems and Organizations: A System Life Cycle Approach for Security and Privacy)
      - transitions certification and accreditation (C&A) language to DoD Information Assurance Risk Management Framework (DIARMF) Assessment and Authorization (A&A)
      - encourages organizations to leverage automation to increase effectiveness of executing the RMF process

      - assessment and continuous monitoring of controls can be matured by automation of development pipelines using modern practices such as DevSecOps and modern technologies such as virtualization and containerization
      - all this automation can report and feed into a system authorization package that allows executives to make timely decisions about the trustworthiness of the system



Terms and Definitions
---------------------
Agile Development - Development that uses small team environments and focuses on collaborative, iterative learning, building, testing, and deployment of capabilities to operational use. Addresses the need for rapid software development and deployment cycles, following patterns of activities such as “scrum,” “sprint,” or “safe” to manage change and develop and deploy working, reliable, and verifiable function.
Application Programming Interface (API) - A set of routines, standards, protocols, and tools for building software applications to access a web-based software application or web tool.
Building Security in Maturity Model (BSIMM) - A descriptive model that provides a baseline of observed software security initiatives and activities from a collection of software development shops (Source: thecyberwire.com/glossary). Used as a reference tool to identify goals and objectives, gauge progress or select activities to implement into an organization’s strategy.
Business Continuity Plan (BCP) - The documentation of a predetermined set of instructions or procedures that describe how an organization’s mission/business processes will be sustained during and after a significant disruption.
Business Impact Analysis (BIA) - An analysis of an information system’s requirements, functions, and interdependencies used to characterize system contingency requirements and priorities in the event of a significant disruption.
Center for Internet Security (CIS) - A nonprofit organization created in 2000 with a mission, according to its website, to “make the connected world a safer place by developing, validating, and promoting timely best practice solutions…to protect against pervasive cyber threats.” It has developed standards and technology tools toward the implementation and management of cyber defenses.
Change Control Board - A group of qualified people responsible for the process of regulating and approving changes to hardware, firmware, software, and documentation throughout the development and operational life cycle of an information system.
Common Criteria (CC) - Governing security document that provides a comprehensive, rigorous method for specifying security function and assurance requirements for products and systems.  Source: CNSSI 4009-2015
Common Vulnerabilities and Exposures (CVE) - Security vulnerabilities and exposures that have been publicly disclosed.
Common Vulnerability Scoring System (CVSS) - An open framework for evaluating the severity and risk of software vulnerabilities.
Common Weakness Enumeration (CWE) - A community-developed list of software and hardware weakness types, serving as common language, a measuring stick for security tools, and as a baseline for weakness identification, mitigation, and prevention efforts. Source: cwe.mitre.org

Configuration Management (CM) Plan - A comprehensive description of the roles, responsibilities, policies, and procedures that apply when managing the configuration of products and systems.
Containerization - A software deployment process that bundles application source code and dependencies (e.g., files and libraries) needed to run on any platform.
Continuity of Operations (COOP) Plan - A predetermined set of instructions or procedures that describe how an organization’s mission-essential functions will be temporarily sustained following a disruption that impedes normal operations.
Continuous Monitoring Plan - Maintaining ongoing awareness to support organizational risk decisions.
Control Objectives for Information and Related Technologies(COBIT) - A framework providing guidance for the improvement of IT governance and information management, which helps organizations integrate industry standards, guidelines, and best practices to enterprise governance solutions. Source: isaca.org
Cybersecurity Maturity Model Certification (CMMC) - A program developed by the U.S. DoD that is designed to enforce protection and sensitive unclassified information that is shared by the Department with its contractors and subcontractors. Source: dodcio.defense.gov/CMMC/About
DevSecOps - A paradigm covering development, security, and operations throughout the software development life cycle (SDLC) to ensure security, safety, and resilience in the final system.
DevSecOps Maturity Model (DSOMM) - A model that demonstrates security measures that are applied when using DevOps strategies, how these can be prioritized, and how security can be enhanced. Source: owasp.org/www-project-devsecops-maturity-model/
End-of-life (EOL) - The point at which vendors no longer support a software program or provide updates or patches, making it vulnerable and increasing its potential risk to organizations over time.
Federal Information Processing Standards (FIPS) - A standard for adoption and use by federal departments and agencies that has been developed within the Information Technology Laboratory and published by NIST, a part of the U.S. Department of Commerce. A FIPS covers some topic in information technology to achieve a common level of quality or some level of interoperability.
FISMA - Federal Information Security Modernization Act of 2014 (previously Federal Information Security Management Act of 2002). FISMA 2002 requires each federal agency to develop, document, and implement an agency-wide program to provide information security for the information and systems that support the operations and assets of the agency, including those provided or managed by another agency, contractor, or other sources.
Governance (Information Security) - The process of establishing and maintaining a framework and supporting management structure and processes to provide assurance that information security strategies are aligned with and support business objectives, are consistent with applicable laws and regulations through adherence to policies and internal controls, and provide assignment of responsibility, all to manage risk. Source: NIST Special Publication 800-100—Information Security Handbook: A Guide for Managers
Gramm–Leach–Bliley Act (GLBA) - Also known as the Financial Modernization Act of 1999, GLBA is a federal law enacted in the United States to control the ways that financial institutions deal with the private information of individuals.

Health Insurance Portability and Accountability Act (HIPAA) - This US law is the most important healthcare information regulation in the United States. It directs the adoption of national standards for electronic health care transactions while protecting the privacy of individual's health information. Other provisions address fraud reduction, protections for individuals with health insurance, and a wide range of other healthcare related activities. Est. 1996.
Incident Response (IR) - The structured approach an organization takes to manage a cyber incident. The goal of incident response is to decrease recovery time, reduce damage and the cost of an incident, as well as prevent similar attacks from happening in the future.
Information Security - The protection of information and information systems from unauthorized access, use, disclosure, disruption, modification, or destruction to provide confidentiality, integrity, and availability.
Information Security Continuous Monitoring (ISCM) - NIST 800-137 defines information security continuous monitoring (ISCM) as “maintaining ongoing awareness of information security, vulnerabilities, and threats to support organizational risk management decisions.”
Institute of Electrical and Electronics Engineers (IEEE) - IEEE is an international professional organization that sets standards for telecommunications, computer engineering and similar disciplines.
International Organization of Standards (ISO) - The ISO develops voluntary international standards in collaboration with its partners in international standardization, the International Electro-technical Commission (IEC) and the International Telecommunication Union (ITU), particularly in the field of information and communication technologies.
Internet Engineering Task Force (IETF) - The internet standards organization, made up of network designers, operators, vendors and researchers, defines protocol standards (e.g., IP, TCP, DNS) through a process of collaboration and consensus.
Intellectual Property (IP) - Intellectual property (IP) refers to creations of the mind such as musical, literary, and artistic works; inventions; and symbols, names, images, and designs used in commerce, including copyrights, trademarks, patents, and related rights.
Key Performance Indicator (KPI) - A performance-based metric that measures strategic or operational targets and is often used for decision-making.
Key Risk Indicator (KRI) - Critical predictors/indicators of undesirable events that can adversely impact an organization. The kind of metrics which are forward looking and contribute to the early warning sign that facilitates enterprise to report risks, prevent calamity, and remediate them promptly.
Life Cycle - Evolution of a system, product, service, project or other human-made entity from conception through retirement.
National Institute of Standards and Technology (NIST) - The NIST is part of the U.S. Department of Commerce and addresses the measurement infrastructure within science and technology efforts within the U.S. federal government. NIST sets standards in a number of areas, including information security within the Computer Security Resource Center of the Computer Security Divisions.
National Vulnerability Database (NVD) - The U.S. Government repository of standards-based vulnerability management data, enabling automation of vulnerability management, security measurement, and compliance (e.g., FISMA). The U.S. government repository of standards-based vulnerability management data represented using the Security Content Automation Protocol (SCAP). This data informs automation of vulnerability management, security measurement, and compliance. NVD includes databases of security checklists, security related software flaws, misconfigurations, product names, and impact metrics.

Open Vulnerability and Assessment Language (OVAL) - A language for representing system configuration information, assessing machine state, and reporting assessment results.
Open Web Application Security Project(OWASP) - A nonprofit organization that is committed to improving the security of software and protecting web applications from cyber threats. As an online community, it offers free and open resources, such as articles and documentation, methodologies, and tools and technologies to facilitate web application security.
Payment Card Industry (PCI) - Payment Card Industry (PCI) is a multifaceted security standard that includes requirements for security management, policies, procedures, network architecture, software design, and other critical protective measures.
Payment Card Industry Data Security Standard (PCI DSS)- An information security standard administered by the Payment Card Industry Security Standards Council that applies to merchants and service providers who process credit or debit card transactions.It sets the minimum data security requirements for all merchants handling, processing, or storing cardholder data.
Privacy Impact Assessment (PIA) - An analysis of how information is handled to ensure handling conforms to applicable legal, regulatory, and policy requirements regarding privacy; to determine the risks and effects of creating, collecting, using, processing, storing, maintaining, disseminating, disclosing, and disposing of information in identifiable form in an electronic information system; and to examine and evaluate protections and alternate processes for handling information to mitigate potential privacy concerns.
Residual risk - The risk that remains after security measures have been applied.
Risk - A measure of the extent to which an entity is vulnerable to a potential circumstance or event, and typically a function of:
The adverse impacts that would arise if the circumstance or event occurs.
The likelihood of occurrence.
Risk Assessment - The process of identifying, estimating, and prioritizing risks to organizational operations (including mission, functions, image, and reputation), organizational assets, individuals, and other organizations, resulting from the operation of an information system. Part of risk management, it incorporates threat and vulnerability analyses and considers mitigations provided by security controls planned or in place.
Risk Management - The program and supporting processes to manage information security risk to organizational operations (including mission, functions, image, reputation), organizational assets, individuals, and other organizations. It includes: 1) establishing the context for risk-related activities, 2) assessing risk, 3) responding to risk once determined, and 4) monitoring risk over time.
Risk Management Framework (RMF) - A structured approach used to oversee and manage risk for an enterprise.
Risk Mitigation - Prioritizing, evaluating, and implementing the appropriate risk-reducing controls/countermeasures recommended by the risk management process.
Risk Monitoring - Maintaining ongoing awareness of an organization’s risk environment, risk management program, and associated activities to support risk decisions.
Risk Response - Accepting, avoiding, mitigating, sharing, or transferring risk to organizational operations (e.g., mission, functions, image, or reputation), organizational assets, individuals, and other organizations.

Software Assurance Maturity Model (SAMM/OpenSAMM) - A framework developed by OWASP to ensure that development projects include security features and practices in the software development life cycle.
SCRUM - A framework to facilitate a team’s development process through collaboration and by breaking down the work into increments, allowing for experimentation and feedback that informs continuous learning and improvement.
Security Content Automation Protocol (SCAP) - A suite of specifications that standardize the format and nomenclature by which software flaw and security configuration information is communicated, both to machines and humans.  Source: NIST SP 800-126
Security Controls - The management, operational, and technical controls (i.e., safeguards or countermeasures) prescribed for an information system to protect the confidentiality, integrity, and availability of the system and its information.
Security Technical Implementation Guide (STIG) - Based on Department of Defense (DoD) policy and security controls, an implementation guide geared to a specific product and version. Contains all requirements that have been flagged as applicable for the product that have been selected on a DoD baseline.
Software Assurance - The level of confidence that software is free from defects either intentionally designed into the software or accidentally inserted at any time during its life cycle and that it functions in the intended manner.
Software Development Life Cycle (SDLC) - A formal or informal methodology for designing, creating, and maintaining software (including code built in hardware). Source: NIST SP 800-128
System Security Plan (SSP) - Formal document that provides an overview of the security requirements for an information system and describes the security controls in place or planned for meeting those requirements.
Verification - The process of producing objective evidence that sufficiently demonstrates that the system satisfies its security requirements and security characteristics with the level of assurance that applies to the system.
Virtualization - The simulation of the software and/or hardware upon which other software runs; this simulated environment is called a virtual machine. Source(s): NISTIR 8006 from NIST SP 800-125—Adapted
Vulnerability - Weakness in an information system, system security procedures, internal controls, or implementation that could be exploited by a threat.
Vulnerability Assessment - Systematic examination of an information system or product to determine the adequacy of security measures, identify security deficiencies, provide data from which to predict the effectiveness of proposed security measures, and confirm the adequacy of such measures after implementation.


Quiz


Question 1    1 / 1 point
Which of the following best describes an aspect of Agile retrospectives? (D2.1, L2.1)

A) They are intended to transfer the risk to the business.
B) They are critical components of threat modeling.
C) They may be used in place of abuse case modeling.
--> D) They are typically conducted at the end of each sprint.

Correct. Although it is up to the organization to determine the frequency, retrospectives are typically conducted at the end of each sprint.



Question 2    1 / 1 point
Which of the following best describes a dynamic software development method concerned with software quality and team member quality of life which includes pair programming, collective ownership, and on-site "embedded" customers? (D2.1, L2.1)

A) Scrum
B) Waterfall
--> C) Extreme Programming (XP)
D) Prototyping

Correct. The practices mentioned in the question are specific to Extreme Programming (XP), which is an Agile implementation as described in this domain.



Question 3    1 / 1 point
Which of the following could be considered a disadvantage of Agile methods? (D2.1, L2.2)

A) They do not accommodate changes in requirements.
B) They require fast computers for execution of the code.
C) They do not work with older code.
--> D) They do not allow sufficient time for traditional security planning or analysis.

Correct. Agile methods don't allow sufficient time for traditional security planning or analysis.



Question 4    0 / 1 point
Which of the following best describes a Security Technical Implementation Guide (STIG)? (D2.2, L2.4)

--> A) Security guidance for a specific software product or operating system
B) Security general best practice guidance
C) Security automation tool for a specific software product or operating system
D) Security automation tool for general best practices

STIG is not a general best practice guide and security automation tool.



Question 5    1 / 1 point
Feng is a new security analyst at a digital health care provider. He was given the task of checking security technical implementation compliance for MS SQL Server 2016. He discovers that there are more than 125 configuration items for each of 11 servers he needs to check today. What can be used to automate this effort? (D2.2, L2.4)

A) Security Technical Implementation Guide (STIG)
--> B) Security Content Automation Protocol (SCAP)
C) Software Identification (SWID)
D) Federal Information Processing Standards (FIPS)

Correct. SCAP is a framework that provides a standardized approach to automatically checking and validating the security configuration of systems.



Question 6    1 / 1 point
Which of the following is most accurate regarding frameworks? (D2.2, L2.4)

A) Frameworks are accepted as best practices.
--> B) Frameworks are practices and guidelines often employed when regulations or standard practices are immature.
C) Frameworks are standards that are always internationally recognized.
D) Frameworks are only applicable to large organizations.

Correct. Frameworks are practices that are generally employed in the absence of standard practices.



Question 7    1 / 1 point
Nadal is a team member in your security group. He is struggling to define what milestones will bring operational security efforts in the group to an acceptable level. What should he do first? (D2.3, L2.8)

A) He should seek buy-in from stakeholders for the desired maturity level.
B) He should define the present state of security.
--> C) He should work with security stakeholders to define the present state of security.
D) He should seek resources from stakeholders for the desired maturity level.

Correct. Nadal should work with security practitioners within his organization to define the present state of security in order to understand the delta between present and desired state. After that, he can present a case for maturity to organizational stakeholders to gain approval and resources.



Question 8    1 / 1 point
Which of the following documents is most likely to demonstrate adequate software system protections? (D2.4, L2.9)

--> A) System Security Plan (SSP)
B) Business Continuity Plan (BCP)
C) Continuity of Operations Plan (COOP)
D) Business Impact Assessment (BIA)

Correct. SSP describes future and present protections for the system.


Question 9    1 / 1 point
Ahmad notices that antivirus updates are consistently being handled improperly by IT analysts. He is looking for documentation that provides step-by-step antivirus update guidance to see if the documentation contains errors. Based on the document titles below, which would be best to check? (D2.4, L2.10)

A) NIST Special Publication 800-83: Guide to Malware Incident Prevention and Handling for Desktops and Laptops
B) Antivirus Update Guidelines
C) Antivirus Update Policies
--> D) Antivirus Update Procedures

Correct. Ahmad should investigate the company procedures to see step-by-step actions prescribed to IT analysts.



Question 10   1 / 1 point
Izzy ran a software assurance tool on non-production source code and forgot what she named the output file and the time the tool was executed. What content would most likely indicate her tool results? (D2.5, L2.11)

--> A) A file containing common weakness enumerations (CWEs)
B) A file containing common vulnerability enumerations (CVEs)
C) A file containing Security Technical Implementation Guides (STIGs)
D) A file containing server errors

Correct. SwA tools that analyze source code for defects would produce CWEs. A Compiled or runtime application could be exploited to demonstrate a CVE.



Question 11   1 / 1 point
Which of the following describes ordinal, qualitative categories of Common Vulnerability Scoring System (CVSS) score metrics? (D2.5, L2.11)

--> A) None, Low, Medium, High, Critical
B) 0.0-3.9, 4.0-6.9, 7.0-10.0
C) Low, Moderate, High, Very High
D) 0, 1, 2

Correct. This is potentially a tough question if you are unfamiliar with data types or CVSS. The CVSS v3.0 ratings are None, Low, Medium, High, and Critical.



Question 12   1 / 1 point
Which of the following is NOT an example of a technique used to address data remanence (jäännös)? (D2.6, L2.12)

A) Cryptographic erasure
--> B) Replication
C) Overwriting
D) Degaussing

Correct. Cryptographic erasure, overwriting, and degaussing the media are examples of such techniques, whereas replication is not.



Question 13   1 / 1 point
Claire is seeking guidance for implementing data sanitization practices within her organization. She plans to use a well-known source of guidance as a foundation and tailor the approach to meet her organization's needs. Which of the following NIST publications would be best suited? (D2.6, L2.12)

A) NIST SP 800-160 Volume 1
B) NIST SP 800-160 Volume 2
--> C) NIST SP 800-88
D) NIST SP 800-53

Correct. NIST Special Publication 800-88 "Guidelines for Media Sanitization" offers guidance for the disposal of data that can be mapped to software security considerations.



Question 14   1 / 1 point
NIST SP 800-39 "Managing Information Security Risk" describes a multitiered security reporting structure. Within this structure, which level of reporting is LEAST likely to encompass end point and network appliance logs? (D2.7, L2.13)

A) Tactical
--> B) Organization
C) Business Processes
D) Information Systems

Correct. The higher up the reporting structure, the more strategic and abstracted the reporting becomes—organizational is the highest level.



Question 15   0 / 1 point
Software feedback loops can take the positive or negative output and inject it into other processes. What is the goal of automatic feedback for positive and negative respectively? (D2.7, L2.13)

--> A) Accelerate the process, stabilize the process
B) Stabilize the process, accelerate the process
C) Accelerate the process, stop the process
D) Stabilize the process, stop the process

Stabilize the process, Accelerate the process presents the goals of automatic positive and negative feedback in the reverse order.
Accelerate the process, Stop the process suggests that the goal of automatic negative feedback is to stop the process, which is not accurate.
Stabilize the process, Stop the process implies that both positive and negative feedback aim to stop the process, which is not true.



Question 16   1 / 1 point
MathicusPrime, a large technology company, invented an algorithm for secure text messages, audio, and video. MathicusPrime needs legal protection for its intellectual property but is not concerned with the algorithm being made publicly available. Which of the following provides the most appropriate protections? (D2.8, L2.14)

--> A) Patent
B) Copyright
C) Trade secret
D) Trademark

Correct. Organizations may seek patents for algorithms. Patents provide legal protection, and since MathicusPrime is not trying to keep its algorithms a secret, seeking a patent makes sense.



Question 17   1 / 1 point
Which of the following is NOT a secure configuration management (CM) process? (D2.2. L2.5)

A) Planning
--> B) Implementing file security
C) Controlling configuration changes
D) Monitoring

Correct. CM deals with monitoring data items; it does not explicitly imply file security.




Question 18   1 / 1 point
Which of the following best highlights a software security benefit of configuration management (CM)? (D2.8, 2.9, L2.14, 2.15)

A) Maintaining Development Confidentiality
B) Maintaining Operational Confidentiality
C) Maintaining Development Integrity
--> D) Maintaining Operational Integrity

According to NIST, configuration management is a collection of activities focused on establishing and maintaining the integrity of information technology products and information systems, through control of processes for initializing, changing, and monitoring the configurations of those products and systems throughout the system development life cycle. Answers A, B, and C are less correct.



Question 19   0 / 1 point
Which of the following best describes the goal of establishing an Information Security Continuous Monitoring (ISCM) program? (D2.9, L2.15)

A) To define the ISCM strategy
B) To update the ISCM strategy
C) To implement the ISCM strategy
--> D) To make assets harder to exploit, especially without detection, through hardware and software asset management

C is not wrong—it's just a weaker selection than D. Updates ensure relevance but should never be considered a goal. Defining strategy is not the goal of ISCM program—it's a precursor.



Question 20   1 / 1 point
Ezra is investigating the test processes surrounding an unsuccessful software launch. The software was built correctly based on contractual requirements but does not satisfy the customer's usability needs. Which of the following should Ezra investigate (D2.7, 2.9, L2.13, 2.16)

A) Integration Testing
B) Unit Testing
C) Verification
--> D) Validation

Correct. Validation ensures that we are building the right product.





DOMAIN 3: Secure Software Requirements
--------------------------------------

- Software Security Requirements

  -- organizations must ensure that their overall activities and operations are aligned with applicable laws, regulations, and standards
    * may impose requirements including protections for sensitive information
    * need to assure (and be able to show) that IT systems are properly secured, and the information contained in those systems is protected adequately
    * IT controls must be evaluated to ensure that there are no gaps and that controls are effective, meaning that they are working as intended

  -- requirements management
    * technical requirements
      - functional
        -- capabilities, features
        -- complete
        -- documented
        -- security
          * security functionality and features
          * from pragmatic point of view, to build secure app, it is important to identify attacks that the app must defend against, according to its business and technical context
          * derived in many ways, for example SQUARE framework
            - delineates (rajaa) and supports (through manual processes, checklists, cheat sheets, etc.) requirements specification process that explicitly considers and addresses security throughout sw functionality
            - ensures that all features are complete, clear and testable

      - non-functional
        -- performance
        -- scalability
        -- interoperability requirements
        -- (security) --> should be mapped to functional requirements (to avoid security gaps in implementation)

      - use & abuse cases
        -- narrative descriptions of interactions (user - system) informing functional requirements
        -- defines the sw functional requirements, but not necessarily the internal structure of the sw
        -- use case modeling
          * identifying actors
          * intended system behavior (use cases)
          * sequence of actions in relation to use cases and actors (users, non-human processes) = subjects
          * components = objects are items subject can act upon
            - high level objects must be broken down into more granular objects for better accuracy of subj -- obj -relations


      - Application Security Verification Standard (ASVS) by OWASP
        -- provides app security verification criteria
        -- may also be used as a catalog and a standard source of security requirements that are categorized and placed into various buckets

        -- dev teams may include applicable requirements from ASVS for each release of their sw
        -- these requirements can serve as basic verifiable statements and expanded upon through user stories as needed

        -- defines three verification and assurance levels that are appropriate for different types of apps requiring various levels of trust
          * example of this concept:

            Secure Software Development Life Cycle Requirements (Partial List)
            #                   Description                      L1  L2  L3
            ------------------------------------------------------------------
            1.1.1  Verify the use of a secure sw dev
                   lifecycle that addresses security in
                   all stages of development. (C1)                   ✓   ✓

            1.1.2  Verify the use of threat modeling for
                   every design change or sprint planning to
                   identify threats, plan for countermeasures,
                   facilitate appropriate risk responses, and
                   guide security testing.                           ✓   ✓

            1.1.3  Verify that all user stories and features
                   contain functional security constraints,
                   such as "As a user, I should be able to
                   view and edit my profile. I should not
                   be able to view or edit anyone else's
                   profile."                                         ✓   ✓

            1.1.4  Verify documentation and justification
                   of all the application's trust boundaries,
                   components, and significant data flows.           ✓   ✓



  -- business requirements
    * blueprints that guide sw design and implementation
    * challenges:
      - managing and satisfying requirements (that may be evolving along the way)
      - improper elicitation (esille tuominen)
      - specification
      - validation of requirements

    * if requirements are not clear:
      - poor product quality
      - extensive timelines
      - scope creep
      - increased cost (for re-architecting)
      - missed requirements
      - defects
      - stakeholder dissatisfaction


  -- Security focused stories
    - user story represents a high-level description of a user requirement
    - user story translates requirements into language that can be understood and executed by sw developers

    - INVEST: characteristics of for good stories

      * independent
        - stories can be delivered in any order

      * negotiable
        - details of what’s in the story are created by the programmers and customers during development

      * valuable
        - functionality is seen as valuable by the customers or users of the sw

      * estimable
        - programmers can provide a reasonable estimate for building the story

      * small
        - stories should be built in small amount of time, usually a matter of person-days
        - several stories should be able to be built within one iteration

      * testable
        - tests should be written to verify the sw for this story works correctly


    - organizational requirements are long-term goals



- Compliance Requirements
  -- organizations must engage in a variety of processes to identify and analyze compliance requirements
    * they must analyze their business requirements to determine the security-relevant aspects of those requirements
    * they must also decompose their security policies to arrive at concrete security requirements for sw
    * overall, organizations need to clearly understand and stay abreast of all laws, regulations and industry standards applicable to their sectors, industries, and businesses

  -- Laws, Regulations, Standards, Guidelines
    * security requirements generally, and sw security requirements specifically, may originate from the need to comply with:
      - privacy laws
      - regulations
      - industry standards

    * example: HIPAA (Health Insurance Portability and Accountability Act) of 1996
      - federal law that required the creation of national standards to protect sensitive patient health information from being disclosed without the patient’s consent or knowledge
      - U.S. Department of Health and Human Services (HHS) issued the HIPAA Privacy Rule to implement the requirements of HIPAA
      - HIPAA Security Rule protects a subset of information covered by the Privacy Rule

      - HIPAA privacy rule standards
        -- address the use and disclosure of individuals’ health information (protected health information = PHI) by entities subject to the Privacy Rule
        -- these individuals and organizations are called covered entities
        -- also contains standards for individuals’ rights to understand and control how their health information is used

      - HIPAA security rule
        -- protects a subset of information covered by the Privacy Rule
        -- subset is all individually identifiable health information a covered entity creates, receives, maintains, or transmits in electronic form
        -- this information is called electronic protected health information (e-PHI)
        -- the security rule does not apply to PHI transmitted orally or in written hard copy (i.e., on paper)


    * Sarbanes–Oxley Act (SOX)
      - primarily intended for regulating publicly traded companies, but it has also had a significant impact on information security
      - administered by the Securities and Exchange Commission, SOX aims to protect investors from the possibility of fraudulent accounting activities like those that occurred in the high-profile scandals involving Enron, WorldCom, and Tyco in the early 2000s

      - the act is organized into 11 titles:
        -- Public Company Accounting Oversight
        -- Auditor Independence
        -- Corporate Responsibility
        -- Enhanced Financial Disclosures
        -- Analyst Conflicts of Interest
        -- Commission Resources and Authority
        -- Studies and Reports
        -- Corporate and Criminal Fraud Accountability
        -- White-Collar Crime Penalty Enhancements
        -- Corporate Tax Returns
        -- Corporate Fraud Accountability


    * Gramm–Leach–Bliley Act (GLBA)
      - federal law enacted in U.S. aimed at protecting consumers’ personal financial information held by institutions
      - has data protection requirements

      - compliance with GLBA requires financial institutions to address the financial privacy rule, safeguards rule, and pretexting provisions as described below:
        -- Financial privacy rule
          * requires each financial institution to give privacy notices to customers that explain its information collection and sharing practices

        -- Safeguards rule
          * requires all financial institutions to design, implement, and maintain safeguards to protect the confidentiality and integrity of personal consumer information

        -- Pretexting provisions
          * protect consumers from individuals and companies that obtain their personal financial information under false pretenses (pyrkimyksillä), including fraudulent statements and impersonation


    * Federal Information Security Management Act (FISMA)
      - requires federal agencies to implement a program that provides security for their information and information systems
      - contractors and vendors that provide services to manage systems on behalf of or maintain close relationships with a government agency may be held to similar standards

      - has various requirements including:
        -- Planning for security
        -- Performing periodic risk assessments and establishing policies and procedures based on these assessments
        -- Ensuring that appropriate individuals are assigned security responsibilities
        -- Subjecting security controls to periodic reviews
        -- Establishing policy and procedures for detecting, reporting, and responding to security incidents


    * Payment Card Industry Data Security Standard (PCI DSS)
      - applies to all entities that store, transmit or process payment card (commonly known as credit/debit card) information
      - provides a baseline of technical and operational requirements designed to protect cardholder data and reduce card fraud

      - requirements of PCI DSS are:
        -- Install and maintain a firewall configuration to protect cardholder data
        -- Do not use vendor-supplied defaults for system passwords and other security parameters
        -- Protect stored cardholder data
        -- Encrypt transmission of cardholder data across open, public networks
        -- Use and regularly update antivirus software
        -- Develop and maintain secure systems and applications
        -- Restrict access to cardholder data by business need-to-know
        -- Assign a unique ID to each person with computer access
        -- Restrict physical access to cardholder data
        -- Track and monitor all access to network resources and cardholder data
        -- Regularly test security systems and processes
        -- Maintain a policy that addresses information security


    * Security Standards and Frameworks
      - organizations frequently need to address compliance with multiple security standards and frameworks, including those from
        -- U.S National Institute of Standards and Technology (NIST)
        -- International Standards Organization (ISO)

      - organizations cannot assume that these security standards and frameworks will only affect network security, operations security, and physical security
      - meeting the compliance requirements of these standards may have a far-reaching impact on sw development processes as well
        -- for example, some of the controls in these standards may define security-relevant obligations for designers, developers, testers and those responsible for change management


    * National Institute of Standards and Technology (NIST) Standards and Frameworks
      - one of the core competencies of the institute is the development and use of standards
      - although NIST standards are primarily developed for federal systems, many industries in the private sector have adopted some NIST standards voluntarily
      - though NIST standards are specific to the United States, they may also apply in outsourced situations:
        -- contractual obligations may require foreign companies to which sw development is outsourced to comply with these standards

      - NIST SP 800-88: Guidelines for Media Sanitization, Revision 19
        -- information security concern regarding information disposal and media sanitization resides not in the media but in the recorded information
        -- the issue of media disposal and sanitization is driven by the information placed intentionally or unintentionally on the media
        -- electronic media used on a system should be assumed to contain information commensurate (oikeassa suhteessa) with the security categorization of the system’s confidentiality

      - NIST SP 800-53: Security and Privacy Controls for Federal Information Systems and Organizations
        -- contains families of controls for implementing system security
        -- example of a software-specific control would include SA-8-Security Engineering Principles
          * organization applies information system security engineering principles in the specification, design, development, implementation, and modification of the information system

      - NIST SP 800-160, Volume 1: Systems Security Engineering Considerations for a Multidisciplinary Approach in the Engineering of Trustworthy Secure Systems
        -- security design principles and concepts serve as the foundation for engineering trustworthy secure systems, including their constituent (ainesosa) subsystems and components
        -- these principles and concepts represent research, development, and application experience starting with the early incorporation of security mechanisms for trusted operating systems, to today’s wide variety of fully networked, distributed, mobile, and virtual computing components, environments, and systems

      - NIST SP 800-160, Volume 2: Developing Cyber-Resilient Systems: A Systems Security Engineering Approach
        -- addresses security, safety, and resiliency issues from the perspective of stakeholder requirements and protection needs using established engineering processes to ensure that those requirements and needs are addressed across the entire system life cycle to develop more trustworthy systems

      - NIST SP 800-190: Application Container Security Guide
        -- security concerns associated with container technologies
        -- makes practical recommendations for addressing those concerns when planning for, implementing, and maintaining containers
        -- many of the recommendations are specific to a particular component or tier within the container technology architecture

      - NIST SP 800-218: Secure Software Development Framework (SSDF) Version 1.1: Recommendations for Mitigating the Risk of Software Vulnerabilities
        -- set of fundamental, sound practices for secure sw development called the Secure Software Development Framework (SSDF)
        -- organizations should integrate the SSDF throughout their existing sw development practices, express their secure sw development requirements to third-party suppliers using SSDF conventions, and acquire sw that meets the practices described in the SSDF

        -- SSDF helps organizations to meet the following secure software development recommendations:
          * organizations should ensure that their people, processes, and technology are prepared to perform secure software development
          * organizations should protect all components of their software from tampering with and unauthorized access
          * organizations should produce well-secured software with minimal security vulnerabilities in its releases
          * organizations should identify residual vulns in their sw releases and respond appropriately to address those vulns and prevent similar ones from occurring in the future


      - NIST Cybersecurity Framework (CSF)
        -- NIST framework for Improving Critical Infrastructure Cybersecurity
          * risk management-based approach to addressing cybersecurity risk
          * being reviewed for revision at the time of writing, but the present version includes:
            - core framework
            - tiered implementation (porrastettu toteutus)
            - framework specific profiles

          * each CSF component supports integrating business mission and cybersecurity tasks




    * International Standards Organization (ISO) Standards
      - primary body that develops international standards for several industry sectors in conjunction with the International Electrotechnical Commission (IEC) and International Telecommunication Union (ITU)
      - unlike many other standards that are broad in their guidance, most ISO standards are highly specific

      - ISO/IEC 5055
        -- sw standard related to the measurement of the internal structure of a sw product based on critical business factors:
          * security
          * reliability
          * efficiency
          * maintainability

        -- helps organizations determine the trustworthiness and resilience of a sw system by measuring integrity and quality


      - ISO/IEC 9126
        -- sw quality standard that aligns with businesses who seek ISO 9001 certification
        -- provides mission and system level sw product evaluation tools
        -- tests for six main software characteristics which include
          * portability
          * maintainability
          * efficiency
          * usability
          * reliability
          * functionality


      - ISO/IEC 25010
        -- more modern version of ISO 9126 which is designed to ensure software manufacturing embraces a standard level of consistent quality
        -- includes the six characteristics from ISO 9126 plus:
          * security
          * compatibility


      - ISO/IEC 27000 series
        -- ISO 27001
          * outlines the specifications for an information security management system (ISMS)
          * relevant to all types of organizations, and it provides for an independently audited certification
          * scope of an ISMS goes above and beyond security in software, but various controls in this standard are relevant to secure SDLC

        -- ISO 27002
          * supporting standard for ISO 27001 that guides the implementation of information security controls
          * more than 100 security controls recommended in ISO 27002 are placed logically in 14 groups of related controls, collectively intended to address various security objectives that pertain to confidentiality, integrity, or availability

       -- ISO 27005
          * provides guidance on managing information security risks
          * specifically, informs risk management activities for any size and sector organization specific to information security risk assessment and treatment

       -- ISO 27017
         * code of practice for information security that provides supplementary cloud implementation guidance for ISO 27001
         * ISO 27018 has a similar goal and structure to ISO 27017 but is focused on privacy concerns

       -- ISO 27034
         * multipart standard that provides industry best practices on specifying, designing/selecting, and implementing information security controls through a specified set of processes integrated throughout the SDLC
         * applicable to all software development options (internal, external, outsourced, or hybrid)
         * method agnostic
         * complements other systems development standards and methods

       -- ISO 27701
         * recent addition that covers the EU General Data Protection Regulation (GDPR) considers using the implementation of a privacy information management system (PIMS)
         * provides additional guidance for ISO 27001 to achieve compliance


      - ISO/IEC 15288: Systems and Software Engineering: System Life Cycle Processes
        -- describes the processes and life cycle stages for system and application engineering
        -- sw security practitioners may benefit by comparing an organization’s present technical processes to the fourteen technical processes described within the standard
          * including
            - integration
            - verification
            - validation
            - disposal




  -- Data Classification Requirements
    * organizational data should be properly labeled, and a classification system implemented before data can be protected
    * critical or sensitive data cannot be protected if no distinctions exist between the information within an organization
    * various approaches for implementing an information classification system
      - NIST SP 800-60: Guide for Mapping Types of Information and Information Systems to Security Categories
      - ISO 27001 (A5.12 Classification of Information and A5.13 Labeling of Information)

    * through classification, organizations can facilitate compliance and ensure data is adequately protected
    * sensitivity marking and labeling can go a long way, helping organizations comply with the handling requirements of different data types
    * data may be structured, unstructured, or even semistructured
    * sensitive data must be protected regardless of its structuring throughout its life cycle

    * data governance
      - requires the establishment of an effective information classification process and the definition/specification of roles and responsibilities
      - policy and procedures must be established for classifying organizational data/information based on its level of sensitivity, value, and mission criticality
      - management and protection of an organization’s information assets requires a system
      - as described by the Data Governance Institute:
        -- “Data Governance is a system of decision rights and accountabilities for information-related processes, executed according to agreed-upon models which describe who can take what actions with what information, and when, under what circumstances, using what methods.”

      - may require:
        -- process to classify and categorize data based on defined criteria (e.g., data sensitivity levels)
        -- clear definition and establishment of specific roles and corresponding responsibilities for data owners, data custodians (huoltaja), and data users, among others

    * data classification
      - process requires a consistent approach to categorizing data based on criteria that are well thought out, specific, and predefined
      - helps determine what baseline security controls are appropriate for safeguarding that data, which in turn can help ensure data protection is done in the most cost-effective manner

      - process relies on the assumption that the organization has a clear understanding of the data it has in its possession, regardless of data format, and structured or unstructured
      - may require the organization to perform a discovery exercise to develop this understanding and establish an inventory of data assets

      - modern enterprise has evolved into a giant producer and consumer of data
      - despite the large volume of controls and efforts to protect various data types, few organizations can map exactly where their sensitive data is located and what security controls are deployed to guard it

      - where data is dispersed (hajallaan), this can increase the risks and potential vulnerabilities associated with data storage and management
      - for organizations to adequately mitigate these risks, they should first locate data types to understand and map where data resides, along with the type and content of the data
        -- this step is called data discovery

      - classification
        -- next step after data discovery
        -- implies that data will be identified and understood according to sensitivity, value, usefulness, age, or any other relevant criteria as determined by the organization or by the applicable laws and regulations

        -- following are some examples of sensitivity categories for different organizations:
          * commercial businesses
            - confidential
            - private
            - sensitive
            - public

          * U.S. military
            - top secret
            - secret
            - confidential
            - sensitive but unclassified
            - unclassified

          * U.S. government agencies
            - for internal use only
            - confidential
            - private
            - sensitive
            - public

        -- data classification policy and process should be used for implementing data controls such as DLP (Data Loss Prevention) and encryption
        -- data classification is also a requirement in various regulations, standards, and information security frameworks


    * labeling
      - after classification, labeling and marking must be applied
      - for each sensitivity classification, different handling requirements and procedures must be specified pertaining (liittyen) to the access, usage, and destruction of the data
      - for example, in a corporation, confidential information may only be accessed by senior management and a select few people throughout the company
      - to access the information, two or more may be required to enter their access codes
      - auditing can be detailed and monitored daily
      - paper copies of the information may be kept in a vault

      - secure data labeling and marking involves several different aspects, including security policy, security classification, and data sensitivity
      - terms “security markings” and “security labels” are defined in NIST publications as follows:
        -- security labels: means used to associate a set of security attributes with a specific information object as part of the data structure for that object
        -- security markings: means used to associate a set of security attributes with objects in a human-readable form that enables organizational process-based enforcement of information security policies

        -- to distinguish between these two terms, think of marking hard copies with the classifications, while labeling objects in IT systems

      - classifications are fundamental to many security services
        -- for example, data loss prevention systems may use classifications; intrusion detection systems may use classifications; database management systems may associate classifications with fields, records, tables, or other entities; and operating systems may associate classifications with processes and users

      - labeling systems can help assess the sensitivity of the information involved, which in turn can help apply the degree of protection that is most appropriate for that type of information
        -- applying strong authentication, strong cryptographic protection, tamper detection mechanisms, and other controls is only valuable where such a level of protection is required
        -- ultimately, this can help organizations achieve cost-effective security measures


    * data ownership
      - enterprises may take different approaches to the ownership of the enterprise data
      - possibilities include:
        -- assignment of data ownership to individuals or silos within the enterprise
        -- assignment of the data ownership to the enterprise
      - regardless of the approach or the rationale, accountability must be addressed for access management and compliance purposes

      - data owner and data controller
        -- owners are typically business groups within the enterprise, as opposed to technical teams
        -- data owner falls within the bounds of the due care responsibilities and who will be held responsible for any negligent acts that result in corruption or disclosure of the data
        -- various responsibilities are associated with data owners with respect to their data, such as:
          * data classification
          * determination of the retention requirements
          * granting or denying subjects’ access requests
          * delegation of the responsibility for day-to-day data maintenance to data custodians

        -- data owner must classify the data based on classification criteria such as the usefulness, value, or age of data
        -- classification could also be based on the perceived impact of a breach in confidentiality, integrity, or availability of the data
          * this would likely require a clear understanding of all applicable laws, regulations, standards, and liabilities related to a breach

        -- data controller determines the purposes for which, and how, personal data is processed
        -- if your company/organization decides why and how the personal data should be processed, the company is the data controller
          * employees processing personal data within your organization do so to fulfill your tasks as data controller

    * data custodian and data processor
      - data custodian is given the responsibility for the maintenance and protection of the data
        -- responsible for the preservation of CIA of the data in their custody
        -- data custodian’s general activities include
          * performing and restoring backups
          * implementing security mechanisms
          * fulfilling requirements specified in organizational security policies, standards and guidelines

        -- in terms of sw development, the data custodian role might be slightly different and involve assurance of authorized access to data, sustaining data integrity, versioning of data sets, and maintaining a history of changes

      - data processor processes personal data only on behalf of the controller
        -- processor is usually a third party external to the company
        -- in the case of groups of undertakings (yritysryhmä), one undertaking may act as processor for another undertaking
        -- duties of the processor toward the controller must be specified in a contract or another legal act
          * example: contract must indicate what happens to the personal data once the contract is terminated

    * data user and data subject
      - data user
        -- individuals who routinely use the data as part of their work-related tasks
        -- require the level of access to the data necessary for performing their duties and activities
        -- have data security responsibilities: they must follow operational security procedures to ensure the CIA of the data

      - data subject
        -- individual that personal data is about
        -- person who can be identified, directly or indirectly, by reference to an identification number or to one or more factors specific to their physical, physiological, mental, economic, cultural, or social identity (e.g., telephone number, IP address)

    * types of data
      - not all data is equal
      - data takes one of three general forms:

        -- structured data
          * centrally managed and allow better control of enterprise data
          * often accessed and managed using a database or repository such as Structured Query Language (SQL)
          * examples: credit cards, names, addresses, and dates
          * some data is structured, but most is not

        -- unstructured data
          * most data are unstructured
          * does not have a predefined data model or is not organized in a predefined manner and is, therefore, difficult to process or analyze using conventional tools and methods
          * modern NoSQL (or “not only SQL”) data management tools may be used to access and manipulate these types of data pools

        -- semistructured data
          * relies on metadata (e.g., JSON, CSV, XML) to aid data cataloging and analysis

      - data can also be defined based on information capacity                                                                   most
      - common information types include:                                                                                          ^
        -- quantitative (määrällinen): numerical often continuous values                                                           |
        -- ordinal: categorical feature where order is important (e.g., t-shirt sizes)                                             |    information
        -- categorical: discrete values such as binary but more than two possibilities (e.g., Apple, Orange, Peach, Pear)          |      capacity
        -- binary: two possible choices such as 1 or 0, Yes or No                                                                  |
                                                                                                                                 least

    * data life cycle
      - data must be managed with respect to the data life cycle and adequately protected during each phase
      - life cycle begins at the time of data creation or acquisition and continues until the data has been properly disposed
      - concerns and considerations for the protection of data in each phase of the life cycle vary pending storage, processing, or archived considerations

      - life cycle CSUSAD:
        create --> store --> use --> share --> archive --> destroy


   * secure data retention, retrieval, destruction
     - most categories of data processed for specific scopes and purposes must be retained (säilyttää) for a set amount of time
     - organizations must understand their obligations to meet applicable data retention and preservation requirements
     - same is true with respect to the destruction of data when it is no longer needed, and sanitization of the media to address the risk of data remanence (jäännös)

     - data retention
       -- data retention policy will establish a protocol for retaining information for operational or regulatory compliance needs
       -- objectives of a data retention policy are to keep important information for future use or reference, to organize information so it can be searched and accessed later, and to dispose of information that is no longer needed
       -- policy balances the legal, regulatory, and business data archival requirements against data storage costs, complexity and other data considerations
       -- organization must recognize and understand the applicable retention requirements posed by various laws, regulations, and standards
       -- protection of retained data must be assured through appropriate safeguards and countermeasures (e.g., restricted access control and encryption)

     - data destruction
       -- key part of data protection procedures is safely disposing of data once it is no longer needed
       -- failure to do so may result in data breaches and/or compliance failures
       -- safe disposal procedures are designed to ensure that there are no files, pointers, or data remanence left behind within a system that could be used to restore the original data
       -- with respect to the data destruction requirements for each classification, different techniques and approved tools for overwriting, purging, or cryptographic erasure may be needed
       -- for certain sensitivity classifications, the physical destruction of the media (again, using appropriate tools and techniques) may be the only acceptable method


   * security models
     - define rules of behavior for an information system to enforce policies related to system security
     - typically involving confidentiality and/or integrity policies of the system
     - define allowable behavior for one or more aspects of system operation
     - technology enforces the rules of behavior to ensure security goals (e.g., confidentiality, integrity) are met

     - two confidentiality models:

       -- Bell-LaPadula (BLP)
         * intended to address confidentiality in a multilevel security (MLS) system
         * defines two primary security constructs, subjects, and objects
           - subjects are the active parties
           - objects are the passive parties

         * to determine what subjects will be allowed to do, they are assigned clearances
         * clearances outline what modes of access (e.g., read, write) subjects can use when they interact with objects

         * uses labels to keep track of clearances and classifications
         * implements set of rules to limit interactions between different types of subjects and objects

         * early security model, does not provide a mechanism for a one-tone mapping of individual subjects and objects
           --> needs to be addressed by other models or features within a practical operating system

         * defines two properties, the ss-property and the *-property:

           - simple security property:
             -- subject cannot read/access an object of a higher classification (no read up)

           - star property
             -- subject can only save an object at the same or higher classification (no write down)

         * does not attempt to define technical constructs or solutions
         * identifies high-level set of rules
         * if implemented correctly, prevent exposure or unauthorized disclosure of information in a system processing different classification levels of data

       -- Brewer and Nash
         * focuses on preventing conflict of interest when a given subject has access to objects with sensitive information associated with two competing parties
         * principle is that users should not access the confidential information of both a client organization and one or more of its competitors

         * at the beginning, subjects may access either set of objects, but once a subject accesses an object associated with one competitor, they are instantly prevented from accessing any objects on the opposite side
           - this is intended to prevent the subject from sharing information inappropriately between the two competitors even unintentionally
           - it is called the Chinese Wall Model because, like the Great Wall of China, once on one side of the wall, a person cannot get to the other side

         * unusual model in comparison with many of the others because the access control rules change based on subject behavior


    - two integrity models:

      -- Biba
        * designed to address data integrity and does not address data confidentiality
        * like Bell-LaPadula, a lattice-based (ristikkoperusteinen) model with multiple levels
        * defines similar but slightly different modes of access (e.g., observe, modify) and also describes interactions between subjects and objects
        * where Biba differs most obviously is that it is an integrity model; it focuses on ensuring that the integrity of information is being maintained by preventing corruption

        * at the core of the model is a multilevel approach to integrity designed to prevent unauthorized subjects from modifying objects
        * access is controlled to ensure that objects maintain their current state of integrity as subjects interact with them
        * instead of the confidentiality levels used by Bell-LaPadula, Biba assigns integrity levels to subjects and objects depending on how trustworthy they are considered to be
        * like Bell-LaPadula, Biba considers the same modes of access but with different results

        * defines three properties, the ss-property, the *-property as in BLP and the invocation property

          - simple integrity property
            -- subject cannot observe an object of lower integrity (no read down)

          - star property
            -- subject cannot modify an object of higher integrity (no write up)

          - invocation property
            -- subject cannot send logical service requests to an object of higher integrity

      -- Clark-Wilson
        * Biba only addresses one of three key integrity goals
        * Clark-Wilson model improves Biba by focusing on integrity at the transaction level and addressing three major goals of integrity in a commercial environment
        * to address the second goal of integrity, Clark and Wilson realized that they needed a way to prevent authorized subjects from making undesirable changes
          - this required that transactions by authorized subjects be evaluated by another party before they were committed on the model system
          - provides a separation of duties where the powers of the authorized subject were limited by another subject given the power to evaluate and complete the transaction

        * to address internal consistency (or consistency within the model system itself)
          - Clark and Wilson recommended a strict definition of well-formed transactions
            -- the set of steps within any transaction would need to be carefully designed and enforced
            -- any deviation from that expected path would result in a failure of the transaction to ensure that the model system’s integrity was not compromised
            -- to control all subject and object interactions, Clark-Wilson establishes a system of subject–program–object bindings such that the subject no longer has direct access to the object
            -- instead, this is done through a program with access to the object
            -- this program arbitrates (sovittelee) all access and ensures that every interaction between subject and object follows a defined set of rules
            -- the program provides for subject authentication and identification and limits all access to objects under its control


    - other models
      -- Graham-Denning
        * primarily concerned with how subjects and objects are created, how subjects are assigned rights or privileges, and how ownership of objects is managed
        * primarily concerned with how a model system controls subjects and objects at a basic level where other models simply assumed such control

        * this access control model has three parts: a set of objects, a set of subjects, and a set of rights
        * subjects are composed of two things: a process and a domain
          - domain is the set of constraints controlling how subjects may access objects
          - subjects may also be objects at specific times

          - the set of rights govern how subjects may manipulate passive objects

          - this model describes eight primitive protection rights called commands that subjects can execute to have an effect on other subjects or objects:

            -- secure object creation
            -- secure object deletion
            -- secure subject creation
            -- secure subject deletion
            -- secure provisioning of read access right
            -- secure provisioning of grant access right
            -- secure provisioning of delete access right
            -- secure provisioning of transfer access right

        * modern implementation
          - most modern operating systems implement elements of security models
          - not perfect implementations of academic models and focus on practical implementations that provide functionality consistent with one or more of the security models
          - precise implementation of the security models has practical limitations and is rarely employed except in specialized systems with intentionally limited functionality

      -- Harrison, Ruzzo, Ullman (HRU)
        * like Graham-Denning
        * composed of a set of generic rights and a finite set of commands
        * also concerned with situations in which a subject should be restricted from gaining particular privileges
          - to do so, subjects are prevented from accessing programs, or subroutines, that can execute a particular command (to grant read access for example) where necessary



  -- Privacy Requirements
    * data security and data privacy are related but there are also key differences
    * think about an organizational scenario where various layers of defense (e.g., restricted access control, encryption of data at rest) have successfully secured user privacy data, but privacy laws were still violated by collecting the privacy data without consent

    * the best security is built into sw and sw development operations at inception and monitored and controlled throughout the organizational and sw life cycle
    * simply put, sw security cannot be an afterthought
    * the same can be stated about privacy
    * privacy considerations must be included in sw projects when identified, and data privacy requirements must be identified early within the project life cycle


    * privacy risk
      - privacy laws across the globe protect individuals against misuse of their personal data
      - businesses need to operate within the confines of these laws
      - anonymization can be used to remove the association between the data subject and the identifying dataset
      - most privacy laws and regulations require organizations to address user consent and protection, retention, and secure disposition of personal data

      - data protection and privacy laws and regulations provide protection against misuse and disclosure of
        -- personally identifiable information (PII)
        -- personal healthcare information
        -- individuals’ financial information

      - unauthorized access, use, or disclosure of personal data can adversely affect individuals and organizations, often to a severe degree
      - for an organization, the adverse impact is typically in two areas: loss of public trust and legal liability

      - how problems during the processing of privacy data can adversely affect individuals, and in turn have a detrimental impact on the organization that processed the privacy data
        -- problem
          * arises from data processing

        -- individual
          * experiences direct impact (embarrassment, disrimination, economic loss, ...)

        -- organization
          * resulting impact (customer abandonment, noncompliance cost, harm to reputation and internal culture, ...)


    * privacy laws and regulations
      - various laws and regulations govern how organizations may collect, store, share or disclose individuals’ private data
      - examples:
        -- California Consumer Privacy Act (CCPA)
          * landmark piece of data privacy legislation
          * enacted in 2018 and enforceable as of July 1, 2020
          * secures new privacy rights for California consumers relating to the access, deletion, and sharing of personal information
          * obligations apply to businesses that meet certain criteria regarding size, location, and type of goods/services provided

        -- EU privacy laws
          * in EU, the Directive on Data Protection was established in 1995 and regulated the processing and storage of personal data
          * General Data Protection Regulation (GDPR), adopted in 2016 and enforceable as of 2018, has superseded (ylittää) this
          * GDPR is intended to strengthen and unify data protection for all individuals within the EU
          * also addresses the export of personal data outside the EU

        -- Canadian privacy laws
          * in Canada, the Privacy Act applies to the government sector
          * requires that any PII that a government organization collects must be directly related to a program or activity of the institution
          * Personal Information Protection and Electronic Documents Act (PIPEDA) applies to the private sector and specifies that an organization may only collect, use, or disclose personal information for purposes that a reasonable person would consider appropriate under the circumstances

        -- Australian privacy law
          * main privacy law is Federal Privacy Act
          * defines principles pertaining to collection, use, disclosure and storage of personal information, as well as data quality, data security, openness, access and correction, identifiers, anonymity, transborder data flows, and sensitive information that apply to private organizations and health services providers


    * Organization for Economic Cooperation and Development (OECD)
      - OECD is an intergovernmental economic organization
      - started in 1980, significant update in 2007
      - established a set of principles to protect privacy rights of individuals within member states:

        -- member states limit their collection of personal data
        -- collection of personal data should be obtained by lawful and fair means
        -- collection of personal data should be done with the knowledge or consent of the data subject
        -- personal data should be relevant to the purposes for which it is to be used
        -- personal data needs to be accurate, complete, and up to date
        -- purposes for which personal data is collected should be specified at the time of data collection
        -- personal data should not be disclosed, made available, or otherwise used for purposes other than those specified
        -- personal data should be protected by reasonable security safeguards


    * cybersecurity and privacy risk
      - while managing cybersecurity risk contributes to managing privacy risk, it is not sufficient, as privacy risks can also arise by means unrelated to cybersecurity incidents

      - safeguarding personally identifiable information (PII)
        -- privacy requirements must be considered wherever PII relating to a person or persons is collected and stored

        -- NIST: PII can be used to trace an individual’s identity (name, social security number, biometric records,...), either “alone, or when combined with other personal or identifying information, linked or linkable to a specific individual”
        -- NIST: “Sensitive PII...if lost, compromised, or disclosed without authorization, could result in harm, embarrassment, inconvenience, or unfairness to an individual”

        -- NIST lays out the following types of sensitive PII:
          * social security number (including truncated form)
          * place of birth
          * date of birth
          * mother’s maiden name
          * biometric information
          * medical information (excluding brief references to absences from work)
          * personal financial information
          * credit card or purchase card account numbers
          * passport numbers
          * employment information (e.g., performance ratings, disciplinary actions, and results of background investigations)
          * criminal history
          * any other information that may stigmatize or adversely affect an individual

        -- NIST: “Context of information is important. For example, a list of names and phone numbers for the department’s softball roster is very different from a list of names and phone numbers for individuals being treated for an infectious disease. If sensitive PII is electronically transmitted, it must be protected by secure methodologies, such as encryption, Public Key Infrastructure, or secure sockets layer. When in doubt, treat PII as sensitive"


      - data protection impact assessments
        -- GDPR requires a data protection impact assessment (DPIA) for all projects involving a high risk to personal information
          * “Where a type of processing in particular using new technologies, and taking into account the nature, scope, context and purposes of the processing, is likely to result in a high risk to the rights and freedoms of natural persons, the controller shall, prior to the processing, carry out an assessment of the impact of the envisaged (kuviteltu) processing operations on the protection of personal data”

        -- failing to protect personal information collected on clients or website visitors can result in severe penalties


    * data anonymization
      - protection of PII is like protection of other types of sensitive data in many ways
      - data must be protected against unauthorized disclosure, alteration, and destruction
      - most safeguards used for protection of other sensitive data types may still be applicable to the protection of PII

      - anonymized information is defined as previously identifiable information that has been de-identified, for which a code or other association for re-identification no longer exists
        -- example:
          * in the case of medical data, the anonymized data refers to that from which the patient cannot be identified by the recipient of the information
          * name, address, and full post code would be removed, together with any other information which, in conjunction with other data held by or disclosed to the recipient, could identify the patient

      - de-anonymization is the reverse process, in which anonymous data is cross-referenced with other data sources to re-identify the anonymous data source

      - various techniques and approaches for data anonymization have been suggested. They include:
        -- replacement: substituting identifying information
        -- suppression: omitting PII from released data
        -- generalization: replacing specific information (e.g, full birthday) with something less specific (e.g, year of birth only)
        -- perturbation (häiriöittää): making random changes to the data

      - anonymized data doesn’t necessarily ensure 100% privacy because an attacker could infer PII using information contained in a different data set and linking it to the anonymized data set

      - ultimately the organization must determine which method(s) can help it achieve its anonymization objectives
      - sw development teams need to understand the ramifications of privacy data in databases that are accessed and operated on by applications
      - they also need to understand the ramifications of using production data in nonproduction environments


    * user consent
      - obtaining user consent is a process for getting permission from an individual before collecting, using, or monitoring privacy information connected to that individual
      - individual can only consent if they understand the purpose, facts, implications, and consequences of collecting the information

      - to give user consent, the individual needs to know:
        -- what data is being collected?
        -- why is it being collected?
        -- how is it being collected?
        -- how long will it be used?
        -- how will it be protected?

      - if the individual’s information is collected and monitored using sw, that sw must have the capability to provide an adequate level of protection for private information

      - example of consent: ISO/IEC 27018:2019
        -- requires cloud service providers (CSPs) that adopt this international standard to operate under five key principles
        -- the “Consent Principle” of this ISO standard suggests that CSPs must not use personal data they receive for advertising and marketing unless expressly instructed to do so by the customer

      - with respect to consent, jurisdiction must also be understood
        -- example: consider that in some states the recording of phone conversations may require the consent of just one party whereas in other states the consent of all parties involved would be needed


    * Disposition (hävittäminen, sijoittelu)
      - key part of data protection procedures is the secure disposal of data once it is no longer needed
      - following are some examples of mandates for proper and secure data disposition

        -- Federal Trade Commission (FTC): Disposal of Consumer Report Information and Records
          * requires businesses and individuals that maintain or otherwise possess consumer reports and records for a business purpose to take appropriate measures to dispose of sensitive information derived from such consumer reports and records

        -- HIPAA Security Rule
          * requires that covered entities implement policies and procedures to address the final disposition of electronic PHI and/or the hardware or electronic media on which it is stored, as well as to implement procedures for removal of electronic PHI from electronic media before the media are made available for re-use

        -- Personal Information Protection and Electronic Documents Act (PIPEDA)
          * personal information that is no longer required to fulfil the identified purposes should be destroyed, erased, or made anonymous
          * organizations shall develop guidelines and implement procedures to govern the destruction of personal information

        -- California Consumer Privacy Act (CCPA)
          * gives California consumers the right to delete personal information held by businesses and by extension, a business’s service provider



    * Right to be Forgotten
      - article 17 of the General Data Protection Requirement (GDPR) is titled “Right to erasure” (also called “right to be forgotten”), which states that:

        -- the data subject shall have the right to obtain from the controller the erasure of personal data concerning him or her without undue delay and the controller shall have the obligation to erase personal data without undue delay where one of the following grounds applies:

          a) the personal data are no longer necessary in relation to the purposes for which they were collected or otherwise processed

          b) data subject withdraws consent on which the processing is based according to point (a) of Article 6(1), or point (a) of Article 9(2), and where there is no other legal ground for the processing

          c) data subject objects to the processing pursuant to Article 21(1) and there are no overriding legitimate grounds for the processing, or the data subject objects to the processing pursuant to Article 21(2)

          d) the personal data have been unlawfully processed

          e) the personal data must be erased for compliance with a legal obligation in Union or Member State law to which the controller is subject

          f) the personal data have been collected in relation to the offer of information society services referred to in Article 8(1)



    * Data Retention
      - earlier it was stated that a data retention policy will establish the protocol for retaining information for operational or regulatory compliance needs
      - data minimization and storage limitation required by various laws and regulations from around the world (e.g., GDPR) will require organizations to clearly define their retention periods with respect to personal data


    * Cross-Border Data Transfer
      - many laws and regulations restrict or do not allow personal data to be transferred across borders or to locations where the privacy level or data protection is deemed to be weaker than necessary and required
      - risks of cross-border data transfer are highlighted and reflected in the rules and requirements expressed in various laws and regulations across the globe
      - examples include

        -- Privacy Shield: The EU-U.S. and Swiss-U.S. Privacy Shield Frameworks were designed by the U.S. Department of Commerce and the European Commission and Swiss Administration, respectively, to provide companies on both sides of the Atlantic with a mechanism to comply with data protection requirements when transferring personal data from EU and Switzerland to the US in support of transatlantic commerce

        -- GDPR: The GDPR imposes conditions and requirements that should be met with respect to the transfer of EU citizens’ personal data outside of EU

        -- Asia-Pacific Economic Cooperation (APEC) Privacy Framework: The APEC Privacy Framework requires the implementation of the APEC Cross-Border Privacy Rules (CBPR) system. The CBPR system balances the flow of information and data across borders with the requirements for effective protection for personal information

        -- Australia Privacy Principles (APPs) and New Zealand Information Privacy Principles (NZ IPPs): These principles make it extremely difficult for enterprises to move sensitive information to cloud service providers that store data outside of the borders of Australia or New Zealand


    * Data Residency, Jurisdiction, and Multinational Data Processing
      - privacy laws and regulations from around the world may place restrictions on where their citizens’ personal data may be collected, processed, or stored
      - organizations need to clearly understand and comply with relevant data residency requirements, regardless of whether the data is hosted on-premises or in the cloud
      - the locality of data where the processing and storage of data takes place may have ramifications (seurauksia)

      - jurisdictional variances become evident during cases involving cross-border data requests or cases of contention
      - even between states in the same country there can be differences in data privacy
      - for example, many U.S. states have created their own security breach disclosure laws, and the details of these laws vary from state to state

      - as user privacy and data rights mature, the jurisdictional landscape is expected to remain volatile when organizations are international and deal with users and data located in multiple geographic locations



  -- Data Access Provisioning
    * considerations for the protection of data in each phase of the life cycle can vary if data is stored, processed, or archived
    * sw system design and implementation must “account” for user and service roles and responsibilities to ensure proper separation of duties
    * accounts accessing data should support the concept of least privilege, the principle that a security architecture is designed so that each entity is granted the minimum system authorizations and resources that the entity needs to perform its function, to limit application attack surface


    * Access Control Subject Object Relationship
      - users and service accounts have a useful life
      - subjects (employees and stakeholders) are onboarded and offboarded
      - objects (the data and resources they require access to) are created, managed through the life cycle, and eventually disposed

      - application access control considerations must support both life cycles in tandem to reach a measure of security and operational efficiency
      - access control can require complex trust relationships to support business rules
        -- especially true when the root of trust is derived from many source organizations


    * User and Service Data Access and Provisioning
      - service and users have similar considerations as the data subject for organizations provisioning accounts
      - organization must start by defining the attribute “rule” architecture, subject definitions, and object definitions
      - these three considerations provide the foundation for authorization rules when provisioning accounts and should be explicitly documented within the organization preferably at a higher than the information systems level of the business
      - consistent rule sets of valid values must be defined and actioned at an enterprise level for subjects and objects
      - authorized decisions must be based on known, consistent values
      - life cycle management of rules is always the responsibility of the provisioning organization whether access attribution occurs inside or outside the organization

      - attribute architecture -> rules
        -- access control relationship between data and accounts is dictated by policy attributes
        -- all mandatory attributes should be defined and limited to acceptable values based on policies
        -- these attributes and value limitations must be distributed to stakeholders to help enable owner organizations with rule and relationship development

      - object atrributes -> data
        -- data attributes should be assigned upon object creation
        -- object attributes are often dictated by business (non-security) processes and requirements
        -- attributed data should support logical access decisions while providing provisions for validation and audit
        -- subjects should not be capable of modifying data attributes to manipulate access control decisions

        -- some additional considerations for creating object attributes are:
          * users will not know the values of an object attribute (e.g., to which sensitive compartment a given user is authorized)
          * attributes need to be kept consistent in digital and written policies
          * tools and sw must support business processes

      - subject attributes -> account
        -- people subject attributes are provisioned upon involvement with an organization and could be provisioned by various authorities (e.g., human resources, security)
        -- this type of data access approach is well matured based on obtaining authoritative data
        -- account access attributes may include physical location or MAC address of a device from the request originator
        -- account attribute provisioning capabilities should be dependable, and the decision source should be authoritative
        -- quality, assurance, privacy, and service expectations should all be considered
        -- attribution for service accounts should be similar
        -- irrespective of machine or human interfacing, accounts should be monitored and audited with a level of rigor associated with access and object levels


    * Data Access Monitoring Architecture
      - it is unrealistic to attempt to monitor every data interaction within an enterprise
      - security practitioners must carefully align monitoring, logging, and auditing of accounts and object requests with respect to the sensitivity of data and privileges of accounts
      - organization could monitor the status of authorized and unauthorized subject and object interactions on a network to aggregate metrics such as logical asset locations, MAC addresses, and policies/procedures for network connectivity
      - metrics can be consolidated overtime and might be aggregated hourly, daily, or weekly in a progressive manner

      - security practitioners can also consider password policy implementations as a mechanism to ensure stale accounts are deactivated
      - human users who fail to change their password within an organizationally defined time, such as 90 days, should be deactivated
      - likewise, service accounts that do not request objects within an organizationally defined time, such as 10 days, should be deactivated
      - organizations define access metrics and monitoring frequencies in a way that supports business operations
      - in addition to defining access metrics, organizations determine how the information will be collected and delivered within business tiers as well as external to the organization

      - sw architecture requirements should be functional and support data collection, storage, and analysis capabilities
      - use of automated mechanisms and methodologies should be employed to allow for increased efficiencies and insight into access events
      - data access and audit should be architected in a way that maximizes data reuse and limits data calls
      - additionally, sw developers and security personnel should embrace interoperable data specifications such as SCAP, XML, or JSON, when possible, to enable data reuse


    * Misuse and Abuse Cases
      - abuse cases help identify attacks, evaluate risks, and drive security requirements
      - the process of developing misuse cases and the significance of known attack patterns are also discussed

      - use cases are undoubtedly helpful when it comes to eliciting sw functional reqs and reaching reqs maturation but for security practitioners this is only half the equation
      - to engineer trustworthy, secure systems, practitioners must also advocate for abuse and misuse cases
      - each phase of the secure sw life cycle has its own relevant security activities
      - various security-oriented methodologies (e.g., Microsoft SDL) regard misuse and abuse cases as an effective tool for modeling security requirements

      - to contrast use cases, misuse and abuse cases incapsulate thinking with the perspective of malicious subjects aiming to inflict damage rather than end-user activities
      - when considering the security of an application, abuse cases can be indispensable
      - they can aid development teams by identifying specific attacks against the application and once these attacks have beein identified:

        -- business risk from each corresponding attack can be evaluated
        -- security requirements can be developed
        -- safeguards and countermeasures can be determined
        -- impact on the project timelines, schedules and resources can be better estimated


      - dictionary of known attack patterns
        -- commonly utilized by bad actors, can aid in understanding how common attacks work (understanding the anatomy of an attack)

        -- Common Attack Pattern Enumeration and Classification (CAPEC)
          * good example of a publicly available catalog of common attack patterns, where patterns are presented by the mechanism or the domain of attacks
          * was developed by the U.S. Department of Homeland Security

        -- some well-known attack patterns include:
          * HTTP Response Splitting (CAPEC-34)
          * Session Fixation (CAPEC-61)
          * Cross Site Request Forgery (CAPEC-62)
          * SQL Injection (CAPEC-66)
          * Cross-Site Scripting (CAPEC-63)
          * Buffer Overflow (CAPEC-100)
          * Clickjacking (CAPEC-103)
          * Relative Path Traversal (CAPEC-139)
          * XML Attribute Blowup (CAPEC-229)

      - MITRE Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK)



    * Security Requirements Traceability Matrix (SRTM)
      - managing security requirements is crucial for robust sw systems
      - method to verify the test objective
      - ensures traceability between requirements and test cases

      - it is valuable to have the ability to not only capture all security requirements in one place but also trace from a security requirement to a test case or vice versa
      - NIST describes SRTM as “[m]atrix documenting the system’s agreed upon security requirements derived from all sources, the security features’ implementation details and schedule, and the resources required for assessment"

      - SRTM can be used for any type of project and allow security requirements and security tests to be easily traced back to one another

      - creation of SRTM involves mapping security requirements and corresponding verification methods (test cases)
      - simple grid much like an Excel spreadsheet could be used for the documentation, with a column for each of the following:

        -- Requirement identification number
          * example value: S-100
        -- Requirement description
          * example value: "Protect against man-in-the-middle attacks and session hijacking..."
        -- Source of the requirement
          * example value: "NIST SP 800-53 (Rev. 4) SC-23 Session Authenticity"
        -- Objective of the test
          * example value: "Establish ground for confidence at both ends of the communication sessions.."
        -- Verification method for the test
          * example value: "Method to verify the test objective..."

      - each row is for a new security requirement, making an SRTM an easy way to view and compare the various requirements and tests that are needed in the sw project
      - sometimes organizations map RMF (Risk Management Framework) controls from NIST SP 800-53 using a SRTM



    * Third-Party Vendor Security Requirements
      - organizations rely on third-party vendors for various products and services, including information technology
      - weak security measures among these vendors can expose organizations to significant risks

      - Ensure Security Requirements Flow Down to Suppliers and Providers
        -- organizations work with a range of suppliers, some of which provide information technology products and services or may be connected to the organization’s network
        -- weak security posture for such 3rd-party suppliers can pose concerns to organizations, since malicious actors can exploit the weak network of supplier to gain access to the org
        -- in April 2023, GitHub discovered attackers gained unauthorized access to several customer sw repositories by compromising OAuth for third-party integrators such as Travis-CI
        -- the attacks were planned and highly focused on disrupting the confidentiality of certain GitHub clients

        -- third-party risk assessments are used to assess security risks associated with a supplier
        -- to secure the entire supply chain, organizations typically require each third-party supplier to provide assurance (often in the form of legal indemnifications) that any fourth-party supplier meets or exceeds the same level of security requirements as those imposed on the supplier itself
        -- establishing end-to-end supply chain security is challenging, especially when working with a global supply chain that includes many tiers of international vendors



Abuse (Misuse) Case - A reference to a use case from a hostile actor’s perspective. Created from use cases and likely representing an interaction with the system in ways that the system was not designed to be used.
Jurisdiction - The legal authority of a court to hear and decide a certain type of case. It also is used as a synonym for venue, meaning the geographic area over which the court has territorial jurisdiction to decide cases.
Least Privilege - The principle that a security architecture is designed so that each entity is granted the minimum system authorizations and resources that the entity needs to perform its function. Source: NIST SP 800-171 Rev. 2
Object - Passive system-related entity, including devices, files, records, tables, processes, programs, and domains that contain or receive information. Access to an object (by a subject) implies access to the information it contains. Source: NIST SP 800-53rev5
Subject - An individual, process, or device that causes information to flow among objects or change to the system state. Source: NIST 800-53rev5
Technical Debt - The consequences of software development actions that intentionally or unintentionally prioritize client value and/or project constraints such as delivery deadlines, over more technical implementation, and design considerations.



Quiz
----

Question 1    1 / 1 point
Which of the following BEST represents a security requirement? (D3.1, L3.1)

A) All transaction logs must be digitally signed to assure protection against log tampering
B) All encryption keys must be stored in a key vault to protect against unauthorized disclosure.
C) For the client process to assure that it is communicating and authenticating with the intended server, certificate pinning must be implemented.
--> D) All of the above

Answers A, B, and C can each be considered an example of a security requirement. This makes D the best answer.


Question 2    1 / 1 point
Which of the following is LEAST likely to be a characteristic of a good user story? (D3.1, L3.3)

A) Estimable
--> B) Dependent
C) Testable
D) Small

Correct. Good user stories are independent tasks where efforts can be estimated, small enough to fit in a sprint, and verifiable. See INVEST.


Question 3    1 / 1 point
An organization creates an official document which describes the steps to execute static code analysis. Which of the following document types below best describes the effort? (D3.2, L3.5)

A) Guideline
B) Standard
C) Policy
--> D) Procedure

Correct. Procedures spell out the step-by-step specifics of how the policy and its supporting standards and guidelines will actually be implemented in an operating environment.


Question 4    1 / 1 point
Which of the following ensures an international standard software quality level especially as related to security and interoperability? (D3.2, L3.7)

A) ISO/IEC 27000 Series
--> B) ISO/IEC 25010
C) NIST SP 800-190
D) NIST SP 800-53

Correct. ISO 25010 is a more modern version of ISO 9126, which is designed to ensure software manufacturing embraces a standard level of consistent quality. ISO 25010 includes the six characteristics from ISO 9126 plus security and compatibility.


Question 5    1 / 1 point
Which of the following ensures a standard approach to sanitizing media? (D3.2, L3.7)

A) ISO/IEC 9126
B) ISO/IEC 5050
--> C) NIST SP 800-88
D) NIST SP 800-53

Correct. NIST Special Publication 800-88, Guidelines for Media Sanitization covers guidelines for the proper disposal of media.


Question 6    0 / 1 point
Kimiko has defined classifications and labeled all data within her organization. Which of the following general data protections might she implement next to safeguard confidentiality? (D3.3, L3.9)

A) Pretty Good Privacy (PGP)
B) Data Loss Prevention (DLP)
C) Security Information and Event Management (SIEM)
--> D) Hashing

Incorrect. PGP also provides confidentiality and integrity but is focused on email.


Question 7    1 / 1 point
Which of the following is LEAST likely to be used in the classification of data? (D3.3, L3.9)

A) Compliance
B) Organizational Value
C) Age
--> D) Format

Correct. Classification deals with labeling data according to characteristics such as value and age; data format would not impact classification.


Question 8    1 / 1 point
Which of the following BEST describes the majority of data in our world? (D3.3, L3.10)

--> A) Unstructured
B) Binary
C) JSON
D) XML

Correct. More than 80% of the world's data is unstructured in documents and other formats.


Question 9    1 / 1 point
Which of the following information types has the largest informational capacity? (D3.3, L3.10)

A) Binary
B) Ordinal
--> C) Quantitative
D) Categorical

Correct. Informational capacity from least to greatest capacity from the answer choices is binary, categorical, ordinal, quantitative.


Question 10   1 / 1 point
Which of the following is an anonymization approach for relational data? (D3.4, L3.13)

A) Socialization
--> B) Generalization
C) Derivation
D) Elicitation

Correct. The four anonymization approaches are generalization, perturbation, replacement, and suppression.


Question 11   1 / 1 point
The European Union Directive on Data Protection regulated the processing and storage of personal data. Which of the following most closely aligns? (D3.4, L3.12)

--> A) General Data Protection Regulation (GDPR)
B) Payment Card Industry Data Security Standard (PCI DSS)
C) Personal Information Protection and Electronic Documents Act (PIPEDA)
D) Gramm–Leach–Bliley Act (GLB) Act

Correct. The European Union Directive on Data Protection was replaced by the General Data Protection Regulation (GDPR)


Question 12   1 / 1 point
Most U.S. states have created their own security breach disclosure laws, and most of them follow a consistent pattern. Which of the following is NOT a common element of such pattern? (D3.4, L3.15)

A) The definition of the personal information to which they apply
B) Identifying what constitutes a reportable security breach
C) Identifying when an acceptable notification of the breach should take place
--> D) Circumstances under which a data breach is considered an acceptable event

Correct. A data breach is not an acceptable event.


Question 13   0 / 1 point
Stephanie is architecting a new access control mechanism and wants to include design considerations to detect and respond to end-of-life accounts. Which of the following BEST supports Stephanie's secure engineering goals? (D3.5, L3.16)

A) Logging for subjects and objects
B) Disposal for data no longer required
--> C) Subject deactivation when no activity has occurred
D) Subject deletion when no activity has occurred

Incorrect. Subject deletion when no activity has occurred can be an effective measure to address the issue of end-of-life accounts.


Question 14   1 / 1 point
Which of the following is NOT a consideration for organizations when provisioning accounts for users and services? (D3.5, L3.16)

A) Attribute architecture and rules
B) Object attributes and data
C) Subject attributes and accounts
--> D) Physical location of the data center

Correct. The physical location of the data center is not directly related to the consideration of provisioning accounts for users and services.


Question 15   1 / 1 point
Which of the following are used in the development of abuse cases? (D3.6, L3.17)

A) Case reports
--> B) Use cases
C) Risk results
D) Complaints

Correct. Abuse cases can be developed from the inverse of use cases.


Question 16   1 / 1 point
Which of the following is NOT a correct statement regarding misuse cases? (D3.6, L3.17)

A) They are based on use cases.
B) They must be constructed from the perspective of an attacker.
--> C) They represent scenarios of intended user-system interaction.
D) They are based on understanding of common attack patterns.

Correct. Answer C best describes a use case, not a misuse case.


Question 17   0 / 1 point
Hiroshi is working on deriving additional security controls from organizational requirements. As a security practitioner, which of the following activities should he champion? (D3.7, L3.18)

A) Assign responsibility and define controls.
B) Define enterprise, service, and asset controls.
C) Document the controls in a SRTM.
--> D) All of the above.

Incorrect. A, B, and C are all activities that should be championed.


Question 18   0 / 1 point
Which of the following statements BEST represents Security Requirements Traceability Matrix (SRTM)? (D3.7, L3.18)

--> A) SRTM can be used for forward, backward, and unilateral (yksipuolinen) requirements traceability.
B) SRTM can only be used for forward or backward requirement traceability.
C) SRTM guidance is normally stored in a relational database.
D) SRTM is a matrix documenting system security gaps.

Incorrect. SRTM is not about storing guidance in a relational database, nor is it specifically about documenting system security gaps. Its primary purpose is to track and ensure the alignment of security requirements with the various stages of system development.


Question 19   1 / 1 point
Which statement is MOST true regarding third-party supplier requirements? (D3.8, L3.19)

A) Suppliers should flow security requirements to the acquiring firm
B) Purchasers should supply the Software Bill of Materials (SBOM)
--> C) The acquiring firm should flow security requirements to the supplier
D) Vendors should not supply the Software Bill of Materials (SBOM)

Correct. Organizational security is only as strong as the weakest supply chain link. Suppliers (vendors) should be asked to supply SBOM.


Question 20   0 / 1 point
Which of the following is LEAST likely to bolster software supplier trustworthiness? (D3.8, L3.19)

A) Provenance (alkuperä)
B) SLA/SLR
C) Third-party certification
--> D) None of these

Incorrect. A, B and C all support software supply chain trust.





DOMAIN 4: Secure Software Architecture and Design
-------------------------------------------------

- Security Architecture
  -- sw security practitioner should understand how to assess business needs to derive security architecture requirements
  -- cyber survivability requirements will often be drafted to address the business need for confidentiality, integrity, and availability of sw or system
  -- process often begins by defining critical components whose function is essential to mission success
  -- design decisions may be influenced by several factors, including:
    * legal and regulatory requirements
    * benchmarking and good practices
    * cost and finance
    * risk management processes

  -- according to OWASP
    * Security Architecture (SA) practice focuses on the security linked to components and technology you deal with during the architectural design of your software
    * Secure Architecture Design looks at the selection and composition of components that form the foundation of your solution, focusing on its security properties


  -- Zachman Framework
    * it can be difficult for large organizations to respond to changes in shifting markets and/or adversarial forces
    * Zachman framework can assist by providing a mechanism for classifying organizational architecture
    * it's used to model preexisting organizational functions and processes while controlling business changes
    * contains 36 categories for describing complex systems
    * these categories consist of six columns and six rows, which result in a two-dimensional matrix

      - columns ask enterprise level questions:
        -- What are business data, information, objects?
        -- How does business define processes?
        -- Where are business operations?
        -- When are business processes performed?
        -- Why is system or sw needed?
        -- What performance implications exist?

      - rows are (roles):
        -- Planner View: describes the business purpose and strategy
        -- Owner View: reveals automation candidates
        -- Designer View: outlines organizational system requirements
        -- Implementer View: defines production constraints and design details
        -- Subconstructor View: defines implementation-specific details
        -- User View: view of the functioning system within the operational environment

    * though the framework is largely used for information architecture, it can be applied to a business as a whole


- Sherwood Applied Business Security Architecture (SABSA)
  -- was developed to inform information security architectures using a risk-based enterprise approach and includes guidance for service management
  -- shares many similarities with the Zachman Framework, but was independently developed
  -- provides a means to integrate multiple solutions while managing complexity

  -- includes four life cycle phases:
    * Strategy and Planning
    * Design
    * Implementation
    * Management and Measurement

  -- contains six layers (columns)
    * context
    * concept
    * logic
    * physical
    * component
    * operational/management

  -- perspectives (rows):
    * business
    * architect
    * designer
    * constructor
    * technician
    * manager

  -- similar to each row within Zachman, each SABSA layer has a domain or perspective
    --> this aims to ensure that security is considered equitably from many organizational perspectives


- Security Control Identification and Prioritization
  -- security controls are safeguards that mitigate risk of preexisting systems
  -- physical, administrative, and technical controls may be categorized by the functionality provided

  -- security controls may help to prevent, deter, detect, or delay system disruptions, or to correct and recover from security incidents

  -- Security controls may provide more than one function (e.g., CCTV may be used for deterring intruders or for detecting intrusions)
  -- controls should be prioritized based on a range of criteria such as immediate need, security posture, complexity, resource availability, cost, and/or applicable regulations/standards

  -- following are some examples of control classifications by nature of the control:
    * physical controls (fences, doors, locks, and fire extinguishers)
    * administrative controls (incident response processes, management oversight, and security awareness training)
    * technical controls (user authentication (login) and logical access controls, antivirus software, and firewalls)


- Distributed Computing

  -- Client-Server Architecture
    * consists of a client and a server
    * client component requests and consumes the service(s) that are provided by the server component
    * from security perspective, this would represent an expanded attack surface as compared to centralized computing environment
    * adversaries can now go after the client, after the server, or after the communication channel between the client and the server

  -- Three-Tier Architecture
    * three tiers include
      - presentation tier
      - logic tier
      - data tier

    * security must be addressed at every tier of the architecture

  -- N-Tier Architecture
    * most enterprise web applications utilize the n-tiered architecture that generally includes
      - web server tier
      - application server tier
      - persistence (database) tier

    * there are various application-level threats specific to each of the tiers
      --> security of these applications should be considered in the context of the n-tiered architecture

    * there are various application-level compensating controls that must be provided at each tier
    * some of those controls are provided by the underlying set of technologies used (J2EE and .NET examples are discussed later)
    * while others must be provided by the programmers, architects, and designers involved in building the enterprise web application.

    * threats at web tier include:
      - Poor session management
      - Parameter manipulation
      - Path traversal and path disclosure
      - Canonicalization (process of mapping inputs to their canonical equivalent, for example: name Aryan can be represented in more than one way including Arian, ArYan, Ar%79an)
      - URL encoding

    * threats at business logic tier include:
      - Broken access control
      - Input validation problems
        -- Buffer overflow
        -- Cross-site scripting (XSS)
        -- SQL injection
        -- Command injection
      - Insecure exception management
      - Insecure use of cryptography
      - Insecure default configuration
      - Insecure auditing/logging

    * threats at persistence tier include:
      - Insecure storage


  -- Peer-to-peer (P2P)
    * computing utilizes an architecture with no special machine that provides a service or manages the network resources
    * instead, all responsibilities are uniformly divided among all machines, known as peers, which can serve both as clients and servers
    * when one program controls other programs, it is commonly known as a client/server architecture
    * however, in some distributed computing architecture, the client and the server programs each have the ability to initiate a transaction and act as peers
      - such a configuration is the defining characteristic of P2P architecture

    * management of these resources in P2P network is not centralized but spread among the resources on P2P network uniformly, and each resource can function as a client or a server
    * file sharing programs and instant messaging are well-known examples of this type of architecture
    * P2P file sharing networks are a common ground for hackers to implant malware
    * when P2P networks are designed, it is imperative to include strong access control protection to prevent the upload of malicious files from sources that are not trusted


  -- Message Queuing
    * can be used to allow asynchronous communication of smaller and independent building blocks within distributed applications
    * in this scenario, the producer and consumers interact with the queue to add or process messages
    * from a security perspective, protection of the messages in the queue (i.e., encryption of message body in the queue) and restriction of access to the queue are among aspects that must be addressed



- Service-Oriented Architecture
  -- evolution of sw architecture has proved to be inevitable
  -- service-oriented architecture (SOA) introduced new capabilities and options to the architect
  -- offered an approach that helped address the interoperability challenge found in typical IT environments
  -- increased reusability was another benefit of SOA

  -- paradigm used by SOA is also referred to as the “find-bind-execute” paradigm:
    * registering the service in a public registry used by the service consumer to find the service and execute

  -- Enterprise Service Bus (ESB)
    * sw architecture model used for designing and implementing the interaction and communication between mutually interacting sw applications in SOA
    * as sw architecture model for distributed computing, ESB is a specialty variant of the more general client-server software architecture model
      - it promotes strictly asynchronous message-oriented design for communication and interaction between applications
      - primary use is in enterprise application integration (EAI) of heterogeneous and complex landscapes

  -- Web Services
    * examples of SOA implementation
    * provide platform and vendor neutrality
    * support interoperability, achieved through XML-based open standards

    * standards like Web Services Description Language (WSDL), Simple Object Access Protocol (SOAP), and Universal Description, Discovery, and Integration (UDDI) provide the approach for defining, publishing and using web services

    * since their inception in early 2000s and the release of WSDL and SOAP, web services have evolved
    * RESTful web services built on Representational State Transfer (REST) architecture were introduced by the web consortium subsequent to SOAP
    * these web services were built to work best on the web, and provide the opportunity for the construction of fast, simple, and lightweight RESTful applications
    * unlike SOAP, which is a protocol and relies strictly on XML for transport, REST is an architectural style that supports various formats (e.g., JSON)

    * Securing Web Services
      - with respect to securing web services, authentication and authorization are of utmost importance
      - confidentiality, privacy, integrity, and nonrepudiation must also be addressed through controls that were discussed earlier including encryption (transport-level and/or application-level) and digital signatures

      -  security principles discussed earlier in previous domains are relevant to web services security as well

      - foundational element of innovation in today’s app-driven world is the Application Programming Interface (API)
      - from banks, retail, and transportation to IoT, autonomous vehicles, smart cities - APIs are a critical part of modern mobile, SaaS, and web applications and can be found in customer-facing, partner-facing, and internal applications
      - by nature, APIs expose application logic and sensitive data such as Personally Identifiable Information (PII) and because of this, APIs have increasingly become a target for attackers

      - API integrity must be preserved and applies to the APIs that are developed and owned by you, and those that are used by your applications
      - API security is paramount

      - following list is the OWASP API Security Top 10 2023, which are API-specific security risks

        -- API1: Broken Object Level Authorization:
          APIs tend to expose endpoints that handle object identifiers, creating a wide attack surface of Object Level Access Control issues. Object level authorization checks should be considered in every function that accesses a data source using an ID from the user

        -- API2: Broken Authentication:
          Authentication mechanisms are often implemented incorrectly, allowing attackers to compromise authentication tokens or to exploit implementation flaws to assume other user's identities temporarily or permanently. Compromising a system's ability to identify the client/user compromises API security overall.

        -- API3: Broken Object Property Level Authorization:
          This category combines API3:2019 Excessive Data Exposure and API6:2019 - Mass Assignment, focusing on the root cause: the lack of or improper authorization validation at the object property level. This leads to information exposure or manipulation by unauthorized parties.

        -- API4: Unrestricted resource consumption:
          Satisfying API requests requires resources such as network bandwidth, CPU, memory, and storage. Other resources such as emails/SMS/phone calls or biometrics validation are made available by service providers via API integrations, and paid for per request. Successful attacks can lead to Denial of Service or an increase of operational costs.

        -- API5: Broken Function Level Authorization:
          Complex access control policies with different hierarchies, groups, and roles, and an unclear separation between administrative and regular functions, tend to lead to authorization flaws. By exploiting these issues, attackers can gain access to other users’ resources and/or administrative functions.

        -- API6: Unrestricted Access to Sensitive Business Flows:
          APIs vulnerable to this risk expose a business flow - such as buying a ticket, or posting a comment - without compensating for how the functionality could harm the business if used excessively in an automated manner. This doesn't necessarily come from implementation bugs.

        -- API7: Server Side Request Forgery:
          Server-Side Request Forgery (SSRF) flaws can occur when an API is fetching a remote resource without validating the user-supplied URI. This enables an attacker to coerce (pakottaa) the application to send a crafted request to an unexpected destination, even when protected by a firewall or a VPN.

        -- API8: Security Misconfiguration:
          APIs and the systems supporting them typically contain complex configurations, meant to make the APIs more customizable. Software and DevOps engineers can miss these configurations, or don't follow security best practices when it comes to configuration, opening the door for different types of attacks.

        -- API9: Improper Inventory Management:
          APIs tend to expose more endpoints than traditional web applications, making proper and updated documentation highly important. A proper inventory of hosts and deployed API versions also are important to mitigate issues such as deprecated API versions and exposed debug endpoints.

        -- API10: Unsafe Consumption of APIs:
          Developers tend to trust data received from third-party APIs more than user input, and so tend to adopt weaker security standards. In order to compromise APIs, attackers go after integrated third-party services instead of trying to compromise the target API directly.


  -- Microservices
    * best referred to as an architecture
    * method of sw development based on independent services best suited for web and mobile applications
    * independence in this context implies that these services may even be developed using different programing languages and use different storage with no centralized management
    * services are meant to be designed to serve a single specific function
    * examples of functions could include e-commerce cart, social media login and user roles, among others



- Secure Operational Architecture: Rich Internet Applications
  -- depending on the physical components and architecture, deployment scenarios may include single server, distributed, and high availability
  -- the simplest configuration is the single server deployment
  -- with all modules on a single node, management of security may also be simplified

  -- in distributed deployment scenario, multiple nodes will be involved, which can extend reliability and efficiency

  - high availability, a special type of multinode deployment, may require additional nodes and multiple instances of key processes
  - this type of deployment can result in built-in redundancy and automatic failover

  - Rich Internet Application (RIA)
    * web application with many of the characteristics of desktop application sw (e.g., visual user interfaces with interactive components, drag-and-drop functionality, quick response to mouse-clicks, transition effects, animation), typically delivered by way of a site-specific browser, a browser plug-in, an independent sandbox, extensive use of JavaScript, or a virtual machine

    * list of frameworks/technologies used to build RIAs is long and includes HTML 5, AJAX and Angular
    * some of the technologies that were initially used for building RIAs include Adobe Flash, JavaFX, and Microsoft Silverlight
      --> common RIA platforms that came with supporting runtime libraries, execution engines, and rendering mechanisms
      --> enabled developers to deliver expressive content (streaming audio and video, 3D graphics) to users across browsers, desktops, and other devices
      --> various security concerns existed with these technologies

    * benefits
      - reduced server load (since processing is offloaded to the client as well)
      - smaller footprint on the desktop
      - ability to be used from any computer with a web browser
      - applications updates that can be transparent to the user
      - consistent experience regardless of browser or OS used

      - enhanced user experience, such as information sharing, collaboration, and functionality
      - gives more power to the end user as in the case of Mashups

    * security
      - RIAs may run within a web browser or within a secure sandbox
      - common web application vulnerabilities (e.g., SQL injection, cross-site scripting) apply to RIAs as well (perhaps with a twist)
      - since RIAs work differently from other types of web applications in certain aspects, there are also new opportunities to introduce vulns
      - RIAs must be subjected to various security testing techniques and methods, including automated code scan/analysis and security code reviews

      - Client-Side Exploits
        -- developers and architects often build complex logic in their browser clients, with complex data validation and security rules, forgetting that these rules run in the user domain and can be disabled or bypassed by the user
        -- all validation, authorization, and authentication must be done by the server
        -- data validation may be done by the client to help prevent unnecessary round trips to the server, but all validation must be backed up at the server as well
        -- solely trusting the client to validate the data and input can be easily circumvented and can potentially lead to serious server-side compromise
        -- never rely solely on the client for validation in RIA

      - Remote Code
        -- java applets (special, small application programs written in Java) are good examples of mobile code
        -- code that is transmitted across the network (downloaded from servers) and executed within the client browser also offers capabilities for more interactivity
        -- applets, as examples of mobile code, certainly present security concerns, some of which may be addressed through policies around what actions the applet may or may not take, or what resources it may or may not access

      - Constant Connectivity
        -- RIA usage on any machine with an internet connection leads to these applications being executed from potentially unsafe machines (airports and internet cafes)
        -- these machines are more likely to be infected with key loggers and other malware than corporate machines running the more traditional desktop applications



- Pervasive and Ubiquitous Computing (Kattava ja kaikkialla oleva tietotekniikka)
  -- IoT
    * according to NIST (Considerations for Managing Internet of Things (IoT) Cybersecurity and Privacy Risks):
      - “Cybersecurity and privacy risks for IoT devices can be thought of in terms of three high-level risk mitigation goals"

        1) Protect device security: In other words, prevent a device from being used to conduct attacks, including participating in distributed denial of service (DDoS) attacks against other organizations, and eavesdropping on network traffic or compromising other devices on the same network segment. This goal applies to all IoT devices.

        2) Protect data security: Protect the confidentiality, integrity, and/or availability of data (including personally identifiable information [PII]) collected by, stored on, processed by, or transmitted to or from the IoT device. This goal applies to each IoT device except those without any data that needs protection.

        3) Protect individuals’ privacy: Protect individuals’ privacy impacted by PII processing beyond risks managed through device and data security protection. This goal applies to all IoT devices that process PII or that directly or indirectly impact individuals.


- Embedded Software
  -- computing may be embedded into the hardware itself to control the limited set of functions on a hardware device
  -- embedded sw does not require direct user input or interactions
  -- typically, embedded systems are associated with smaller devices with smaller processors, and minimal OS designed for a specific purpose

  -- embedded sw is different from firmware
  -- while the first may be the only piece of code that runs on the hardware, that normally will not be the case with firmware (it hands over the control to another software, such as the operating system)

  -- ebedded sw has less visibility (as compared to application software) so far as a user is concerned and is designed to fulfill a specific purpose; nevertheless, its security is just as important as that of other types of software

  -- secure update
    * unlike enterprise or desktop computing environments, embedded systems are likely cost-sensitive with computational limitations, leaving limited room for security considerations (e.g., processing of encryption with large key lengths)
    * one implication is that this can create a barrier for the application of best practices and implementation of resource intensive safeguards
    * this may present some challenges, but at the same time highlights the need for device makers and software developers to make security a high priority and address the patching and updating of the software
    * examples of embedded systems include computers in automobiles, wearables, appliances

  -- Field-Programmable Gate Array (FPGA) security features
    * FPGA semiconductor devices are meant to be programmed to desired functionality after manufacturing, which contrasts with application-specific integrated circuits that are custom manufactured with specific design and for specific tasks

    * programmability of FPGA creates the opportunity for its application/usage in various industries and markets including automotive, aerospace and defense, consumer electronics, and many others

    * security-relevant aspects of FPGA include concerns around copy/theft/clone of intellectual properties of the application and data, tampering with the application to disable features or leak information, and introducing malware
    * with the assumption that the FPGA is in the hands of an adversary, usage of the FPGA for malicious activities is part of the concern



- Cloud Architectures
  -- one definition of cloud computing, provided by NIST, is the following:
    * “Cloud computing is a model for enabling ubiquitous (kaikkialla oleva), convenient, on-demand network access to a shared pool of configurable computing resources (e.g., networks, servers, storage, applications, and services) that can be rapidly provisioned and released with minimal management effort or service provider interaction. This cloud model is composed of five essential characteristics, three service models, and four deployment models.”

  -- benefits of cloud computing for business and IT include efficiency, cost reduction, flexibility, and scalability on demand
  -- most notable benefit of the adoption of cloud computing for the organization is touted as the ability to transition from capital expenditure (CapEx) to operation expenditure (OpEx)
  -- specific benefits of the adoption of cloud computing to the organization would be tied to each service model:
    * infrastructure as a service (IaaS)
    * platform as a service (PaaS)
    * software as a service (SaaS)

  -- organizational benefits from cloud computing vary by deployment model:
    * private
    * public
    * community
    * hybrid

  -- service models
    * Software as a service (SaaS)
      - distributed model where sw applications are hosted by a vendor or cloud service provider and made available to customers over network resources
      - most widely used and adopted form of cloud computing, with users often needing only an internet connection and credentials to have full use of the cloud service, application, and data housed

      - two delivery models currently used
        -- first is hosted application management (hosted AM), where a cloud provider hosts commercially available software for customers and delivers it over the web (internet)
        -- second is sw on demand, where a cloud provider provides customers with network-based access to a single copy of an application created specifically for SaaS distribution
          * typically within the same network segment

      - within either delivery model, SaaS can be implemented with a custom application, or the customer may acquire a vendor-specific application that can be tailored to the customer

      - several key benefits for organizations, including:
        -- Ease of use and limited/minimal administration
        -- Automatic updates and patch management
        -- Always running the latest version and most up-to-date deployment (no manual updates required)
        -- Standardization and compatibility (all users have the same version of software)
        -- Global accessibility

    * Platform as a service (PaaS)
      - way for customers to rent virtualized servers and associated services for running existing applications or developing and testing new ones
      - several key benefits for developers, including:
        -- Operating systems can be changed and upgraded frequently
        -- The ability to work together on sw development projects within the same environment when development teams are scattered across various geographic locations
        -- Services are available and can be obtained from diverse sources that cross international boundaries
        -- Upfront and recurring or ongoing costs can be significantly reduced by utilizing a single vendor, rather than maintaining multiple hardware facilities and environments

    * Infrastructure as a service (IaaS)
      - model where the customer can provision equipment as a service to support operations, including storage, hardware, servers, and relevant networking components
      - while the consumer has use of the related equipment, the cloud service provider retains ownership and is ultimately responsible for hosting, running and maintaining the infrastructure
      - referred to as hardware as a service by some customers and providers

      - multiple key benefits for organizations, including:
        -- Usage metered and priced based on units (or instances) consumed, allowing it to be billed back to specific departments or functions
        -- Ability to scale infrastructure services up and down based on use, which is particularly useful and beneficial where there are significant spikes and dips in usage within the infrastructure
        -- Reduced cost of ownership, meaning no need to buy assets for everyday use, no loss of asset value over time, and reduction of other related costs of maintenance and support
        -- Reduced energy and cooling costs, plus a green IT environmental effect, with optimum use of IT resources and systems


  -- deployment models
    * public cloud
      -- delivered over the internet
      -- computing resources (e.g., hardware and software) are owned and managed by the cloud service provider (CSP)
      -- multitenant environment where the computing infrastructure is shared by tenants
      -- most common type of cloud and the least costly as compared to the other types

    * private cloud
      -- computing infrastructure exclusively used by a single organization, which can result in enhanced security and greater control over critical operations
      -- as expected, single tenancy also results in higher costs as compared to other types of cloud
      -- often used by larger organizations (e.g., financial institutions) and government agencies

    * community cloud
      -- multitenant environment that differs from a public cloud in the sense that the tenants (organizations) likely have similar needs and concerns
      -- as expected, costs will likely be higher than the public cloud, but less than a private cloud
      -- when compliance and regulatory measures are key requirements (e.g., government or healthcare), community cloud could be a viable solution

    * hybrid cloud (public cloud + on-prem/private cloud)
      -- not every service or application must be hosted in private cloud, and not every organization is ready/willing to entirely dismantle its own data centers and migrate everything into cloud
      -- combination of on-premises infrastructure or private cloud with the public cloud could be advantageous and offer great flexibility
      -- bursting out of a private cloud and into a public cloud during peak demand times (e.g., tax filing season) for sw services is an example of the flexibility mentioned


-- Overarching (kattavat) Risks and Shared Security Model
  * security in the cloud is affected by various factors
  * developers must determine security requirements based on the selected cloud deployment and service model
  * at all times, developers must keep in mind two key risks associated with applications run in the cloud:
    - multitenancy
    - third-party administration

    * SHARED SECURITY MODEL * <---- see picture in the book


-- Cloud Resource Management
  * cloud assets/resources are controlled through the management plane sw
  * parts of the management plane are exposed to cloud service providers’ customers in the form of a management console, through graphical and command line interfaces and APIs

  * key functionality of the management plane is to create, start, and stop vm instances, and provision them with proper virtual resources such as CPU, memory, permanent storage, network connectivity
  * management plane is the most powerful tool in the entire cloud infrastructure, it will also integrate authentication, access control, and logging and monitoring of resources used
  * management plane is used by the most privileged users: those who install and remove hardware, system sw, firmware and the like
  * management plane is also the pathway for individual tenants who will have limited and controlled access to the cloud’s resources
  * management plane components are at the highest risk concerning software vulnerabilities

  * compute
    - compute resources of a cloud service provider are a combination of:
      -- Processing threads
      -- Volatile memory
      -- Data storage

    - these compute resources are managed and allocated on a per-guest OS and/or a per-host basis within a resource cluster
    - use of reservations, limits, and shares provides the contextual ability for an administrator to allocate the compute resources of a host

  * cloud storage
    - on technical level, persistent mass storage in cloud computing typically consists either of spinning hard disk drives or solid-state drives (SSDs)
    - for reliability purposes, disk drives are often grouped to provide redundancy

    - cloud providers also provide object storage, a method for storing unstructured data using the concept of units or objects rather than a traditional file system
    - the objects (files) are stored with additional metadata (content type, redundancy required, creation date, etc.) that are accessible through programming or web interfaces
    - volume storage is a virtual hard drive that can be attached to a virtual machine instance and be used to host data within a file system
    - storage is written to committed blocks of data
    - volumes attached to IaaS instances behave just like a physical drive or an array

  * storage area network (SAN)
    - network that provides access to consolidated, block-level data storage
    - primarily used to enhance storage devices, such as disk arrays, tape libraries, and optical jukeboxes, accessible to servers so that the devices appear to OS as locally attached devices
    - typically has its own network of storage devices that are generally not accessible through the local area network (LAN) by other devices
    - cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small-to-medium-sized business environments
    - does not provide file abstraction, only block-level operations
    - file systems built on top of SANs however, do provide file-level access and are known as shared-disk file systems

  * network-attached storage (NAS)
    - file-level computer data storage server connected to a computer network providing data access to a heterogeneous group of clients
    - often manufactured as a computer appliance, a purpose-built, specialized computer that serves files using its hardware, sw, or configuration
    - NAS systems are networked appliances that contain one or more storage drives, often arranged into logical, redundant storage containers or redundant array of independent disks (RAID)
    - removes the responsibility of file serving from other servers on the network
    - typically provides access to files using network file sharing protocols such as NFS, SMB/CIFS, or AFP
    - from the mid-1990s, NAS devices began gaining popularity as a convenient method of sharing files among multiple computers
    - potential benefits of dedicated NAS compared to general-purpose servers also serving files include faster data access, easier administration, and simple configuration



-- Mobile Applications
  * typical 21st-century mobile applications have a client-server architecture
  * client component runs on the device, which operates by means of Android, iOS, or some other mobile OS
  * client app, which is what the user interacts with to pay bills or send a secure message, for example, is the mobile application from user’s perspective
  * his is the piece that is downloaded from the app store and installed on the device

  * other component, the server, is hosted somewhere on the premises or in the cloud and is what the client component interacts with
  * the server component is important, as this is where data is stored and processed
  * interaction between the client and the server is over the internet using some form of application programming interface (API)

  * Mobile Security Risks
    - attacks targeting sw vulns may attempt to exploit vulns in client component or server component, or they may target data transmission between client and server components
    - there are reports that compare critical vulns between applications written for Android versus iOS, but information can also be gleaned (poimia) from the OWASP Mobile Top 10 list
    - OWASP Mobile Top 10 by order of importance:

      -- M1—Improper platform usage
      -- M2—Insecure data storage
      -- M3—Insecure communication
      -- M4—Insecure authentication
      -- M5—Insufficient cryptography
      -- M6—Insecure authorization
      -- M7—Client code quality
      -- M8—Code tampering
      -- M9—Reverse engineering
      -- M10—Extraneous functionality

-- Mobile Threat Landscape
  * as capabilities of mobile devices continue to grow, more computer functionality is incorporated
  * expanded capabilities have resulted in new use cases for the devices, and these use cases open the door for potential misuse, as well as new threats, vulns and attack vectors

  * various communication mechanisms in a mobile device, which pose security threats:

    - Cellular
      -- used for voice, text and data services provided by cell radio network carriers

    - Near-Field Communication (NFC)
      -- used for low data rate transfers, smart card emulation, and reading RFID tags

    - Secure Digital (SD) Card
      -- used for additional storage capacity or transferring data between devices

    - Power & Synchronization Cable
      -- wired connection used for charging and exchanging data with a computer

    - Subscriber Identify Module (SIM)
      -- removable hardware token providing data storage and cellular access

    - Bluetooth Technology
      -- personal area networking used cellular access

    - Wi-Fi
      -- local area networking used for access to connected resources or the internet

    - Global Positioning System (GPS)
      -- use of orbiting satellites to determine the geographic location of the device


  * Security of Mobile Devices
    - characteristics of mobile devices are highly relevant to the discussion of applicable risks, threats and vulnerabilities
    - as presented by NIST Special Publication "800-124: Guidelines for Managing the Security of Mobile Devices in the Enterprise", threats at a high level include, among others:

      -- Mobility and lack of physical security
      -- Untrusted (hijacked and rooted) devices
      -- Untrusted networks (insecure channels used to access the internet)
      -- Untrusted applications
      -- Untrusted content
      -- Interaction with other devices

  * Mobile User Interface Design
    - type, size, and granularity of the user interface (UI) utilized by mobile applications varies widely
    - mobile UI design must consider constraints and contexts, screen, input, and mobility as outlines for design

    - mobile device is sized to fit in the user’s hand, so the input capability must allow the users to manipulate a system, and the device’s output (usually the screen) should allow the system to indicate the effects of the user’s manipulation



-- Hardware Platform Concerns
  * term platform in this context refers to the specific structures within which sw will run
  * various hw platforms may have their own machine languages
  * sw may be designed for a specific platform, with specific processor and architecture specifications
  * hw platform capabilities or the lack of certain needed capabilities may also have an impact on the design of security features of the sw
    - example of such capabilities is hardware-assisted cryptography

  * vulns that arise from the device hardware are a great concern
  * preventions for Integrated Circuits (IC) hw exploits are complex and difficult to implement
  * they are typically remediated by upgrading hw, securing supply chains, and manual inspections
  * hw firmware patches are issued less frequently than sw updates, and patch management systems are not streamlined for updating hw and firmware across a diverse range of manufacturers
  * knowledge and skill set required for updating computer hardware and firmware, especially network appliances, is different than patching endpoint sw
  * remediation for hw vulnerabilities may be neglected or may not receive the attention we provide to sw vulnerabilities

  * Side-Channel Mitigation
    - side-channel attack is enabled by information leakage from a physical cryptosystem
    - characteristics that could be exploited include timing, power consumption, and electromagnetic and acoustic emissions

    - side-channel attack does not require the physical hacking of the device
    - instead, secret information, such as cryptographic keys, will be extracted by probing the signatures produced by device
      -- electromagnetic radiation
      -- voltage spikes
      -- power consumption
      -- other issues

    - latest security specification introduced by FIPS 140-3 includes the requirement of security tests for noninvasive attacks, which are side‐channel attacks for exploitation of weak channels
    - some of the defenses implemented by cryptographic modules against side-channel attacks are
      -- constant execution time
      -- stabilization of characteristic signatures for sensitive operations

  * Speculative Execution Mitigation
    - microprocessors use speculative execution to improve performance
    - when speculative execution is utilized, the microprocessor will use ideal clock cycles to execute a block of upcoming instructions before its time
    - for example, rather than waiting for the outcome of a condition that determines the flow of the execution in one of the two paths, the microprocessor executes based on a speculated path
    - when the result of the condition is determined, the processor either abandons the result of the speculated execution or leaps ahead
    - speculative execution results in executing a set of instructions -> executed code may leave a residue of sensitive data in computer cache or memory that could be exploited using side-channel attacks
    - for mitigation, the speculative execution should be disabled.

    - both Meltdown and Spectre attacks exploit the vulns of modern high-speed processors that use speculative execution techniques
    - Cybersecurity and Infrastructure Security Agency Alert TA18-004A provides guidance for Meltdown and Spectre Side-Channel Vulnerability

  * Embedded Hardware Security Modules
    - in general, hardware security module (HSM) is a secure hardware unit dedicated to the secure management and life cycle of cryptographic keys
    - HSM is used for generation, storage and provisioning of crypto keys and hw Random Number Generator (RNG)
    - embedded hw security modules (eHSM) are hw elements that are integrated into digital devices for cryptographic functions
    - eHSMs are designed to be used in IoT devices and for automotive security



-- Cognitive Computing
  * Machine Learning
    - Adversarial Machine Learning (AML) is the process of reverse engineering the behavior and characteristics of an ML model to manipulate the inputs or outputs of the ML system to obtain
    - threat actors may manipulate data inputs into an anomaly detection model to avoid network detection
    - it is additionally possible to disrupt the availability of a system that leverages ML components by forcing erroneous disruptive predictions
    - data inputs for training models should be sanitized like interface inputs and for similar security reasoning



-- Control Systems
  * control systems are hw and sw that are used to command and manage other systems
  * they have embedded sw running on them, which needs to be secure
  * various types of control systems have applications in everyday life (central heating systems) and are also used in industrial processes in automotive, medical and biomedical areas

  * Industrial Control Systems (ICS) Security
    - one of the most common types of industrial control systems (ICS) is the Supervisory Control and Data Acquisition (SCADA) system
    - ICS are computer-controlled systems that monitor and control industrial processes in the physical world
    - SCADA systems are used to control and monitor physical processes, examples of which are
      -- transmission of electricity
      -- transportation of gas and oil in pipelines
      -- water distribution
      -- traffic lights
      -- other systems used as the basis of modern society

    - it is therefore extremely important to protect these control systems from hacker attacks

    - move from proprietary tech to more standardized and open solutions, and increased number of connections between SCADA systems and office networks and the internet, makes them more vulnerable
    - consequently, the security of some SCADA-based systems has come into question, as they are seen as potentially vulnerable to cyberattacks


-- Safety and Criticality
  * due to the time criticality of operations that are typically performed by control systems, availability is generally a prime objective
  * another paramount objective is human safety, depending on the operations involved

  * NIST SP 800-82: Guide to Industrial Control Systems (ICS) Security:
    - today, widely available sw applications and internet-enabled devices have been integrated into most ICS, delivering many benefits, but also increasing system vulnerability
    - threats to ICS can come from numerous sources, which can be classified as adversarial, accidental, structural, and environmental
    - although this guide provides guidance for securing ICS, other types of control systems share similar characteristics and many of the recommendations from this guide are applicable and could be used as a reference to protect such systems against cybersecurity threats
    - for example, although many building, transportation, medical, security and logistics systems use different protocols, ports and services, and are configured and operate in different modes than ICS, they share similar characteristics to traditional ICS



- Secure Interface Design
  -- interface and integration points are critical, but often under-protected parts of systems
  -- whether an interface is machine-to-machine or human-to-machine, security practitioners must ensure that cyber resiliency at points of interaction are addressed
  -- when designed components are secured individually and then integrated into a larger system without security being considered, additional risk-mitigating actions are necessary
  -- where human-to-machine interactions include configuration, choices must be intuitive and easy to make for software users

  -- Security Management Interfaces (SMI)
    * interactions with software and its functionality happen via interfaces, and failure to protect these interfaces can result in a software breach
    * security management interfaces that are used to configure security features, out-of-band interfaces that are used for remote management, and log interfaces that are used in logging and auditing all need to be designed securely with authentication and authorization controls

    * interface security is part of the overall security of sw
    * to protect against common threats and mitigate associated risks, various considerations for secure interface design should be taken into account

    * Security Management Interfaces (SMI) are administrative interfaces for an application that have the highest level of privileges on the system and perform tasks such as:
      - User provisioning: adding/deleting/enabling user accounts
      - Granting rights to different user roles
      - System restart
      - Changing system security settings
      - Accessing audit trails, user credentials, exception logs

    * strong controls must be designed and built in when developing SMI because a compromise can be devastating:
      - ranging from complete compromise to installing backdoors to disclosure, alteration, and destruction (DAD) attacks on audit logs, user credentials, exception logs, and more

    * given that administration interfaces are typically operated using elevated privileges, compromise can lead to magnified adverse impact and severe

    * considerations for securing administration interfaces include:
      - Requiring strong authentication (e.g., using certificates)
      - Disallowing or limiting remote administration, if possible, or requiring secure channels (e.g., VPN)
      - Enforcing role-based authorization and separation/segregation of roles
      - Using multifactor authentication
      - Auditing (logging) access to administrative interfaces and functionality

    * also consider the following guidelines:

      - using secure defaults
        -- where and when possible, use secure default settings
        -- history tells us that in most cases the defaults (secure or not) will be used, and only a few make the effort to do otherwise

      - using user notification
        -- ensure that the user is informed if an operation is not secure
        -- an example is placing a visual (a lock icon displayed in the browser’s address bar) when secure protocols such as SSL/TLS are used on the login page of an online banking portal

      - revocable authorization
        -- wherever possible, the ability to revoke an authorization granted previously must be in place
        -- the process of revoking an authorization must be straightforward and understandable

      - files placed in secure locations
        -- restrict the locations where sensitive files are saved
        -- if the option of specifying other locations is provided to the user, assure that security implications of each choice are conveyed

      - clear security choices
        -- when presenting users with choices, any security implications of such choices should be clear concerning consequences and trade-offs



  -- Out-of-Band Management (OOB)
    * allows an administrator to connect to a computer’s management controller when the computer is turned off, in sleep or hibernate modes, or otherwise unresponsive through OS
    * in contrast, in-band management is the classic approach whereby an agent runs in the full OS on the managed computer and the management controller accomplishes tasks by communicating with agent

    * also known as lights-out management (LOM), OOB involves the use of a dedicated management channel (interface) for remote access and management
    * remote management can be enabled on many computers (not only servers) by adding a remote management card
    * access to the remote management card using the management channel should be protected

    * Log Interfaces
      - sw design should consider interfaces to turn logging on or off in development, test, and production environments as needed
      - additionally, an interface to configure what to log (e.g., errors, application events, operating system events) and how much to log (e.g., status, informational messages, warnings, full stack) is recommended to address capacity issues
      - further, visual user interfaces to graphically represent log data to correlate threats and discern patterns should be planned for and designed to help with auditing



  -- Upstream and Downstream Dependencies
    * terms upstream and downstream can be used in various contexts
    * in the context of sw projects, upstream/downstream dependencies may represent tasks to be started and completed (or things that may need to happen) prior to starting another task (before something else can happen)
    * although this type of dependency is common, other types of dependencies could also exist where two tasks/processes need to start or finish at the same time, or even where one task is not allowed to finish until another task has started
    * understanding the dependences between tasks is a necessity when creating project schedules

    * dependencies may be internal (in the scope of the project) or external (outside the scope of the project)
    * dependencies may also exist in sw development, when one component has dependencies on other components
    * consider an example of an upstream process X that may have encrypted data that subsequently needs to be passed to downstream process Y for processing
    * before processing in the downstream process can begin, the encryption key must be securely retrieved from a key store (i.e., key vault)
    * changes in upstream processes or software components may affect processes or components downstream



  -- Protocol Design Choices
    * protocols are specifications of technical agreements to control the exchange of information between connected devices
    * protocols are agreed-upon rule sets that define the type and content of the data being exchanged

    * cardinal security rule in protocol design is to disallow information to be exchanged in an insecure fashion
    * network security protocols, such as IPsec, TLS, SSH, are all designed to provide a secure connection between two connected devices to allow for secure transmission of data

    * network security protocols are designed to enforce the following principles:

      - Confidentiality (preventing unauthorized data disclosure)
      - Integrity (preventing unauthorized data modification or destruction)
      - Availability (ensuring authorized users have access to data)

    * as well as ensuring integrity and confidentiality, network security protocols provide standardized set of encryption algorithms (symmetric and asymmetric) and hashing algorithms
    * this means that using network security protocols ensures the design will also provide the following:

      - Authentication (mitigating against identity theft or masquerading attacks by using user authentication such as certificates)
      - Message integrity (ensuring that the message contents haven’t been altered by using digital signatures and HMACs)
      - Nonrepudiation (ensuring accountability)


- Reusable Technologies
  -- security practitioners are concerned with confidentiality, integrity, and availability of the systems and sw throughout the life cycle
  -- one consideration for maintaining continuity of operations for systems includes the useful life of all subcomponents, especially when system is designed to last an extended period
  -- one way to reduce the risk of subcomponent replacement within the system life cycle is to embrace modular components
  -- modular or reusable security components can also reduce the overall financial impact of implementing secure system designs - especially with authentication and authorization

  -- Authentication
    * may be performed using single or multiple factors
    * single sign-on (SSO) is used to simplify authentication and provide access to different resources on different systems
    * federated identity mechanisms aim to provide access to multiple systems across different enterprises
    * SAML and OAuth support these concepts but have different workflows and may be better suited for specific scenarios

    * authentication is the process of verifying a subject’s identity through one or more authentication factors
    * this verification may involve a password (something that you know), a physical token (something that you have) or a biometric measure (something that you are)

    * general authentication guidelines have been provided through various resources, including OWASP; these include:
      - Hashing passwords for storage versus encrypting them
      - Using only safe functions to compare password hashes
      - Transmitting passwords using strong transport such as TLS
      - Providing generic authentication error messages

    * broken authentication, one of the OWASP Top 10, serves as a good example:

      - threat agents and attack vectors
        -- attackers have access to hundreds of millions of valid username and password combinations for credential stuffing, default administrative account lists, automated brute force, and dictionary attack tools
        -- session management attacks are well understood, particularly in relation to unexpired session tokens

      - security weakness
        -- prevalence (yleisyys, vallitsevuus) of broken authentication is widespread due to the design and implementation of most identity and access controls
        -- session management is the bedrock of authentication and access controls and is present in all stateful applications
        -- attackers can detect broken authentication using manual means and exploit them using automated tools with password lists and dictionary attacks

      - impacts
        -- attackers must gain access to only a few accounts, or just one admin account, to compromise the system
        -- depending on the application’s domain, this may allow money laundering, Social Security fraud, and identity theft, or disclose legally protected highly sensitive information

    * Multifactor Authentication (MFA)
      - involves two or more factors
      - once again, the primary factors used for authentication today include something that you know, something that you have, and something that you are


  -- Single Sign-On (SSO)
    * simplifies user authentication by allowing subjects to log into system with a single set of credentials and subsequently gain access to resources on multiple systems without maintaining a different set of login credentials
    * details of how SSO works may be different from one implementation to another, but they all use the concept of a central domain through which authentication is performed

    * various approaches and technologies have been introduced to implement SSO and the related federation of identities for access management
    * in nutshell, eliminating multiple login systems in multiple domains requires authentication to happen in a central domain and then be shared securely (i.e., using JSON signed Web Token) with other domains

    * high level flow
      - when a subject attempts to access a resource in one domain, the subject is redirected to the central authentication server
      - the absence or existence of an authentication-related cookie determines whether the user has already been authenticated or not
      - if no authentication-related cookie exists, one must be generated
      - if one exists, the subject will be redirected back to the resource it was originally trying to access

    * SSO systems are composed of
      - credential database
      - master secret server
      - single sign-on (SSO) server

    * regardless of the approach or technologies used for this purpose, safeguards and countermeasures must be put in place to mitigate the risk of a single point of failure or compromise


  -- Authorization
    * upon successful authentication, authorization follows
    * refers to rights and privileges granted to an individual or process that enable access to computer resources and information assets

    * simply put, authorization is the process where requests to access a particular resource should be granted or denied
    * includes the execution rules that determine what functionality and data the user (or Principal) may access, ensuring the proper allocation of access rights after authentication is successful

    * when designing authorization within the sw, it is important to ensure the rights and privileges of the subject are checked before access is granted to the objects
    * access control can be designed using different models

    * the three primary access control models are:

      - Discretionary Access Control (DAC) (harkinnanvarainen)
        -- leaves an amount of access control to the discretion of the object owner or anyone who is authorized to control the object’s access

      - Nondiscretionary Access Control (NDAC)
        -- policies in this category have rules that are not established at the discretion of the user

      - Mandatory Access Control (MAC)
        -- access control policy decisions are made by a central authority, not by the individual owner of an object
        -- the owner cannot change access rights

    * additionally, access control can be role-based (RBAC) or designed using the resource itself

    * described by OWASP in its “Cheat Sheet Series:”
      - in RBAC, access decisions are based on an individual’s roles and responsibilities within the organization or user base
      - process of defining roles is usually based on analyzing the fundamental goals and structure of an organization and is usually linked to the security policy
      - for instance, in a medical organization, the different roles of users may include those such as a doctor, nurse, attendant, patients...
      - obviously, these members require different levels of access to perform their functions, but also the types of web transactions and their allowed context vary greatly depending on the security policy and any relevant regulations (HIPAA, Gramm-Leach-Bliley, etc.)

    * RBAC advantages
      - roles are assigned based on organizational structure with emphasis on the organizational security policy
      - easy to use
      - easy to administer
      - built into most frameworks
      - aligns with security principles like segregation of duties and least privileges

    * RBAC problems
      - documentation of the roles and accesses must be strictly maintained
      - multitenancy cannot be implemented effectively unless there is a way to associate the roles with multitenancy capability requirements
        -- e.g. OU in Active Directory
      - there is a tendency for scope creep to occur
      - does not support data-based access control

    * RBAC areas of caution
      - roles must be transferred or delegated using strict signoffs and procedures
      - when users change roles, admin must make sure the earlier access is revoked to ensure they are assigned to roles on a need-to-know basis
      - assurance for RBAC must be carried out using strict access control reviews



  -- Credential Management
    * complex task that includes the generation, verification, and life cycle of credentials:

      - different forms of credentials
        -- traditionally, credentials consist of a user ID and pw, which can come in different forms such as a smartcard or a certificate

      - credential management API (Windows)
        -- devs writing for MS Windows can use CM API including CM UI functions to obtain and manage credential information like usernames/passwords
        -- these functions request Windows account information to be used in place of the credentials established while logging on
        -- such requests typically occur when the logon credentials do not have permissions required by the application
        -- Credential Management UI functions provide interfaces with the look and feel of the Windows UI
        -- these functions include customizable options that add a user’s information to the user’s credentials store

    * good credential management system should be flexible enough to
      - accommodate both internal and external access
      - differentiate roles and privileges
      - implement sw security mechanisms for access control

    * attacks directed toward CM systems will typically attempt to compromise a credential that can be used in a privilege escalation attack
    * if detected, process must be in place to reissue credentials, which can be both expensive and time-consuming
    * security awareness training and setting appropriate management expectations can go a long way toward preventing such attacks in the first place



  -- Single Sign-On (SSO)
    * important credential-management design consideration
    * important interconnectivity design consideration
    * while the benefit of SSO is that it simplifies user authentication across multiple applications (requires the user to log in only once and then be able to access applications or resources downstream), it requires careful thought and consideration because it often contradicts the complete mediation design principle
    * often implemented using tokens that are created upon successful authentication and then passed around between applications

    * weak passwords in SSO environment can pose a significant risk
    * single point of failure (SPOF) should also be considered
    * when implementing SSO, along with design considerations for SPOF, necessary risk assessments should be conducted and risk posture identified


  -- X.509
    * before discussing digital certificates and X.509 standard, asymmetric algorithms and their function should be explored
    * the process:


                 plaintext                ciphertext                  plaintext
      sender   ------------->  encrypt  --------------->  decrypt  ---------------->  recipient
                                  ^                          ^
                                  |                          |
                                  |                          |
                              recipient                   recipient
                              public key                 private key


    * asymmetric algorithms work based on a pair of keys
      - private key and a public key
      - whichever key is used to encrypt a message, the other key will be needed to decrypt the message
      - which key is used to encrypt depends on the objectives
      - to preserve the confidentiality of a message using asymmetric algorithms, the message is encrypted using the public key of the message recipient, so that only the message recipient may decrypt the message using their private key

    * if public key is to be made available securely to those who need it (as “public” implies), mechanism for this purpose would be needed
    * public-key infrastructure used for this purpose comprises several components, including:

      - Certificate authority (CA)
      - Registration authority (RA)
      - Digital certificates

    * X.509 is the standard for digital certificates
      - there are several components (fields) in digital certificates, including the public key and the validity period for the certificate
      - certificate is normally signed by CA but may also be self-signed for specific purposes
      - also keep in mind that there are different types of certificates, including:

        -- SSL client certificates
        -- SSL server certificates
        -- Object signing certificates

    * when certificates expire, they are added to certificate revocation lists (CRLs)
    * these lists could potentially be long and time consuming to process (potentially may also be stale)
    * Online Certificate Status Protocol (OCSP) may address the shortcomings of CRLs



  -- Flow Control
    * in distributed computing, the flow of information between processes on two systems that may or may not be trusted, poses security challenges
    * several security issues are related to information flow
    * sensitive information (e.g., bank account information, health information, Social Security numbers, credit card statements) stored in a particular Web application should not be displayed on a client browser to those who are not authorized to view that information

    * protection against malware such as spyware and Trojans means that network traffic that carries malicious payload is not allowed to enter the network
    * by controlling the flow of information or data, several threats to software can be mitigated and delivery of valid messages guaranteed
    * the enforcement of security policies concerning the flow of information to and from an application that is independent of code-level security protection mechanisms can be useful in implementing security protection when the code itself cannot be trusted
    * firewalls, proxies, and middleware components such as queuing infrastructure and technologies can be used to control the rate of data transmission and allow or disallow the flow of information across trust boundaries

    * proxy servers
      - by assuring that traffic always travels through a proxy (an intermediary), various objectives may be achieved
      - proxy servers may offer varying levels of functionality
      - pproxy may be used as a gateway sitting between users and the internet, in which case the proxy servers may also improve response time through their caching capabilities

    * firewalls
      - devices or programs that control the flow of traffic between networks and hosts
      - enable organizations to delineate security zones supporting various security postures in each zone
      - commonly used to protect the internal trusted network from the outside, or control connectivity to specific areas, and prevent unauthorized access to sensitive systems and resources (e.g., production systems in the cloud)
      - may be network based or host based
      - Web Application Firewalls (WAFs) can provide a layer of protection for websites against common types of attacks including SQL injection and cross-site scripting (XSS)

      - for example, many enterprise networks employ firewalls to restrict connectivity to and from the internal networks used to service more sensitive functions, such as accounting or personnel
      - by employing firewalls to control connectivity to these areas, an organization can prevent unauthorized access to its systems and resources
      - inclusion of a proper firewall provides an additional layer of security
      - organizations often need to use firewalls to meet security requirements from mandates (e.g., FISMA)
        -- some mandates, such as the Payment Card Industry Data Security Standard (PCI DSS), specifically require firewalling

    * protocols
      - network protocols are a set of rules that govern the communications between computers on a network
      - rules of network protocol include guidelines that regulate the following characteristics of a network:
        -- access method
        -- allowed physical topologies
        -- types of cabling
        -- speed of data transfer

      - these are some of the most common (present and past) network protocols:
        -- Ethernet: uses an access method called CSMA/CD (carrier sense multiple access/collision detection) for flow control
        -- Token ring: uses an access method that involves token passing for flow control
        -- Fiber Distributed Data Interface (FDDI): protocol that is used primarily to interconnect two or more LANs; it uses an access method that involves token passing for flow control

    * queuing in flow control
      - queue consists of a collection of data packets collectively waiting to be transmitted by a network device using a predefined structure methodology
      - these packets are bound to be routed over the network, lined up in a sequential way with a changing header and trailer, and taken out of the queue for transmission by a network device using some defined packet processing algorithm, such as first in, first out (FIFO) order, or based on some priority



  -- Data Loss Prevention (DLP)
    * for a typical enterprise, millions of emails are sent and received, and thousands of files are downloaded, saved, or transferred via various channels or devices daily
    * enterprises routinely store and transmit sensitive data that customers, business partners, regulators, and shareholders expect them to protect
    * unfortunately, companies constantly fall victim to massive data loss, and high-profile data leakage involving sensitive personal and corporate data continue
    * data loss could harm a company’s competitiveness and reputation and could also invite compliance or reputational risks
    * organizations should take measures to understand the sensitive data they hold, how it is controlled, and how to prevent data leaks

    * to properly safeguard sensitive data, organizations must develop or adopt a classification system for data (Unclassified, Confidential, Secret, Top Secret) and label all data first
    * each classification level should be protected to a level commensurate (oikeassa suhteessa) with classification
      - for example, data labeled Top Secret should be safeguarded to higher levels than Confidential data
    * in addition, metrics and policies should be created with respect to data creation, storage, transmission, and disposal

    * organizations can implement technical controls, which are often referred to as data loss prevention (DLP) solutions
      - include features like application firewalls and IPS/IDS to facilitate data transmission monitoring and approval using email alerts, logging, and traffic blocking
      - may be network-centric and monitor traffic or be installed on user devices and servers

    * while some sw and hw are called DLP solutions, the effort to prevent data confidentiality and integrity issues, starting with labeling data, including all the above-mentioned actions, and ending with technical tool implementations, is referenced as a DLP strategy where mitigations are included in the overall organizational efforts for data management



  -- Virtualization
    * involves sharing underlying resources to enable a more efficient and agile use of hw, which drives management efficiency through reduced personnel resourcing and maintenance
    * provides the ability to run multiple operating systems (guests) and their associated applications on a single physical host
    * guest is an isolated software instance that is operable with other guests also on the host, taking advantage of the resource abstraction capabilities provided by the hypervisor to dynamically utilize resources from the host as needed

    * HW virtualization, or platform virtualization, refers to the creation of a virtual machine (VM) that acts like a real computer with an operating system

    * SW executed on these VMs is separated from the underlying hw resources
      - for example, computer that is running MS Windows may host a VM that looks like a computer with the Linux operating system; Linux-based software can be run on the VM


  -- Hypervisor
    * is small form-factor software, firmware, or hardware that gives the impression to the guest operating systems that they are operating directly on the physical hw of the host
    * allows multiple guest operating systems to share a single host and its hw
    * manages requests by VMs to access the physical hw resources of the host, abstracts them, and allows the VM to behave as if it is an independent machine

    * there are two types of hypervisors:

      - type 1
        -- commonly known as an embedded, native, or bare-metal hypervisor
        -- works directly on the host hw and can monitor operating systems that run above the hypervisor
        -- although small in size, its main task is sharing and managing hw resources between different guest operating systems

      - type 2
        -- installed after a traditional operating system and supports other guest operating systems running above it as VMs
        -- completely dependent on the host operating system for its operations

    * security
      -- hypervisor acts as the abstraction layer that provides the management functions for required hw resources among VMs
      -- compromising the hypervisor enables the hacker to gain control over the VMs as well as the host
      -- one example of a hypervisor attack is hyperjacking, which involves installing a rogue hypervisor that can take complete control of a host
        - this may be accomplished through the use of a VM-based rootkit that attacks the original hypervisor, inserting a modified rogue hypervisor in its place


  -- Software-Defined Infrastructure
    * infrastructure comprising Software Defined Networking (SDN), Software Defined Storage (SDS), and Software Defined Compute (SDC) can provide the opportunity for sw to control the entire computing infrastructure
      --> would enable the specification and configuration of the infrastructure resources that are needed by an application to run, as part of the application’s code

    * from a security perspective, it is important to recognize that software-defined infrastructure components such as SDN are not free of risk
    * some of the issues that were associated with the traditional architectures may still apply, and new risks due to new attack vectors (e.g., attack on SDN controller) should also be considered



  -- Containers
    * both VMs and containers allow multiple apps to share the same physical infrastructure, but they use different methods of separation
    * VMs use a hypervisor that provides hardware-level isolation of resources across VMs
    * each VM sees its own virtual hardware and includes a complete guest OS in addition to the app and its data
    * VMs allow different OSs, such as Linux and Windows, to share the same physical hardware

    * with containers, multiple apps share the same OS kernel instance but are segregated from each other
    * OS kernel is part of what is called the host operating system
    * host OS sits below the containers and provides OS capabilities to them
    * containers are OS-family specific
      - Linux host can only run containers built for Linux
      - Windows host can only run Windows containers
    * container built for one OS family should run on any recent OS from that family

    * VMs and containers each have their uses, and in fact some container deployments may use the VM as opposed to running directly on the hardware. This figure shows the VM deployment, a container deployment on “bare metal” (without VMs), and a container deployment within a VM

    * Container Risks
      - from a security perspective, the major risks to the core components of container technologies include:
        -- compromise of an image or container
        -- misuse of a container to attack other containers, the host OS, other hosts, and more

      - contributing to these risks are image risks, registry risks, orchestrator risks, host OS risks, and others
      - for example, image risks may be due to image vulnerabilities, image configuration defects, embedded malware, embedded clear text secrets, or use of untrusted images



  -- Trusted Computing Base (TCB)
    * summation of all the hw, fw, and/or sw components of a computer system that are critical to its security
    * this means that those parts of a computer system outside the TCB must not be able to misbehave in a way that would allow them to violate the security policy

    * in contrast, Trusted Platform Module (TPM) is a secure cryptoprocessor
      - used to secure hw by integrating cryptographic keys into devices, the specification for which is written by a computer industry consortium called Trusted Computing Group (TCG)

    * NIST SP 800-193 (Platform Firmware Resiliency Guidelines) describes TPM and its capabilities as
      - “Security coprocessor capable of securely storing and using cryptographic keys and measurements of the state of the platform. These capabilities can be used, among other things, to secure data stored on the system, provide a strong device identity, and to attest the state of the system"

    * some sw may take advantage of the TPM chip on the motherboard, an example of which would be Microsoft Bitlocker



  -- Database Security
    * DB is a prime target for application-level attacks
    * application-level attacks are used to exploit vulns in your data access code to gain access to the database
    * if all other attack vectors are closed, then the application’s front door becomes the path of choice for an attacker to steal, manipulate, and destroy data

    * control categories

      - encryption
        -- various precautions can be taken to secure databases
        -- one way is to encrypt sensitive data in db and find mechanisms to protect the encryption key, which helps mitigate risk of unauthorized disclosure for subjects without knowledge of the key
        -- many db management systems (DBMSs) support encryption
        -- it is recommended that software development teams familiarize themselves with the encryption capabilities of their DBMS
        -- the encryption solution required to achieve the specific security objectives may be as simple as using the capabilities offered by the DBMS or involve more complex solutions
        -- for example, real-time I/O encryption/decryption of data and logs may be achieved via Transparent Data Encryption (TDE), which may be sufficient depending on requirements
        -- for compliance with certain standards, FIPS 140-2-compliant cryptographic services modules/providers may have to be used

      - triggers
        -- db trigger is a reference to a special type of procedure that automatically executes in response to the occurrence of an event, including
          - INSERT, DELETE, UPDATE, CREATE, or even DROP among other data manipulations or data definition events
        -- triggers can be useful for logging events of special interest

      - views
        -- db view can be thought of as a dynamic table: one that can exclude certain columns from a table or one that spans multiple tables
        -- can be valuable from a security perspective
        -- when views are created, they may be constructed in such a way that they will obscure the schema, which also is valuable from a security perspective
        -- view is a way of portraying information in db
        -- best view for a particular purpose depends on the information the user needs

        -- view can be created by arranging the data items in a specific order, highlighting certain items, or showing only certain items
        -- for any db, there are a number of possible views that may be specified
        -- db with many items tend to have more possible views than db with few items
        -- often thought of as a virtual table, the view doesn’t actually store information but rather pulls it out of one or more existing tables
        -- although impermanent, a view may be accessed repeatedly by storing its criteria in a query
        -- as well as being a security mechanism, the view can also be used to hide complexity and support legacy code

      - privilege management
        -- every db user needs certain privileges to do their job
        -- granting appropriate privileges to appropriate users is the major task in privilege management
        -- privilege is the key to unlock db access, so managing privileges and roles is a vital responsibility of db admins
        -- admin also needs to manage privileges for a user role
        -- role is a named group of related privileges that is granted to users or other roles
        -- for db with multiple users where many require a similar set of access level, it is beneficial to use roles



  -- Programming Language Environment
    * there are many programming languages
    * some are functional, like R for statistics, some are object-oriented such as C#, which is popular for .NET framework development tasks, some are high-level and like scripting, while others are low level, such as Embedded C, and used for developing firmware

    * abstracted from language, framework, and business use case considerations, all sw dev efforts require environments for writing, running, testing, and maintaining source code
    * considerations often include compilers, linkers, packagers, debuggers, emulators, and many other productivity tools for sw devs
    * combination of modern dev tools into a unified development environment is referred to as an integrated development environment (IDE)

    * sw design constraints (rajoitukset), such as a mandated production operating system, may drive language and IDE requirements
    * some languages use an intermediary runtime environment such as Java JVM and .NET Core
      - using these frameworks allows developers to write source and deploy to multiple hosting operating systems
      - the intermediary runtime handles compatibility across platforms

    * automatic memory management and destruction of unneeded objects
      - managed code refers to code written for managed programming languages such as .NET and Java
      - these languages provide generalized ways to handle the details of memory management and garbage collection
      - garbage collection facilitates the destruction of instantiated dangling objects in memory that have gone out of scope but are not explicitly destroyed (closed), at the cost of a small amount of overhead
        -- this tradeoff ends error-prone tasks and allows the writing of more compact, readable and error-free programs

    * common language runtime (CLR)
      - CLR of .NET Framework has its own secure execution model that isn’t bound by the limitations of the operating system it is running on
      - unlike the old principal-based security, CLR enforces security policy based on where code is coming from rather than who the user is
        -- this model, called code access security, makes sense in today’s environment because so much code is installed over the internet and even a trusted user doesn’t know when that code is safe

      - CLR is VM component of MS’s .NET framework and is responsible for managing the execution of .NET programs
      - in a process known as just-in-time (JIT) compilation, the compiled code is converted into machine instructions that, in turn, are executed by the computer’s CPU
      - CLR provides additional services including memory management, type safety and exception handling
      - all programs written for the .NET framework, regardless of programming language, are executed by the CLR
        -- provides exception handling, garbage collection, and thread management
      - CLR is common to all versions of the .NET framework


    * java virtual machine (JVM)
      - is a CLR
      - is an example of application virtualization; it acts as an intermediary between the Java application code and the operating system
      - JRE is a bundle developed and offered by Oracle Corporation that contains the Java Virtual Machine (JVM), class libraries, and other components necessary to run Java apps and applets
      - JVM converts Java byte code into machine language and executes it
      - JRE is both a standalone and a web browser plug-in; the latter allows Java applets to run within a browser
      - without the JRE, Java programs and applets cannot be executed

      - security in Java relies on multiple mechanisms that include subjecting bytecode to verification, and enforcing defined security policies to prevent Java code from taking disallowed actions
      - this is accomplished through Java bytecode verifier and Java security manager
      - there is also a garbage collector, which facilitates memory management



  -- Operating System Controls and Services
    * os manages the resources of the system (e.g., CPU and memory) and provides a well-defined interface (i.e., through system calls) for its services
    * os comprises various subsystems, including those responsible for scheduling tasks, memory management, and I/O management
    * typical operating system consists of some form of bootloader, kernel, utilities, demons/services, shell/command interpreter, and graphical and command line interfaces

    * kernel
      - heart of the os
      - kernel services are available to processes when needed
      - modern OSs have clear distinction made between the kernel mode and the user mode
        -- implies that kernel code executes at the highest privilege level especially when compared with user application code

      - concept of security kernel: responsible for implementation of security functions such as the implementation of the reference monitor to address the security principle of complete mediation
        -- example: how your os validates whether certain operations (e.g., read/write/execute) requested by subject (process or user) on objects (e.g., file within the filesystem) should be allowed

    * program execution
      - one service of os is provisioning of the ability to create a process from a program and assign resources (e.g., memory space)
      - one may better relate to this by thinking about how binaries are loaded into the memory and execution is initiated

    * kernel interrupts
      - although discussion of kernel interrupts may only be relevant to certain types of sw developed today, it is important to understand that interrupts are essential facilities that may be used to signal that an event needs attention, allowing os to take an action in response to the occurrence of that event
      - example:
        -- using the interrupts to signal to the processor that a key has been pressed on the keyboard
        -- using this mechanism, one could understand that this would be analogous to “Tap me on the shoulder to get my attention” when some event occurs
        -- this could certainly present a performance advantage over constant polling of a device regarding the occurrence of some event

    * modes of operation
      - modern microprocessors support multiple modes, including at least
        -- supervisor mode to provide unlimited access to system resources
        -- user mode restricting any such access

    * memory management
      - kernel must manage resources for various processes and threads of execution
      - memory space of a process requires protection from interference by subjects
      - from a security perspective, memory is critical resource whose improper management/protection can lead to not only quality issues in software but also many common sw security vulns

    * virtual memory
      - less frequently accessed memory can be temporarily stored on a disk or other media to make that space available for use by other programs
      - through paging or segmentation techniques, os may support the use of virtual memory = usage of permanent storage space as an extension of the main memory
      - this may be accomplished utilizing various algorithms used by os for this purpose



    * most general-purpose modern OSs support multitasking and manage resources (e.g., allocate/deallocate) for processes that compete for these resources using various algorithms
    * time-sharing is a common method

    * support for disk access and filesystems is also among the common services of many OSs
    * particulars of the filesystem implementation may be different from one operating system to another, but reliability and optimization in utilization of the storage space are key requirements
    * centralized and distributed filesystems may have their own advantages and security challenges that sw security professionals need to understand

    * OSs may also include drivers for commonly used devices; otherwise, these drivers must be obtained from other sources (e.g., device manufacturer) before proper communication between the system and device can take place
      - assurance of the authenticity and integrity of all third-party sw, including drivers, should always be a consideration for information security professionals



- Threat Modeling
  -- process of threat modeling entails raising and answering questions about protecting assets
  -- threat modeling’s application to sw design has been gaining widespread adoption in recent years
  -- during threat modeling exercises, security practitioners systematically identify and rate the threats that are most likely to affect business critical systems
  -- threats are prioritized by impact and component criticality and addressed accordingly

  -- threat model can be created for a new or preexisting sw or system
  -- threat modeling activities can become complex and resource intensive, so organizations should plan accordingly
  -- sometimes organizations will opt to scope threat modeling efforts to certain components based on business mission or constraints



  -- Understand Common Threats
    * cyberattacks have spared no industry or sector
    * organizations of all sizes, and in virtually all industries (financial, healthcare, manufacturing, energy, retail, ...) recognize cyberthreats as a key area of concern
    * malware and insider threats remain among the biggest threat sources

    * threat modeling is one of the most important security-relevant activities
    * threat modeling process
      - identifies threats that can do harm to the system
      - provides an estimate of potential harm
      - identifies the need for security features, careful code review and security testing

    * CSSLP needs to understand common and third-party threats as well as to perform an attack surface analysis
    * we will discuss threats in the context of sw security risks, including
      - spoofing or tampering by malicious actors such as hackers and criminals
      - misuse by nonmalicious actors such as insiders
      - various types of malicious sw such as ransomware

    * in addition, there are many other factors that could also have an adverse impact on the security objectives of confidentiality, integrity, and availability, few examples are:
      - sw reliability issues, such as a mobile messaging application that delivers a message to the wrong recipient
      - natural disasters, such as earthquakes, that adversely impact availability
      - circumstantial spikes on load and volume, such as the increase in the use of virtual communication services and infrastructure due to a pandemic that requires social distancing

    * while it is impossible to anticipate all possible threats, organizations need to understand most relevant threats to them and recognize that threats may be at any of various levels:
      - the organization level
      - mission/business process level
      - information system level



  -- MITRE ATT&CK Framework
    * previous domain MITRE’s Adversarial Tactics, Techniques, and Common Knowledge (ATT&CK) was introduced
      - ATT&CK was created out of a need to document adversary behaviors
      - ATT&CK is largely a knowledge base of adversarial techniques: breakdown and classification of offensively oriented actions that can be used against platforms such as Windows
      - unlike prior work in this area, the focus isn’t on the tools and malware that adversaries use but on how they interact with systems during an operation

      - ATT&CK organizes these techniques into a set of tactics to provide context for the technique
      - each technique includes information that’s relevant to both:
        -- red team or penetration tester to understand how a technique works
        -- defender to understand the context surrounding events or artifacts generated by a technique in use

    * use cases
      - ATT&CK framework can help organizations mature their security programs and enhance their strategies to prevent, detect, and respond to threats
      - various use cases for ATT&CK include:
        -- Adversary Emulation
        -- Red Teaming
        -- Behavioral Analytics Development
        -- Defensive Gap Assessment
        -- SOC Maturity Assessment
        -- Cyber Threat Intelligence Enrichment

      - example: using ATT&CK for adversary emulation
        -- in red teaming, adversary emulators construct a scenario to test certain aspects of an adversary’s tactics, techniques, and procedures (TTPs)
        -- red team then follows the scenario while operating on a target network to test how defenses might fare against the emulated adversary

        -- frameworks such as CALDERA, which uses the ATT&CK model and consists of a core system and plugins that provide additional functionality, may be used to build and launch adversary profiles to identify susceptible (altis) areas, test defenses, and train blue teams



  -- Advanced Persistent Threat and Insider Threat
    * APT definition by NIST:
      - “an adversary with sophisticated levels of expertise and significant resources, allowing it through the use of multiple different attack vectors (e.g., cyber, physical, and deception), to generate opportunities to achieve its objectives”
      - these are typically “to establish and extend its presence within the information technology infrastructure of organizations to continually exfiltrat[e] information and/or to undermine or impede critical aspects of a mission, program, or organization, or place itself in a position to do so in the future....”

      - NIST adds, “APT pursues its objectives repeatedly over an extended period...adapting to a defender’s efforts to resist it, and with determination to maintain the level of interaction needed to execute its objectives”

    * adversaries include nation states, activist groups, and criminal groups
    * published industry reports have identified technology, energy, financial, healthcare among sectors frequently targeted by adversaries
    * tactics deployed by the threat actors constantly change and evolve

    * Insider Threat
      - should not be strictly interpreted as employees who intend to directly harm the organization through theft, sabotage, or other means
      - statistics and reports from industry indicate that ignorance and negligence (and not necessarily malicious intentions) are also a major cause of security breaches and incidents
      - insider threat could include employees or former ones, contractors or business associates, and those with inside information on organizations’ processes, practices, or data
      - simply put, these insiders are generally more aware of potential abuse vectors



  -- Common Malware
    * malicious sw (malware) remains a significant threat, and malware incidents are common
    * NIST SP 800-83 provides guidance for the prevention and handling of malware incidents
    * malware comes in many forms (viruses, worms, Trojans, rootkits) and symptoms of malware infection vary
    * common symptoms include increased CPU usage, slow operation of the computer, and modified or deleted files

    * viruses
      - have the ability to replicate itself (infect other files) and target binary executable files
      - may spread via the network or through infected removable media, among other possibilities
      - when the infected host file is executed, the virus is executed as well
      - event (e.g., user interaction) will trigger the delivery of the payload
      - some viruses are metamorphic (they can evolve into variants by modifying themselves)
      - come in many varieties
      - depending on the particular virus, they may infect files, boot records, or other parts of the computer

    * worms
      - also considered self-replicating, worms spread throughout a network
      - unlike viruses, they do not need the support of a file and event to trigger the delivery of the payload

    * trojans
      - program designed for malicious activities that runs under the disguise of a legitimate/benign program
      - do not reproduce (in contrast to viruses and worms)
      - creation of backdoors is common with Trojans

    * ransomeware
      - type of malware that typically attempts to block access to files (e.g., through cryptographic means) for the purpose of extortion and ransom payment

    * spyware/adware
      - designed to gather sensitive information or support advertising as part of their activities respectively

    * bots
      - shortened form of the word robot, bots are processes that normally automate tasks and services
      - may not be created for malicious activities, but malicious bots can become part of a remote command and control center for launching flooding (DoS) attacks
      - botnets are an entire network of compromised systems

    * combating the malware threat
      - scanning files on end points for patterns that are indicative of known malware (unique code sequences for particular virus) is common among endpoint protection technologies, but more is needed
      - also included in best practices for addressing the malware problem are creating awareness for the user base and keeping up with security patches

      - as malware attacks gain in sophistication, so do the technologies and methods for combating the threat
      - many scanners have been introduced over the years, and URL/domain blocking services are being used to combat malware

      - websites, online services, and APIs have also been created to aggregate many scan engines (e.g., VirusTotal)
      - these utilities increase the chances of detecting malware that may have been missed by one engine working alone


  -- Third-Party Software Threats
    * risk associated with third-party sw components and libraries will be discussed in later domains
    * for now, recognize that third-party sw always carries a security risk
    * when sw is developed outside the organization’s control and without sufficient visibility into development life cycle processes, there are risks that need to be addressed
      - this is true of purchased off-the-shelf sw as well as custom sw that has been developed through outsourcing/offshoring arrangements.

    * to reduce the threat of third-party software, organizations should:

      - remove all software that is not used/needed
        -- includes removing or disabling all sw features that are unlikely to be used, ultimately reducing the attack surface
        -- there are tools that can help with this task, but most organizations do not follow this path

      - work with vendors that have proper and defined patch management release cycles and inform customers of discovered vulnerabilities and availability of updates to address them
        -- 3rd-party patch mgmnt solution should be implemented, which should provide assurance that 3rd-party patches are frequently identified, tested, and applied subject to patch management cycles

       - develop a clear understanding of all libraries and components that make up the sw developed by third-party vendors
         -- may partially be achieved by reviewing the licensing arrangement of the vendor
         -- for assurance, comprehensive list of libraries and components must be obtained from the vendor

       - maintain an inventory and list of authorized sw in the organization and restrict the usage of unauthorized third-party software on corporate IT assets

       - reduce the attack surface by blocking ports and restricting connections and communications on the network and between hosts
         -- can be done by using web application firewall (WAF) functionality where appropriate in layered defenses


  -- Threat Modeling Concepts and Terminology
    * according to the Homeland Security Systems Engineering and Development Institute (HSSEDI)™ Operated by The MITRE Corporation:
      - “a model is an abstract representation of some domain of human experience, used
        (1) to structure knowledge
        (2) to provide a common language for discussing that knowledge
        (3) to perform analyses in that domain”

    * HSSEDI continues
      - “variety of terms are used in threat modeling, including threat, threat actor, threat event, threat vector, threat scenario, campaign, attacker, attack, attack vector, attack activity, malicious cyber activity, and intrusion. Different threat modeling approaches define these terms differently, due to assumptions about the contexts and purposes for which they will be used. Terminology related to threat is embedded in a larger setting of terminology about risk”

    * threat modeling works to identify, communicate, and understand threats and mitigations within the context of protecting something of value
    * threat modeling can be applied to a wide range of entities, including sw, applications, systems, networks, distributed systems, IoT, business processes, and more
    * there are few technical products that cannot be threat modeled
    * threat modeling can happen at any stage of development, but it is preferably done early so that the findings can inform the design


  -- Software Threat Modeling Tools, Methodologies and Process
    * most sw systems face various types of threats, and the number of threats continues to grow
    * threat modeling is a critical step in securing sw against attacks
    * process of threat modeling involves:
      - first identifying potential threats to the application
      - then ranking these threats by risk
      - finally selecting appropriate countermeasures or mitigations for the threats

    * here are some guidelines for sw threat modeling:

      - select threat modeling participants
        -- there are no preset rules as to who may or may not participate in a threat modeling exercise
        -- participating members may be security engineers, architects, developers, or other sw stakeholders (even customers may be involved) as deemed appropriate by organization and project

      - threat model early in the life-cycle
        -- although threat modeling can be performed during any stage of dev, for its findings to inform and influence the design of the sw, it would be best to conduct it as early as possible
        -- threat modeling may be considered a design phase security activity, ideally taking place as soon as the architecture is established
        -- keep in mind that identifying and remediating potential issues at later stages can be much more costly, if doing so is even possible at all

      - determine what to threat model
        -- organizations may threat model new applications or existing applications
        -- they may choose to threat model an entire application or just specific components/modules of the application (e.g., the authentication module)
        -- it would be wise to start small, gain experience and do it successfully, and then increase the scope of the threat model

      - document the threat model
        -- as part of creating threat model, documentation is also produced
        -- this documentation can be useful for those who perform code reviews, for those who create security test cases, and others
        -- simply put, the work that is accomplished and the documents produced as part of a threat modeling exercise will benefit activities in other phases of the data life cycle as well

      - update the threat model
        -- threat model should be reviewed and updated as required to reflect the changes in application design and functionality
        -- as potential threats are discovered and the implementation details of the application become known, the threat model should also be updated



  -- Threat Modeling Tools
    * industry has a variety of threat modeling tools available to the sw community
    * the following are three of the most widely used ones:

      - Microsoft threat modeling tool
        -- introduced in the late 2000s
        -- makes threat modeling easier for all developers through a standard notation for visualizing system components, data flows, and security boundaries
        -- helps threat modelers identify classes of threats they should consider based on the structure of their software design
        -- designed with non-security experts in mind, making threat modeling easier for all developers by providing clear guidance on creating and analyzing threat models

      - OWASP Threat Dragon
        -- introduced in the late 2010s
        -- used to create threat model diagrams and to record possible threats and decide on their mitigations

      - Threatspec
        -- open source
        -- main value can be attributed to helping to close the gap between dev and security by bringing the threat modeling process further into dev process
        -- this is achieved by having devs and security engineers write threat specifications alongside code, then dynamically generating reports and data-flow diagrams from code
          * this allows engineers to capture the security context of the code they write, as they write it



  -- Threat Modeling Methodologies

    * STRIDE

         Threat                   Property Violated        Threat Definition
    -----------------------------------------------------------------------------------------------------------------------
    S    Spoofing identity        Authentication           Pretending to be something or someone other than yourself
    T    Tampering with data      Integrity                Modifying something on disk, network, memory, or elsewhere
    R    Repudiation              Non-repudiation          Claiming that you didn't do something or were not responsible; can be honest or false
    I    Information disclosure   Confidentiality          Providing information to someone not authorized to access it
    D    Denial of service        Availability             Exhausting resources needed to provide service
    E    Elevation of privilege   Authorization            Allowing someone to do something they are not authorized to do



    * DREAD

      - no longer widely used, developed by MS
      - based on STRIDE, but using ranking 1-10 based on predefined criteria (attack, attack freq, effort to exploit, expertise required)
      - sample assessment:

        DREAD Criterion         Score      Comments
        --------------------------------------------------------------------------------
        Damage                  8          Work disruption but no data loss
        Reproducibility         10         Failure every time...
        Exploitability          7          Not very easily conducted as it requires...
        Affected Users          10         Everyone will be affected
        Discoverability         10         Every threat will be...


    * PASTA (Process for Attack Simulation and Threat Analysis)

      - risk centric threat modeling framework
      - can also help with identifying compliance gaps
      - contains seven steps:

        1) Define objectives
          -- identify business objectives
          -- identify security and compliance requirements
          -- business impact analysis

        2) Define technical scope
          -- capture the boundaries of the technical environment
          -- capture infrastructure/application/software dependencies

        3) Application decomposition
          -- identify use cases/define app entry points & trust levels
          -- identify actors/assets/services/roles/data sources
          -- data flow diagramming (DDs)/trust boundaries

        4) Threat analysis
          -- probabilistic attack scenarios analysis
          -- regression analysis on security events
          -- threat intelligence correlation and analytics

        5) Vulnerability & weakness analysis
          -- queries of existing vulnerability reports & issues tracking
          -- threat to existing vulnerability mapping using threat trees
          -- design flaw analysis using use and abuse cases
          -- scorings (CVSS/CSS)/enumerations (CWE/CVE)

        6) Attack modeling
          -- attack surface analysis
          -- attack tree development | attack library mgt.
          -- attack to vulnerability & exploit analysis using attack trees

        7) Risk & impact analysis
          -- qualify & quantify business impact
          -- countermeasure identification and residual risk analysis
          -- identify risk mitigation strategies

      - attack trees
        -- diagrams depicting (kuvata) potential attacks on a system
        -- goal: easy and simple to undestand, but often requires thorough understanding of system details and security objectives
        -- root of the tree is the goal of the attack (each goal is represented as separate tree)
        -- leaves are ways to achieve that goal


    * other methodologies

      - Trike
        -- methodology used to perform threat modeling
        -- focuses on a requirements model that ensures that each asset’s level of assigned risk is classified as acceptable by the system’s stakeholders

      - ATASM
        -- Architecture, Threats, Attack Surfaces, and Mitigations (ATASM)
        -- threat-modeling approach that highlights the importance of structural understanding of a system for the purpose of threat modeling (architecture)

      - Threat Library/List Approach
        -- using a predefined set of common and prevalent threats, a team can identify instances in the product by tracking the triggers
        -- for example, cross-site scripting might be present if a product offers a web interface and no input validation and output sanitization
        -- the team is free to evolve the threat library as technologies and frameworks change

      - Lightweight/Rapid Threat Modeling
        -- range of processes that use
          * lighter-weight variations of other methodologies
          * additional approaches that use quick classifications and other ways to achieve a similar result in less time for less critical systems



  -- Threat Modeling Process
    * sw threat modeling processes are expected to mature, and tools and technologies are expected to evolve and improve over time
    * described by OWASP:
      - "technical steps in threat modeling involve answering questions:
        -- What are we working on?
        -- What can go wrong?
        -- What will we do with the findings?
        -- Did we do a good job?

      - the work to answer these questions is embedded in some sort of process, ranging from incredibly informal Kanban with Post-its on the wall to strictly structured waterfalls
      - the effort, work, and timeframes spent on threat modeling relate to the process in which engineering is happening and products/services are delivered
      - idea that threat modeling is waterfall or ‘heavyweight’ is based on threat modeling approaches from the early 2000s
      - modern threat modeling building blocks fit well into agile and are in wide use"



  -- Threat Modeling in Agile Environments
    * threat modeling in agile environments may have its own challenges and require the adoption of methods and processes that are better suited for such environments
    * for one thing, consider that threat modeling is an activity that primarily belongs to the design stage, and that in agile implementations the design stage may not be as clearly defined as it normally would be with waterfall-like methodologies

    * this challenge has been clearly pointed out by SAFECode:
      - “a pitfall may lie in trying to create threat models for each sprint and then trying to merge them to threat-model the entire system
      - at the beginning of a set of sprints, not enough is known to threat-model the whole system, because every sprint may affect the threat model"


    * SAFECode also offers guidelines to this effect, summarized here

      - Sprint 0/Sprint Planning
        -- starting and completing the release threat model based on the overall design

      - Sprints 1 through N
        -- update the thread model only if new user stories or source code changes invalidated the original model -> release

      - Release Definition of Done (DoD)
        -- make sure that the thread model reflects the system state, is up-to-date and controls are implemented



  -- Threat Modeling in DevOps Environments
    * challenges for threat modeling in DevOps environments stem from frequency of sw changes and absence of traditional change review/management processes, among other development characteristics
    * SAFECode notes:
      - “as DevOps blurs the line between sw changes and infrastructure changes, there are additional triggers for threat modeling that historically would have been covered by the infrastructure change management process

      - two most prevalent (yleinen) additional triggers (for threat modeling) in DevOps are:

        1) alteration of the sw deployment process
          -- most DevOps environments include tools that automatically build components based on activity in a source code repository, and then deploy those components to production
          -- compromising any portion of this process can lead to a full compromise of the service, so any change to the authentication/authorization, data flow and elements involved in the deployment automation warrants a threat model review

        2) changes to executing env of the services, including new ports/protocols, alterations to app server or OS config, account and permissions alterations, etc
          -- there are numerous resources describing what events should trigger a review during a change management process, and those triggers map equally well to threat modeling”



  -- Attack Surface Evaluation
    * in threat modeling activity, attack surface analysis exercise is about reducing attack surface of the sw and hence the opportunity for sw compromise -> lower risk
    * this objective requires a clear understanding of the attack surface of the sw through proper analysis
    * attack surface analysis is often conducted by security architects and penetration testers
    * development teams need to understand and monitor the attack surface while designing, building and changing the system

    * rationales (perustelut) for this activity include the benefits gained through:
      - identification of functions and parts of the system that require review/testing for security vulns
      - identification of high-risk areas of code needing protection through various defense layers
      - identification of changes to the attack surface, triggering the need for some kind of threat assessment

    --> primary purpose of attack surface analysis is to identify changes to the attack surface, which then triggers the need for some kind of threat assessment


  -- Attack Surface Analysis Process
    * attack surface analysis is about mapping the parts of a system to be reviewed and tested for security vulns
    * point of attack surface analysis is to understand the risk areas in an application, to make developers and security specialists aware of
      - what parts of the application are open to attack
      - find ways of minimizing this
      - to notice when and how the attack surface changes and what this means from a risk perspective

    * attack surface analysis process consists of:

      - defining attack surface of an application:
        -- as presented by OWASP, attack surface describes all the different points where an attacker can breach a system and where in that system data can be pilfered (ryöstää)
        -- the following constitutes the attack surface of an application:

          * sum of all paths for data/commands into and out of the application
          * code that protects these paths, including resource connection and authentication, authorization, activity logging, data validation, and encoding
          * all valuable data used in the application, including secrets and keys, intellectual property, critical business data, personal data and PII
          * code that protects this data, including encryption and checksums, access auditing, and data integrity and operational security controls

        -- this model is then overlaid with the types of users, roles, and privilege levels that can access the system
        -- bearing in mind that complexity can increase as the types of users increase, focus should remain especially on the two extremes:
          * unauthenticated, anonymous users
          * highly privileged admin users (e.g., database administrators and system administrators)

      - identifying and mapping attack surface
        -- building a baseline description of the attack surface can be accomplished in pictures and notes by reviewing design and architecture documents from attacker perspective, reading the source code, and identifying points of entry/exit using scan tools
        -- although there may be many entry points, all need to be identified
        -- few such entry points are:
          * user interface forms and fields
          * HTTP headers and cookies
          * APIs
          * databases, files and local storage
          * Run-time arguments

        -- since the total number of attack points can add up quickly, it may make the task more manageable if the model is broken into types based on function, design, and technology
        -- examples of such types include login/authentication entry points, admin interfaces, and APIs, among others

      - measuring and assessing attack surface
        -- the work that has been done up to this point and the map of the attack surface that has been created enables the identification of high-risk areas
        -- measuring the attack surface of the application and tracking changes to it over time is another part of the attack-surface analysis process mentioned earlier
        -- measurement of the attack surface over time may be accomplished through Relative Attack Surface Quotient (RASQ)
          * using this method, an overall attack surface score can be calculated
          * as changes are made to the system, this score should be remeasured
          * clearly, it is desirable for the attack surface to shrink over time, but adding new code (additional functionality) will likely increase the attack surface

      - managing the attack surface
        -- once there is a baseline understanding of the attack surface, it can be used to incrementally identify and manage risks as changes are made to the application
        -- ask yourself the following questions:
          * What has changed?
          * What are you doing differently?
          * What holes might you have opened?

        -- typically, app attack surface will increase over time as more interfaces are added, components age, users and roles are expanded, and unintended system integrations occur
        -- attack surface can be reduced by
          * simplifying the model (reducing the number of user levels, for example, or not storing any unnecessary confidential data)
          * turning off unused features and interfaces
          * introducing operational controls, such as a software-based web application firewall (WAF) and real-time application-specific attack detection


  -- Attack Surface Tools
    * while vuln scanning tools can help with the attack surface evaluation process (identifying entry points into sw), specialized tools can be used, including:

      - Microsoft Attack Surface Analyzer
        -- aids with the identification of potential security risks introduced by changes to an operating system’s security configuration by identifying changes in key areas, including
          * file system
          * user accounts
          * system services
          * network ports (listeners)
          * system certificate stores
          * windows registry

      - OWASP Attack Surface Detector
        -- uncovers the endpoints of web app, the parameters these endpoints accept, and the data type of those parameters
        -- this includes the unlinked endpoints a spider won’t find in client-side code, or optional parameters totally unused in client-side code
        -- has the capability to calculate the changes in attack surface between two versions of an application

      - Sandbox Attack Surface Analysis Tool
        -- set of tools to analyze Windows sandboxes for exposed attack surface
        -- allows a user to perform this analysis, extracting accessible resources and services and providing low-level inspection of the OS


  -- Threat Intelligence
    * organizations utilize threat information to enhance their security posture
    * according to NIST
      - “threat Intelligence is threat information that has been aggregated, transformed, analyzed, interpreted, or enriched to provide the necessary context for decision-making processes"

    * threat intelligence matures the understanding of cyberthreats by teaching us the methods used by malicious actors for deploying attacks and indicators that may represent an attack
    * additionally, threat intelligence is used to identify the threats that are relevant to organizations’ assets

    * threat intelligence starts with gathering of the information about existing or upcoming threats
    * such information is typically obtained through organizations’ internal sources, subscription to security alerts from advisories and bulletins, and information sharing with communities

    sharing communities               cyber                                                              risk mitigation/assessment
    security alerts    ---------->    threat          ----->   applies to organization's assets  ----->  attack prevention/deterrence/detection      ------->   threat intelligence report
    internal sources                  intelligence                                                       tool configuration/recommendation


      - internal sources of cyber threat information
        -- organizations capture security information that can be catalogued to produce threat intelligence
        -- for example, firewall logs may be used to identify IP addresses for malicious actors, or email systems may be used to determine attributes of phishing attacks
        -- one main challenge with using organizations’ internal data for studying threats is the protection of sensitive information
        -- threat information should be gathered in accordance with regulatory, privacy, legal and security requirements
        -- special consideration may apply to threat information that is shared with other organizations

      - security alert for cyber threat information
        -- risk advisories such as the US Computer Emergency Readiness Team (US CERT), send their subscribers notifications about upcoming or ongoing cyberthreats and provide repositories for advisories on existing threats
        -- organizations typically subscribe to multiple sources to cover a range of vulnerabilities
        -- for example, vendor security bulletins may include more detailed information regarding a vendor’s provided platform

      - joining a community for sharing cyber threat information
        -- organizations join communities to complement their knowledge of threat intelligence
        -- typically they join a community based on their characteristics or common interests
        -- for example, selecting a community based on geographical region may facilitate joining the regular meetings
        -- similarly, selecting a community based on an industrial sector, such as the energy sector, may be more closely focused on threats that are common in that industry
        -- one important consideration for joining a sharing community is trust
        -- organizations’ information-sharing rules should take into consideration the trustworthiness of the community members

      - cyber threat intelligence program
        -- organizations should have a comprehensive cyberthreat intelligence program consistent with organizations’ security policies
        -- typically, such programs define the goals and objectives for information sharing as well as criteria for selecting and joining communities
        -- NIST Special Publication 800-150 is a guide to cyberthreat information sharing



- Architectural Risk Assessment and Design Reviews
  -- to conduct due diligence (asianmukaista huolellisuutta) during the design stage
    * architecture should be assessed for risks
    * designs should undergo board approvals that include comprehensive design reviews to check for quality, security, safety, and regulatory compliance


  -- Architectural Risk Assessment (ARA)
    * secure design principles include considerations for
      - least privilege
      - defense in depth
      - fail secure
      - many more that must be addressed at design time

    * if design has been completed and app coded, it's more difficult and expensive to address missed opportunities for building cyber resiliency into sw for the functions of
      - authentication
      - authorization
      - accountability
      - authenticity
      - nonrepudiation


    * ARA is security analysis framework that includes tools and techniques intended for applications and services
    * when implemented correctly, ARA helps security practitioners to proactively develop a cybersecurity risk management plan
    * results of ARA planning should highlight sw application security gaps
    * ARA provides a mechanism to select business, impact, and cost-based mitigations for security risks

    * ARA seeks to answer the following questions:
      - are security assessments properly aligned with architecture levels?
      - are the defense-in-depth mitigations adequate?
      - what coverage does the security solution provide?

    * to address these three questions, the ARA essentially follows a six-step process that is divided into three major areas of focus:
      - architectural discovery
      - threat identification
      - risk analysis

      - ARA steps:
        1) create security objectives
        2) identify primary use cases
        3) develop network/application diagrams
        4) develop threat model
        5) add abuse/misuse cases
        6) develop a threat mitigation plan


  -- Secure Design Principles, Patterns, and Tools
    * consider the following levels and layers of architecture
    * consider security principles and practices within the engineering design phase

    * secure design principles, as discussed earlier, provide the guidance needed by architects for producing secure applications by design (from the foundation)
    * to build secure applications from the foundation, naturally the design must support and enforce the security tactics that developers are tasked to implement
    * if the security weakness is in the design, it would be difficult to compensate for that weakness during implementation of the security tactics

    * design patterns are described as descriptions and templates of reusable solutions that address problems occurring frequently during design
    * as described by Carnegie Mellon University’s Software Engineering Institute (SEI):
      - “secure design patterns address security issues at widely varying levels of specificity ranging from architectural-level patterns involving the high-level design of the system down to implementation-level patterns providing guidance on how to implement portions of functions or methods in the system"

    * general classes of patterns described by SEI include:

      - architectural level patterns
        -- maintain focus on high-level allocation of responsibilities between different components of system and define interaction between those high-level components

      - design level patterns
        -- describe how to design and implement pieces of high-level system component
          --> they address problems in the internal design of a single high-level component, not the definition and interaction of high-level components themselves

      - implementation level patterns
        -- address low-level security issues
        -- are usually applicable to the implementation of specific functions or methods in the system
        -- address the same problem set addressed by the CERT Secure Coding Standards and are often linked to a corresponding secure coding guideline



  -- Design Verification
    * considering the objective of security architecture and design analysis, verification of the design to develop level of confidence that the design addresses the requirements would involve:
      - logic analysis, including the algorithms
      - data analysis, including the intended data usage
      - interface analysis, including component interfaces
      - constraint analysis, including restrictions on hw, sw, and the env

    * verification of the correct implementation of design through secure code reviews, inspections and walkthroughs is recommended
    * secure code reviews conducted by personnel without direct involvement in the development of sw can be valuable
    * reviews may be done during and at the end of the development phase
    * objective is to ensure that security design concepts and specifications are met

    * informal secure code reviews are conducted on as-needed basis, occurring at a higher frequency than formal code reviews
    * the term informal may mean different things to different teams, but in this context, it is a reference to selecting reviewers and providing the material, which may be as informal as pseudo-code or handwritten documentation, to be reviewed

    * formal secure code reviews are done at a lower frequency than informal reviews and at the end of the development phase for software components
    * this may involve the appointment of a formal review group and mediated sessions

    * secure code inspection or walkthrough suggests detailed examination of source code with objectives that include the identification of issues or embedded malicious code
    * the inspection or walkthrough, when conducted by a team, may involve participation by members of the development, security engineering, and quality assurance groups



- Nonfunctional Security Properties and Constraints
  -- understanding business-derived security properties and constraints for sw design is essential to ensuring sw models will satisfy objectives when implemented and integrated later in SDLC
  -- security practitioners must be interdisciplinary
  -- basic knowledge of systems engineering is required for modeling effective security properties and constraints

  -- Nonfunctional Security Properties and Constraints
    * to understand nonfunctional security properties and constraints, one must first understand difference between functional and nonfunctional requirements from a system engineering perspective
    * when you think of typical sw requirements, you likely think about functional requirements such as what a product does or what features exist
    * nonfunctional requirements describe system characteristics, or properties, and are often referred to as quality attributes

    * nonfunctional requirements define how the system should perform
    * every sw system will have a set of properties (perhaps more than a dozen) including specialty engineering items such as safety, reliability, and security
    * the term property is often used interchangeably with quality attributes, nonfunctional requirements, and specialties

    * ISO/IEC 25000 series defines nonfunctional requirements as system quality and sw quality requirements
    * standard independent, nonfunctional requirements establish constraints for sw system functionality
    * these requirements describe expected system operations, including security-related concerns for performance, confidentiality, and integrity of data

    * sw security properties are interrelated
    * building more secure authentication system may increase the difficulty for users to log in, but the result is higher confidence in user sessions and a safer system
    * increasing sw system resiliency may require increased costs for hw, including a load balancer and the inclusion of redundant components
      - therefore, a correlation (another property) exists between resiliency and sw system costs
      - on a positive note, resiliency might be inversely related to emergency maintenance costs, which would constitute another property

    * security practitioners should integrate with systems engineers to inform design properties related to specialties
    * consider the following functional and nonfunctional requirement pair

      - functional sw requirements
        -- sw system’s data in transit protocol or mechanism must be able to correct errors for dropped units of data

      - nonfunctional sw requirements
        -- sw systems data transport will reattempt failed transmissions up to five times within 0.3 seconds for critical data

    * this requirement example describes the security and reliability quality attributes
    * property exists between retransmissions, time to resend, and failed data unit delivery
    * sw system has the constraint of either a successful retransmission or five additional attempts within 0.3 seconds for critical data
    * the 0.3 seconds is a threshold


- Secure Operational Architecture
  -- security practitioners must consider implications for securing operational architecture in addition to system-specific properties and constraints
  -- design decisions will impact production operations such as deployment topology and operational interfaces
  -- depending on business requirements for CIA, CI/CD may be required to facilitate the evolution of sw securely throughout SDLC

  -- Stakeholders in Secure Operational architecture’s                                                                                                                                  
    * enterprise security design guidance generally flows from business leadership down to technical leadership and eventually resides with sw and security architects
    * following diagram (NIST 800-160 vol.1 and ISO/IEC 15288 guidance) depicts the organizational stakeholders who play vital role in orchestrating a secure operational architecture


      CEO                               Securing Digital
                                         Transformation

    Business                          Business & Security
   Leadership                              Alignment

   ---------------------------------------------------------------------------------------------------------------------------------------------------------------

      CIO    CISO                      Security Strategy

  Technical Leadership

   ----------------------------------------------------------------------------------------------------------------------------------------------------------------


   Architects                          Architecture & Policy         Secure Operational Architecture                             Initiative Planning / Execution
                                                                                                                            privileged access, OWASP top 10, zero trust
                                       Technical Planning            Security Documentation




  -- Security Operations
    * analyzing security roles to better understand security operations is a worthwhile exercise
    * based on business analysis, opportunities are identified which could constitute developing new sw application

    * to capitalize on opportunities, plans must be drafted that contain security governance and compliance considerations
    * for simplicity, consider a linear process; the application implementation occurs following planning activities
    * implementation includes cyber-resilient designs and security control considerations that together form the basis for sw security preventions

    * after the application is implemented, it can be integrated into business operations
    * once operational, the response to threats in production will constitute (muodostaa) security operations
    * security practitioners must consider the impact of design decisions made earlier in the systems engineering process on security operations
    * see the orchestration diagram below to better highlight security roles and dependencies for security operations



        PLAN                         BUILD                            RUN
    ===================================================================================
    |Governance|                  |Prevention|                     |Response|

    Security Leadership -------------------------------------------------------------->


    Security Architect --------------------------------------------------------------->


               <--------------  Platform Security ------------->  IP Ops &
    Security                        Engineer                       DevOps
   Compliance
               <--------------   App Security ---------------->   Security
                                   Engineer                      Operations



  -- Continuous Integration and Continuous Delivery
    * DevSecOps covers development, security, and operations considerations
    * DevSecOps, when implemented, blurs the separation between building, testing, and deploying source code updates via CI/CD pipelines

    * the inclusion of CI/CD efforts has drastically improved the security and quality of sw while introducing new threat vectors related to sw pipeline workflows
    * security practitioners must ensure sw pipelines include adequate assurances for developing cyber-resilient sw while maintaining the business-driven level of security for development operations




Adversarial Machine Learning (AML) - The process of extracting information about the behavior and characteristics of an ML system and/or learning how to manipulate the inputs into an ML system.
Architectural Risk Assessment (ARA) - A security analysis framework that includes tools and techniques intended for applications and services.
Authentication - The act of identifying or verifying the eligibility of a station, originator, or individual to access specific categories of information.
Data Loss Prevention - The ability to identify, monitor, and protect data in use, data in motion, and data at rest through deep packet content inspection, contextual security analysis of transaction, within a centralized management framework. Source: CNSSI 4009-2015
DREAD - A risk ranking (rating) methodology. Frequently used with STRIDE, the acronym stands for damage potential, reproducibility, exploitability, affected users, and discoverability.
Hypervisor - The virtualization component that manages the guest OSs on a host and controls the flow of instructions between the guest OSs and the physical hardware. Source: NIST SP 800-125
Industrial Control System (ICS) - Computer-controlled systems that monitor and control industrial processes in the physical world.
Internet of Things (IoT) - The interconnection of electronic devices embedded in everyday or specialized objects, enabling them to sense, collect, process, and transmit data. IoT devices include wearable fitness trackers, “smart” appliances, home automation devices, wireless health devices, and cars—among many others. Source: https://www.nist.gov/programs-projects/nist-cybersecurity-iot-program
Machine Learning (ML) - The ability of computers to learn from provided data without being explicitly programmed for a particular task.
Multifactor Authentication (MFA) - An authentication process that involves using two or more distinct instances of the three factors of authentication for identity verification -- (something you know, something you have, something you are). The more factors used to determine a person’s identity, the greater the trust of authenticity.
Network-Attached Storage (NAS) - A file-level computer data storage server connected to a computer network providing data access to a heterogeneous group of clients.
Single Sign-On (SSO) - An authentication mechanism that allows a single identity to be shared across multiple applications.
Software Security Constraints - In system engineering: a restriction, limit, or regulation imposed on a product, project, or process. Source: ANSI/EIA 1998
Software Security Properties - In system engineering: system quality and software quality requirements. Source: ISO/IEC 25000
Storage Area Network (SAN) - A network that provides access to consolidated, block-level data storage.
STRIDE - A threat modeling and categorization methodology that classifies known threats according to the kinds of exploit that are used (or motivation of the attacker).  The acronym stands for spoofing, tampering, repudiation, information disclosure, denial of service, and elevation of privileges.
Supervisory Control and Data Acquisition (SCADA) System - Used to control and monitor physical processes, examples of which are transmission of electricity, transportation of gas and oil in pipelines, water distribution, traffic lights, and other systems used as the basis of modern society.
Threat Intelligence - Threat information that has been aggregated, transformed, analyzed, interpreted, or enriched to provide the necessary context for decision-making processes. Source: NIST SP 800-172
Threat Model - A form of risk assessment that models aspects of the attack and defense sides of a logical entity, such as a piece of data, an application, a host, a system, or an environment. Source: NIST SP 800-53 Rev.5
Trusted Computing Base (TCB) - Totality of protection mechanisms within a computer system, including hardware, firmware, and software, the combination responsible for enforcing a security policy. Source: NIST SP 800-12 Rev.1




Question 1    1 / 1 point
Which security control type best mitigates damage after an incident? (D4.1, L4.1)

A) Limitation controls
B) Preventive controls
C) Detective controls
--> D) Corrective controls

Correct. Corrective controls restore the system or process back to the state prior to a harmful event. As such, they contain the damage and prevent further spread of the incident.


Question 2    1 / 1 point
Which of the following statements is least accurate about virtual machines (VMs)? (D4.3, L4.5)

A) VMs use a hypervisor that provides hardware-level isolation of resources across VMs.
B) VMs access their own virtual hardware and include a complete guest OS in addition to the app and its data.
C) VMs allow different OSs, such as Linux and Windows, to share the same physical hardware.
--> D) VM usage should be discontinued when containerization is available.

Correct. Both virtual machines (VMs. and containers have their uses, and in fact some container deployments may use the VM as opposed to running directly on the hardware.


Question 3    1 / 1 point
Which of the following best describes malware indicator of compromise (IoC)? (D4.4, L4.7)

A) Increased CPU usage
B) Slow computer or web browser speeds
C) Freezing or crashing
--> D) All of these

Correct.  Increased CPU usage, slow computer or web browser speeds, problems connecting to networks, and freezing or crashing could potentially (among other symptoms) be indicative of malware presence.


Question 4    1 / 1 point
What is the purpose of joining a threat intelligence sharing community? (D4.4, L4.9)

A) To gather information about existing or upcoming threats from internal sources.
B) To subscribe to risk advisories and receive notifications about cyberthreats.
C) To enhance the organization's security posture by aggregating and analyzing threat information.
--> D) To complement the organization's knowledge of threat intelligence by sharing information with other members.

Correct. The purpose of joining a threat intelligence sharing community is to complement the organization's knowledge of threat intelligence by sharing information with other members.


Question 5    1 / 1 point
Which statement below best reflects guidance for security management interfaces (SMI)? (D4.2, L4.2)

A) SMI management should include enhanced security mechanisms for authentication due to lack of logging software.
--> B) SMI management should include enhanced security mechanisms for authentication due to the level of trust and potential impact of configuration changes.
C) SMI management should include enterprise security mechanisms for authentication due to lack of logging software.
D) SMI management should include enterprise security mechanisms for authentication due to the level of trust and potential impact of configuration changes.

Correct. SMI represents an administrative privilege that will require additional safeguards beyond a standard enterprise account. Some assurances might include the use of MFA, limiting remote access, or enforcing separation of duties.


Question 6    1 / 1 point
Which of the following mitigations leaves the threat unmitigated? (D4.2, L4.2)

--> A) Warn the user.
B) Disable the feature.
C) Remove the feature.
D) Purchase insurance.

Correct. The answer here is predicated on the ability to understand passive versus active security activities. An alert is detective but not preventative.


Question 7    0 / 1 point
Which of the following best places the architectural software Interface XYZ as the downstream dependency? (D4.2, L4.2)

Downstream dependencies are those tasks or processes that cannot begin until a particular task or project is completed. They are called "downstream" because they occur later in the process or workflow. In other words, they are the tasks that come after the task that you're currently working on.

Upstream dependencies are those tasks or processes that need to be completed before a particular task or project can begin. They are called "upstream" because they occur earlier in the process or workflow. In other words, they are the tasks that come before the task that you're currently working on.

A) Interface XYZ is a standalone component.
B) Interface XYZ is consumed by front-end UVW.
--> C) Business ABC is consumed by Interface XYZ.
D) Storage MNO is consumed by front-end UVW.

Incorrect. The scenario involves interface XYZ as a downstream dependency.


Question 8    0 / 1 point
Nkosazana, a college intern at your organization, is researching digital certificates for a research paper in a course titled CYB204: Modern Identification, Authentication, Authorization, and Accountability (IAAA). Which of the following statements would be the most important concept regarding the deployment of digital certificates to include in his paper? (D4.3, L4.4)

A) Always use a third-party Certificate Authority (CA).
B) Certificates should use 256-bit key strength.
C) Certificates should use X.509 standard.
--> D) Establish certificate management based on life cycle.

Incorrect. A, B, and C are good guidelines to follow pending implementation considerations that are not offered in the context of the question and cannot be assumed. Technologies and standards change over time.


Question 9    0 / 1 point
Which of the following authorization mechanisms least describes when permission is granted only if the subject's clearance matches the sensitivity level of a given object? (D4.3, L4.3)

--> A) Discretionary access control (DAC)
B) Nondiscretionary access control (NDAC)
C) Role-based access control (RBAC)
D) Attribute-based access control (ABAC)

Incorrect. NDAC is an authorization mechanism through which permissions are granted based on predetermined rules and policies.


Question 10   1 / 1 point
What is the role of a hypervisor in virtualization? (D4.3, L4.5)

--> A) It manages hardware resources between different guest operating systems.
B) It provides encryption for virtual machines to ensure data security.
C) It is responsible for creating virtual machines on a physical host.
D) It monitors network traffic and prevents unauthorized access.

Correct. The role of a hypervisor in virtualization is to manage and allocate hardware resources such as CPU, memory, and storage between multiple guest operating systems running on a single host.


Question 11   1 / 1 point
Attack surface analysis aids security practitioners in identifying what? (D4.4, L4.8)

A) Identifying the functions and parts of the system necessary to review/test for security vulnerabilities.
B) Identifying high-risk areas of code that require defense-in-depth protection and what parts of the system need defense.
C) Identifying when the attack surface has changed and there is the need to perform a threat assessment.
--> D) All the above.

Correct. Attack surface analysis is an assessment of the total number of exploitable vulnerabilities in a system, network, or other potential computer attack target, so all answers apply.


Question 12   1 / 1 point
Which of the following best represents the steps for conducting attack surface analysis? (D4.4, L4.8)

--> A) Define attack surface. Measure attack surface. Manage attack surface.
B) Define attacks. Measure surface. Manage attack surface.
C) Define attack surface. Measure organizational risk register. Manage risks.
D) Define risks. Measure risk impact through assessment. Manage risks.

The attack surface-analysis process steps define the attack surface of an application, identify and map the attack surface, measure, and assess the attack surface, and manage the attack surface.


Question 13   1 / 1 point
Which of the following terms best encapsulates a modern knowledge base of cyber adversarial behavior and a taxonomy for adversarial actions throughout the system life cycle? (D4.4, L4.7)

A) OWASP Threat Dragon
B) OWASP Attack Surface
C) MITRE Life Cycle Threats
--> D) MITRE ATT&CK

Correct. MITRE ATT&CK is a knowledge base of cyber adversary behavior and taxonomy for adversarial actions across their life cycle. ATT&CK consists of two parts: ATT&CK for Enterprise (IT networks and cloud) and ATT&CK Mobile.


Question 14   0 / 1 point
Which of the following is the most important task when performing a design security review? (D4.5, L4.10)

A) Attach performance metrics to the review process.
--> B) Decompose your application and be able to identify key items.
C) Highlight all security controls used in the system.
D) Use standardized graphics to document the data flow.

Incorrect. While using standardized graphics to document the data flow can be helpful in visualizing and understanding the flow of data within a system, it is not specifically the most important task when performing a design security review.


Question 15   1 / 1 point
Which of the following statements is/are correct regarding the architecture and design security review process? (D4.5, L4.10)

A) The process primarily relies on using SAST and DAST tools.
B) The process requires mapping functional requirements to test cases.
C) As part of the review process, trust boundaries are excluded from examination.
--> D) As part of the review process, special attention must be given to critical areas in the application, including authentication, authorization, and input validation.

Correct. Special attention must be given to critical areas.


Question 16   1 / 1 point
Which of the following is NOT a step within Architectural Risk Assessment (ARA) a security analysis framework that includes tools and techniques intended for applications and services related to application development? (D4.5, L4.10)

A) Create security objectives
B) Identify use cases
--> C) Add unit testing
D) Develop application diagrams and a threat mode

Correct. Add unit testing does not include Architectural Risk Assessment (ARA) steps.


Question 17   0 / 1 point
Braylon is reviewing some software document artifacts post internal project kickoff. He observes the following statement: "The software application must support multiple users." Which of the following best describes what he is reading? (D4.6, L4.11)

--> A) Functional requirement
B) Nonfunctional requirement
C) Service level agreement (SLA)
D) Nondisclosure agreement (NDA)

Incorrect. Nonfunctional requirements typically specify system attributes and constraints rather than specific functionality.


Question 18   1 / 1 point
Which of the following statements most accurately describes the relationship when modeling nonfunctional security properties within software systems? (D4.6, L4.11)

A) Software security properties are usually independent.
--> B) Software security properties are usually interrelated.
C) Software security is not included in specialty engineering considerations.
D) None of these.

Correct. Software security properties are part of specialty engineering (quality requirements) and often have overlapping areas of concern.


Question 19   0 / 1 point
James works in a software factory that generates revenue by supplying custom software. A medium-sized e-commerce organization hired James's employer to provide a new web service. The contract states that the supplier must use the ISO/IEC 15288 standard. Which of the following is most likely to be viewed as a software project enabler? (D4.7, L4.12)

A) Risk management
--> B) Life cycle management
C) Server management
D) Change management

Incorrect. While risk management is an important aspect of software project management, it is not directly related to ISO/IEC 15288 or the use of that standard.


Question 20   0 / 1 point
Which of the following best describes roles with respect to security operations? (D4.7, L4.12)

--> A) Security leadership guides governance, mitigations, and event responses. Security operations are focused on the responses to events.
B) Security leadership guides governance, mitigations, and event responses. Security operations are focused on the mitigation of risks.
C) Security leadership guides governance. Security operations are focused on the mitigation of risks.
D) Security leadership guides governance. Security operations are focused on the response to events.

Incorrect. Security operations are primarily concerned with the response to events and maintaining business continuity, rather than solely focusing on the mitigation of risks.



DOMAIN 5: Secure Software Implementation
----------------------------------------

- Secure Coding Best Practices
  -- sw producing organizations should establish and enforce secure coding standards
  -- various guidelines and checklists for secure coding practices have been developed and published
  -- in addition to security considerations, coding standards ensure code uniformity and maintainability, which also supports security throughout a software system's life cycle

  -- secure coding practices when adopted and enforced result in the construction of more cyber-resilient sw
  -- these considerations are often incorporated into various stages of sw development to

    * ensure early and frequent verification of security
    * implementation of proper input validation and encoding
    * robust authentication and access controls
    * error and exception management
    * protection of data in transit and at rest,
    * logging
    * managing third-party code and libraries

  -- Secure Coding Standards and Practices
    * well-documented and enforceable coding standards are essential to secure sw development
    * coding standards encourage programmers to follow uniform set of rules and guidelines determined by the requirements of the project and organization
      - and NOT by the programmer’s familiarity or preference
    * once established, these standards can be used as a metric to evaluate source code (using manual or automated processes) for compliance with the standard

    * today there are a plethora of secure coding standards and practices originating from many organizations including industry giants, international standards groups, and nonprofits
    * example: Carnegie Mellon University (CMU) Software Engineering Institute (SEI) Computer Emergency Response Team (CERT), which publishes guidance for secure coding best practices

    * SEI CERT also publishes generic Top 10 Secure Coding Practices guidance:

      - validate input
      - heed (huomioi) compiler warnings
      - architect and design for security policies
      - keep it simple
      - default deny
      - adhere to the principle of least privilege
      - sanitize data sent to other systems
      - practice defense in depth
      - use effective quality assurance techniques
      - adopt a secure coding standard

    * additionally, secure coding standards can be more specific and focused on a particular programming language
    * for example, SEI CERT produces the following coding standards for specific languages:

      - SEI CERT C Coding Standard
      - SEI CERT C++ Coding Standard
      - SEI CERT Perl Coding Standard
      - SEI CERT Oracle Coding Standard for Java
      - Android Secure Coding Standard



  -- Secure Coding Standards in Action
    * one programming rule from SEI CERT C++ Coding Standard is FIO51-CPP (close when no longer needed)
    * if files are not properly closed after usage, adversaries may be able to exhaust the hosting system’s underlining resources which may result in data confidentiality issues pending memory handling during an application abnormal termination
    * SEI standard informs noncompliant and compliant solutions

    * Noncompliant C++ Source Code
      - standard handler std::abort() is called, which results in std::basic_filebuf<T> object persisting because a destructor was not called

    * Compliant C++ Source Code
      - if std::fstream::close() is executed prior to std::terminate(), it ensures proper resource management
      - alternatively, the stream could also be closed implicitly using Resource Acquisition Is Initialization (RAII), which would have the same effect


    * Race Conditions
      - according to CWE-362, concurrent executions using shared resources with improper synchronization are known as “race conditions”

      - the program contains a code sequence that can run concurrently with other code, and the code sequence requires temporary, exclusive access to a shared resource, but a timing window exists in which the shared resource can be modified by another code sequence that is operating concurrently
        -- this architecture can have security implications when the expected synchronization is in security-critical code, such as recording whether a user is authenticated or modifying important state information that should not be influenced by an outsider

      - race condition violates the following closely related properties:
        -- exclusivity
          * code sequence is given exclusive access to shared resource -> no other code sequence can modify properties of shared resource before original sequence has completed execution
        -- atomicity
          * code sequence is behaviorally atomic -> no other thread or process can concurrently execute same sequence of instructions (or a subset) against the same resource



  -- Sequencing and Timing
    * TOCTOU (time-of-check, time-of-use) race condition is possible when two or more concurrent processes are operating on a shared file system
    * typically, the first access is a check to verify some attribute of the file, followed by a call to use the file
    * attacker can alter the file between the two accesses or replace the file with a symbolic or hard link to a different file
    * TOCTOU conditions can be exploited when a program performs two or more file operations on the same file name or path name
    * program that performs two or more file operations on a single file name or path name creates a race window between the two file operations
    * this race window comes from the assumption that the file name or path name refers to the same resource both times
    * if an attacker can modify the file, remove it, or replace it with a different file, then this assumption will not hold

    * many Unix/Linux-based products (OSs, web servers) have exhibited race condition vulnerabilities
    * at a high-level, all sw security practitioners should understand that three conditions must be satisfied for a race condition to exist:
      - concurrency property: two or more control flows execute concurrently
      - shared object property: concurrent flows must access the shared race object
      - change state property: one or more control flows must alter the race object state

    * race windows
      - elimination of race conditions begins with identifying race windows
      - race window is a code segment that accesses the race object in a way that opens a window of opportunity during which other concurrent flows could “race in” and alter the race object

    * mutual exclusion
      - race conditions are eliminated by making conflicting race windows mutually exclusive
      - once a potential race window begins execution, no conflicting race window can be allowed to execute until the first race window has completed
      - race windows are referred to as critical sections because it is critical that the two conflicting race windows don’t overlap execution
      - treating each critical section as an atomic unit with respect to conflicting race windows is called mutual exclusion



  -- Thread Safety
    * in multithreaded environments, implementations must assure that accessing the same resource by various threads does not expose erroneous behavior or produce unpredictable results
    * this concept is referred to as thread safety
    * this is important when the code depends on a certain order of execution for correct behavior

    * thread safety in action
      - multithreading, processing using multiple threads concurrently, is handled differently within different programming languages and frameworks
      - using Java as an example, there are at least four ways to support thread safety while performing concurrent execution
      - examples include
        -- synchronization
        -- invoking the volatile keyword (variable's value may be modified by multiple threads concurrently by ensuring that all threads see the most up-to-date value of the variable)
        -- utilizing the final keyword (final variables are thread safe because reassignment of a reference object is not possible)
        -- atomic variable



  -- Database Concurrency Controls
    * simultaneous execution of transactions in a multiuser database requires concurrency controls; without them data integrity and consistency problems such as lost updates can arise
    * db concurrency controls are used to coordinate simultaneous transactions on db while preserving the integrity of data
    * various concurrency control algorithms (e.g., lock-based) are used for this purpose


  -- Input Validation
    * input validation is one of the most fundamental and most critical mitigation strategies relevant to most common web application vulnerabilities
      - injection vulnerability, buffer overflow vulnerability, ...

    * input validation represents protection at the perimeters of an application and should be considered a key layer of defense in a defense-in-depth strategy
    * often associated with user interfaces but sw security practitioners understand that integration points between sw systems where data flows between systems should also be restricted and validated

    * important question to answer when developing secure code is to determine where to perform input validation
    * it is recommended that input validation be performed both on the server side (backend) as well as on the client side (frontend)
    * at a bare minimum, input validation must be performed on the backend
    * it should never be performed only on the client side (frontend)
    * client-side validation should be used only from a user experience standpoint and never for implementing security in your software
    * client-side validation can be easily bypassed; for this reason, it must never be trusted to provide adequate levels of security

    * Software, especially internet-centric, adheres to Postel’s Law:
      - “Be conservative in what you do, be liberal in what you accept from others”

      - liberal acceptance of inputs increases user and technology interoperability at the expense of increased cyber-attack surfaces
      - this architectural idea has resulted in the interconnectivity of the internet today
      - security practitioners should take a slightly different stance than Postel to ensure business continuity is maintained and sw risks are managed
      - simple stance for defensively architecting applications might include:

        -- all input data is evil and must be sanitized
        -- data must be accepted for interoperability but there is no trust without verification



  -- Filtering and Canonicalization
    * filtering
      - form of validation
      - there are often known good inputs (i.e., inputs the developer is completely certain are safe)
      - there are also known bad characters (i.e., inputs the developer is certain are unsafe)
      - based on this, there are two approaches to managing input:
        -- allow list
          * list of “known good inputs”
          * for example: “A, B, and C are good (and everything else is bad)”
          * this is a good practice to prevent malicious input into the application

        -- block list
          * list of “known bad inputs”
          * for example: “A, B, and C are bad (and everything else is good)”
          * deny-list validation should be avoided in most cases, as it could be relatively trivial for an attacker to bypass such filters
          * for input fields provided by a user, allow-list validation is appropriate

    * canonicalization (= normalization, process for converting data that has more than one possible representation into a "standard", "normal", or canonical form)
      - refers to multiple (alternate) representations of a name
      - equivalent forms of a name resolve to a same standard name (canonical name)
      - example requirements for secure sw code: identify the alternate representations, design sw so that validation decisions are not based upon any one method of file or device name
      - regardless of the development platform, always transform the file name or URL into canonical version to ensure that different variations of file name or URLs equate properly


  -- Output Sanitization
    * improper encoding or escaping can allow attackers to change the commands sent to another component, inserting malicious commands instead
    * output sanitization means sanitizing the data you may already have and securing it prior to rendering it for the end user
    * this is done by converting untrusted input into a safe form where the input is displayed as data to the user without executing as code
    * output sanitization is also known as output encoding

    * one way of performing output encoding is to use an Extensible Stylesheet Language (XSL) engine, which can convert the output to a different encoding
    * when the document is loaded into memory, XML applications such as XSL Transformation (XSLT) engines convert it to Unicode
    * the XSL engine then uses the stylesheet templates to create a transformed version of the content in memory structures
    * when it's done, it serializes the internal content into stream of bytes that it feeds to the outside world
    * during the serialization process, it can convert the internal Unicode to some other encoding for the output

    * output sanitization issues in action
      - output sanitization issues may arise when downstream sw uses an encoded input, but the data does not match what was intended
      * for example, HTML supports entity encoding for HTML body elements on a website
      * programmer may encode output as an attribute that contains functional JavaScript


  -- Error and Exception Handling
    * exceptions are run-time anomalies
    * here are a few possible examples:
      - run-time anomalies
        -- examples:
          * attempted to establish connection, but the server does not respond
          * tried to connect to the database, but connections cannot be established
          * attempted to open a file for reading, but the file does not exist

        -- error and exception handling are a part of the overall security of sw
        -- proper error and exception handling must resolve anomalies, faults, and errors in ways that do not leave the sw, its resources, data, or environment in a vulnerable state
        -- all exceptions must be explicitly handled

      - best practises
        -- few examples of commonly agreed-upon best practices in this regard include:
          * not ignoring exceptions
          * leaning toward catching specific exceptions
          * having a catch-all exception block
          * cleaning up resources in a final block

      - verbose exception/error messages are one source of information leakage about the sw, including the logic of the code or schema of the database
      - leaked information may be used for launching focused attacks (e.g., SQL Injection)
      - verbose error messages should be avoided, and generic responses must be returned


  -- Secure Logging and Auditing
    * logging and auditing are distinct but interrelated topics
    * NIST defines a log as a record of the events occurring within an organization’s systems and networks
    * NIST defines an audit as the independent review and examination of records and activities to assess the adequacy of system controls and ensure compliance with established policies and operational procedures

    * logging describes  who, what, when, where, and how to CAPTURE information to be reviewed at a later date
    * auditing describes who, what, when, where, and how to REVIEW the captured log data

    * log files in general can show discrete events within an application or system, such as those that pertain to failure, error, or state transformation
    * it is important to recognize that logs are not meant to solely serve the purpose of troubleshooting sw by developers and that there may be many stakeholders interested in events and logs, including security personnel and auditors

    * types of information contained in each log will depend on the nature of the log in question
    * establishing audit trails requires, at minimum, the capture of “who,” “what,” “when,” and “where” in logs
    * when capturing the time (“when”), use a consistent timestamp

    * audit trails need to capture enough evidence to help in an investigative process
    * among the factors should be when the event took place, who caused the event (e.g., user ID), what triggered the event (e.g., program, command), and date and time, which will help determine whether the user was who they should have been or a masquerading intruder

    * security practitioners must also be concerned with maintaining logs while considering confidentiality and integrity concerns
      - integrity
        -- logs hashes may be implemented
        -- logs may be stored on write once, read many (WORM) drives where, as the name suggests, data written to the medium cannot be erased or modified

      - confidentiality
        -- log access should be limited based on need-to-know
        -- sometimes logs may contain sensitive data to which an auditor should not have undue (kohtuuton, liiallinen) access
        -- in this case, user data could be sanitized or masked before storage for compliance considerations


 -- Session Management
    * given the stateless nature of HTTP protocol, and the fact that each request/response pair is independent of other web interactions, web applications require the implementation of session management capabilities to retain information about application users for the duration of their sessions

    * session management practices are especially important in the context of authenticated sessions, when session ID is used by web app for the purpose of enforcing access control
    * improper session management that leads to disclosure, prediction, capture or brute forcing the session IDs presents an opportunity for exploitation of vulns by adversaries
    * hijacking a session and impersonating a victim in the web app can have severe consequences

    * in general, technology-specific web development frameworks such as .NET, PHP, and Java (among others) offer built-in session management capabilities
    * built-in approach is preferred over custom (developed in-house) session management implementations, but even these frameworks may not be free of vulns
    * if using cookies for session management, always follow best practices and ensure the protection of the cookies with cryptographic mechanisms or other appropriate safeguards
      - for example, enforce that cookies can only be transmitted over a secure HTTPS connection



  -- Safe Application Programming Interfaces (APIs)
    * services of sw can be made available through a set of APIs
    * think of them as a set of interfaces where sw services can be requested
    * examples of APIs include:

      - APIs from the standard library of a programming language
      - APIs from third-party libraries to perform cryptographic operations
      - APIs to access and process data in a database
      - APIs available through the operating system for kernel services
      - Web APIs to receive real-time traffic information from some provider
      - APIs to send a message or receive a message securely

    * in summary, APIs are used to communicate with other sw layers, products, and services

    * libraries provide the implementations in terms of functions and procedures, and for the API users, they are a set of specifications for invocation without the knowledge of how the APIs are implemented

    * of special importance to the CSSLP is the safety and security of these APIs
    * sw developers must make efforts to avoid using unsafe APIs
    * example of unsafe APIs is the various string manipulation functions from the standard library of C/C++, where improper usage of these functions could result in buffer overflows
    * most static analyzers can detect the usage of unsafe APIs

    * security of APIs in general, and web APIs specifically, must be assured, whether you own them or you use them
    * discussion of the security of web APIs is outside the scope of this domain; however, note that the same security principles that have been discussed so far (least privilege, separation of concerns, defense in depth) are relevant to the security of web services



  -- Resource and Secure Configuration Management

    * type safety
      - refers to the ability of a programming language to prevent type errors
      - type error is incorrect/undesirable program behavior caused by discrepancy between different data types and includes considerations for constants, variables, methods, and functions
      - type enforcement can be
        -- static, catching potential errors at compile time
        -- dynamic, associating type information with values at runtime and consulting them as needed to detect imminent errors
        -- combination of both

      - behaviors classified as type errors by given programming language are usually those that result from attempts to perform operations on values that are not of appropriate data type

      - from a security perspective, type safety is desirable
      - C and C++ are type unsafe, whereas other languages, such as .NET, Ada, and Java, are considered type safe
      - there are special-purpose programming languages like Ada, designed to be type and memory safe to support development of critical sw systems such as embedded systems on aircraft
      - sw security practitioners should understand the tradeoffs between commonly adopted languages and special case languages
        -- for instance, Ada is more compliant with the requirements for critical infrastructure than C++ by default but where will you find competent Ada programmers?
        -- at the time of this writing there are millions of C++ developers compared to a much smaller unknown quantity utilizing Ada

    * memory management
      - memory represents one of the most critical resources of a computer system
      - improper memory management practices can lead to serious stability and security-relevant issues
      - memory leaks associated mostly with unmanaged programming environments (e.g., C/C++) are common, yet not easy to trace back to the source of the problem to fix

      - depending on the nature of the problem, memory leaks may be negligible with little (if any) noticeable effect, or significant enough that they lead to an eventual exhaustion of the resource and abnormal system behavior
      - leaks associated with kernel processes can be particularly problematic and may result in significant stability issues and deterioration of system performance

      - this issue in more general terms is covered by CWE, as Uncontrolled Resource Consumption:
        -- “The software does not properly control the allocation and maintenance of limited resource thereby enabling an actor to influence the amount of resources consumed, eventually leading to the exhaustion of available resources"

      - Rust programming language has been presented as an alternative to C++
        -- it uses a similar syntax and may offer increased memory safety and other advantages


    * automatic memory management and destruction of unneeded objects
      - code written for managed programming languages, such as .NET and Java, can benefit from garbage collection facilities
      - these languages provide generalized way to handle the details of memory management and garbage collection
      - garbage collection facilitates the destruction of instantiated dangling (roikkuva) objects in memory that have gone out of scope but are not explicitly destroyed (closed), at the cost of a small amount of overhead
      - this tradeoff frees developers from various error-prone memory management tasks

      - main benefit of garbage collection is that it frees the programmer from some memory management tasks
      - however, garbage collection has potential disadvantages, including:
        -- consumption of additional resources
        -- potential adverse impact on performance
        -- incompatibility with manual resource management

    * tokenization
      - replaces sensitive data with a unique nonsensitive substitute (token) that retains (säilyttää) data without compromising its security
      - focuses on minimizing data storage for essential information that positively impacts data security, especially for credit card transactions
      - also aids in compliance with PCI-DSS and other applicable government considerations



  -- Isolation
    * is the idea of keeping a VM separate from its physical host system and from other VMs
    * both virtualization and containers are different types of isolation discussed in previous domains
    * here we'll discuss sandboxing, another form of isolation

    * sandboxing
      - sandboxed environments are primarily about isolation and the security benefits that come with it
      - technologies may be used to create an isolated and safe execution environment, and to contain the possible damage and impact of misbehaving code

      - sandbox typically provides a tightly controlled set of resources for guest programs to run in, such as scratch space on disk and memory
      - network access and the ability to inspect the host system or read from input devices are usually disallowed or heavily restricted
      - java applet runnig in a browser is an example of a sandbox
        -- it's a representation of mobile code: code downloaded from a remote server, executing on the client
        -- applet may not be allowed to perform some operations or to have access to certain resources

    * separation kernel protection profile (SKPP)
      - provides the specification of security requirements for the purpose of evaluating “separation kernels”
      - to understand this concept better and recognize its relation to the security concept of isolation, both 'separation kernels' and 'protection profiles' must be understood

      - separation kernel
        -- the concept was first introduced by John Rushby in his paper titled “Design and Verification of Secure Systems”
        -- Rushby’s central idea was that from security perspective, separation (the function implemented by a hypervisor to separate VMs) is such a critical operation that it should not be handled by a large/complex operating system that has a multiplicity of responsibilities (moninaisia vastuita)

        -- separation kernel, solely focused on the separation task, is created by factoring out the separation function from the OS, resulting in a kernel that can be verified for correctness due to its simplicity
        -- initial use case for separation kernel was its application to high-security systems responsible for the separation of DoD and government top-secret, secret, and confidential information classifications

        -- separation kernel is a special type of bare metal hypervisor that only does separation
        -- more specifically, it is a tiny piece of carefully crafted code (as small as 15KB) that utilizes modern hw virtualization features to
          1) define fixed VMs
          2) control information flows

        -- minimal implementation of a separation kernel involves eliminating as many things as possible (device drivers, user model, shell access, ...)
        -- any such kernel architecture, although not best suited for desktop use, may be appropriate for embedded real-time or safety-critical systems


      - protection profiles and common criteria
        -- CC, also known as ISO/IEC 15408, were developed through a combined effort of six countries: US, Canada, France, Germany, Netherlands, UK
        -- this effort built on earlier standards, including
          * Europe’s Information Technology Security Evaluation Criteria (ITSEC)
          * United States’ Trusted Computer System Evaluation Criteria (TCSEC)
          * Canadian Trusted Computer Product Evaluation Criteria (CTCPEC)

        -- this international standard for computer security allows an objective evaluation to validate that a product satisfies a defined set of security requirements
        -- product evaluation per CC involves the selection of Protection Profile (PP), an implementation-independent set of security requirements



  -- Cryptography
    * method of storing and transmitting data in a form so that only those for whom it is intended can read and process it
    * most often associated with scrambling plaintext into cipher text through the process of encryption and the reverse: unscrambling from cipher text back to plaintext through the process of decryption

    * modern cryptography concerns itself with the following objectives:
      - confidentiality (the information cannot be understood by anyone for whom it was not intended)
      - integrity (the information cannot be altered in storage or transit between sender and intended receiver without the alteration being detected)
      - nonrepudiation (the creator/sender of the information cannot deny at a later stage their intentions in the creation or transmission of the information)

      - understand that symmetric algorithms may only be used to address the confidentiality requirement, whereas asymmetric algorithms have other use cases as well
      - further, application developers must ensure that they are using the latest standards (e.g., TLS 1.2 and AES 256 are current as of this writing), as they are subject to change
      - the word standard implies the usage of vetted solutions and avoidance of custom cryptography where and when possible
      - encryption may be considered a dual-use good (having both commercial and military apps), and that it may be subject to export restrictions, depending upon certain criteria


  -- Cryptography vs. Encryption
    * terms encryption and cryptography are often used interchangeably, but a distinction must be made between them
    * if cryptography is a science, then encryption refers to one component of that science

    * remember the basics of how symmetric and asymmetric algorithms work:

      - symmetric algorithms
        -- relatively fast (as compared to asymmetric ones) and require a smaller resource investment, but present key management challenges, as the same key that is used to encrypt a message will also be needed to decrypt it
        -- this key must remain private and in possession only of the parties involved
        -- good examples of symmetric algorithms include DES (which is considered broken) and AES (standard at the time of this writing)

      - asymmetric algorithms
        -- rely on a pair of keys: one that is private (known only to the owner) and one that is public (assumed that everyone could have knowledge of it)
        -- to achieve confidentiality, a message is encrypted using the public key of the recipient, so that only the recipient can decrypt the message
        -- good examples of asymmetric algorithms include Diffie-Hellman (for historical reasons) and RSA (current)
        -- public key cryptography relies on Public Key Infrastructure (PKI), which consists of several components including
          * Certificate Authority (CA)
          * Registration Authority (RA)
            - authority in a network that verifies user requests for a digital certificate and tells the CA to issue it
            - responsible for receiving certificate signing requests – for the initial enrollment or renewals – from people, servers, things or other applications
            - verifies and forwards these requests to CA
            - also responsible for receiving other certificate lifecycle management functions (for example, revocation)
            - implements business logic to accept requests, including methods for verifying the origin of the requester and the party that should have the certificate
            - usually separated from CA for accessibility and security reasons
          * digital certificates

    * more practical usage would involve a hybrid of symmetric and asymmetric algorithms
    * in such a hybrid usage, a symmetric algorithm is used to encrypt the message, and the key used to perform the encryption is asymmetrically exchanged with the other party


  -- Encryption Types
    * payload encryption
      - performing end-to-end payload encryption goes beyond the protection afforded by technologies such as TLS
      - protection against unauthorized disclosure during transmission of data between endpoints may require the encryption/decryption of the message at both ends of the communication
      - when used correctly, end-to-end encryption offers protection of message content (text messages, voice calls, video calls) in transit

    * field-level encryption
      - while encryption of the entire database can help achieve certain objectives, field-level encryption also has its use cases
      - when supported, it can be used to provide additional layer of security for specific data throughout processing by enabling encryption on fields/columns containing sensitive data

    * storage encryption
      - for data at rest, encryption requirements of a typical enterprise can be met through a combination of approaches utilizing various tools and technologies for full disk, virtual disk, volume, database, file and folder, application, and other types of encryption (each with its own benefits and limitations)
      - in this context, encryption represents a primary technical control for protection of the information assets, so it must be used when and where it is required (by laws and regulations) or justified (i.e., from a business perspective).

    * full disk encryption (FDE)
      - also known as whole disk encryption, is the process of encrypting all data on the hard drive used to boot a computer, including the computer’s OS, and permitting access to the data only after successful authentication to the FDE product

    * virtual disk encryption and volume encryption
      - virtual disk encryption is the process of encrypting a file called a container, which can hold many files and folders, and permitting access to the data within the container only after proper authentication is provided, at which point the container is typically mounted as a virtual disk
      - virtual disk encryption is used on all types of end-user device storage
      - container is a single file that resides within a logical volume
      - examples of volumes are boot, system, and data volumes on a personal computer, and a USB flash drive formatted with a single file system
      - volume encryption is the process of encrypting an entire logical volume and permitting access to the data on the volume only after proper authentication is provided
      - volume encryption is most often performed on hard drive data volumes and volume-based removable media, such as USB flash drives and external hard drives


  -- Cryptographic Issues
    * cryptographic agility
      - capacity for sw system to easily evolve and adopt alternatives to the cryptographic primitives it was originally designed to use
      - X.509 certificate format is an example of cryptographic agility, supporting various hash algorithms, signature algorithms, and key sizes to address future needs
      - cryptographic agility is a common theme during the architecture and design of cryptographic libraries, APIs, and frameworks, including those found in .NET and Java
      - when new vulnerabilities are found or algorithms are broken, switching to new crypto providers should not put a significant burden on the sw development team
      - cryptographic agility could lead to complexities and potential vulnerabilities if not implemented correctly

    * algorithm selection
      - sw developers are encouraged to use vetted standards and avoid custom cryptography
      - avoid weak algorithms when possible, as they may provide a false sense of security
      - understand that the field is rapidly changing, and when evaluating algorithm choices, both the algorithm and the key size should be considered
      - Data Encryption Standard (DES), once considered government standard, was broken much more quickly than anticipated due to inadequate key length, enabling brute force attempts
      - follow the best practices, adopting symmetric algorithms, asymmetric algorithms, hash functions, etc
      - for example, best practices may recommend the use of SHA 256 or better, which implies that the use of the MD series of hash functions should be avoided

    * cryptographic libraries
      - sw developers frequently rely on cryptographic libraries and APIs for cryptographic services and functions
      - examples of this include
        -- symmetric block ciphers
        -- Microsoft Cryptography API (CryptoAPI), now replaced by Cryptography API Next Generation (CNG)
        -- Java cryptography libraries and packages

      - developers must make sure to use the right cryptographic libraries and services to achieve the intended security objectives

      - partial list of cryptographic providers available to developers:
        -- Microsoft Base Cryptographic Provider
          * offers basic cryptographic functionality
          * meets requirements for export to other countries

        -- Microsoft Strong Cryptographic Provider
          * extension from the Base Cryptographic Provider
          * available with Windows XP and later

        -- Microsoft Enhanced Cryptographic
          * additional algorithms and longer keys were supported

        -- Microsoft AES Cryptographic Provider
          * offers support for the AES algorithms

    * cryptography as dual-use technology
      - cryptography is considered a dual-use good and as such, the export of cryptographic technologies may be subject to restrictions
      - Bureau of Industry and Security (BIS) of US Department of Commerce is responsible for regulating the export of most commercial items, often referred to as “dual-use” items—those having both commercial and military or proliferation



  -- Access Control
    * key component of security is access control
    * various access control models were discussed in previous domains, and role-based access control (RBAC) was described as a valuable approach
    * also relevant to access control are the topics of trust zones and permissions

    * trust zones
      - concept of trust zones has existed in various realms of security, including network security, physical security and sw security where each zone is associated with a certain security and trust level

      - examples
        -- by controlling flow of traffic between networks and hosts, firewalls enable organizations to create various security zones supporting various security postures in each zone
        -- as part of sw threat modeling, developers identify the boundary where the level of trust in code or data in a system or between subsystems changes

      - applying the concept of zones of trust requires the understanding of the value of the assets that are to be protected in each zone

    * zero trust architecture (ZTA)
      - modern zero-trust model deviates from the traditional focus on perimeter defenses
      - according to NIST, ZTA provides a collection of concepts, ideas, and component relationships (architectures) designed to eliminate the uncertainty in enforcing accurate access decisions in information systems and services
      - to lessen uncertainties (as they cannot be eliminated), the focus is on authentication, authorization, and shrinking implied trust zones while minimizing temporal delays in network authentication mechanisms
      - access rules are restricted to least privilege and made as granular as possible

    * function permissions
      - adhering to the principle of least privilege requires minimalistic approach to the management of function privileges and permissions
      - this principle requires that processes should be confined to as small a protection domain as possible
      - many systems lack the granularity of privileges and permissions, and so the principle is not entirely properly applied
      - in turn, designers apply this principle as best they can
      - consequences in systems built like this can be more severe than the consequences for systems that correctly adhere to this principle

      - Android and iOS permission management models are examples in this regard
      - from the classic example of the early day flashlight apps that required access to the phone’s GPS, contacts, and other personal information, most security-conscious users have been concerned with the excessive app permissions and the contents/resources that are accessed by the apps


  -- Processor Microarchitecture Security Extensions
    * as hw and sw have evolved over the decades, the discovery of many new vulnerabilities has been witnessed
    * although sw seems to possess most identified/researched vulns, as evidenced by the frequency and number of released patches, hw vulns should not be forgotten or underestimated

    * vulns in hw may also be exploited to gain access to sensitive data
    * ultimately, all operations on a computer are dependent on hw at the lowest levels

    * attacks targeting hw can be divided into two broad categories:
      - those that intend to exploit hw features to bypass security checks of os
      - those that attempt to directly exploit the hw architecture itself

    * to understand the gravity of the situation, imagine a scenario where a process executes itself at ring 0 (most privileged), independent of the operating system, with no checks and balances, exploiting the architecture to allocate memory and deploying payloads, all with no supervision or interference of the OS


    * spectre meltdown
      - in 2017, multiple teams of researchers independently discovered a new class of hw vulns in a broad set of microprocessors found in personal computers, servers, tablets, and phones
      - these vulns, which became known as Spectre and Meltdown, took advantage of a performance optimization technique found in these microprocessors to allow an attacker to bypass security mechanisms protecting data stored in computer systems
      - the implications of this vulnerability were severe: it could allow theft of credentials and cryptographic keys, or exfiltration of sensitive data
      - mitigating the risk of these vulns required efforts at multiple levels, including patches in firmware and microcode, updates to os, and modifications in apps

      - different approaches to hw-assisted Trusted Execution Environment (TEE) include Intel’s Software Guard Extensions (SGX) and AMD’s Secure Memory Encryption (SME)

    * SGX
      - set of instructions that increase the security of application code and data
      - developers can partition security-sensitive code and data into an SGX enclave (erillisalue), which is executed in a CPU protected region
      - developer creates and runs SGX enclaves on server platforms where only CPU is trusted to provide attestations and protected execution environments for enclave code and data

    * SME and SEV (Secure Encrypted Virtualization)
      - security components found in some AMD processors provide hw-accelerated memory encryption for data in use
      - SME uses single key to encrypt system memory (key is generated by the AMD Secure Processor at boot)
      - memory encryption is transparent and can be run with any operating system
      - SME is general-purpose mechanism: flexible, integrated into CPU architecture, scalable from embedded to high-end server workloads, and requires no app sw modifications
      - main memory encryption can be utilized to protect a system against a variety of attacks
      - while data is typically encrypted today when stored on disk, it's stored in DRAM in the clear, which can leave data vulnerable to snooping by unauthorized admins or sw/hw probing

      - SEV uses one key per vm to isolate guests and the hypervisor from one another
      - keys are managed by the AMD Secure Processor
      - SEV requires enablement in the guest operating system and hypervisor
      - guest changes allow the VM to indicate which pages in memory should be encrypted
      - hypervisor changes use hw virtualization instructions and communication with the AMD Secure processor to manage the appropriate keys in the memory controller



- Analyzing Code for Security Risks
  -- sw risk can never be reduced to zero
  -- even sw developed with high-security programs will undoubtedly include unintentional defects that result in degradation of CIA of production sw systems
  -- however, these security defects should likely be considered zero days (an attack that exploits a previously unknown hardware, firmware, or software vulnerability)
  -- if sw weaknesses are known and documented with the security community, using various databases such as MITRE CWE, then sw assurance efforts throughout sw implementation phase should prevent those defects from being incorporated into an organization’s source repository


  -- Vulnerability Databases and Lists
    * it has been stated that a successful attack involves the existence of a susceptibility (alttius), access to susceptibility and capability to exploit the susceptibility
    * vuln databases and lists maintain information about discovered vulns that target information systems
    * in addition to the description of vuln, other information maintained may include an assessment of potential damage, workarounds, and associated remediation strategies

    * National Vulnerability Database (NVD)
      - NVD is US government’s repository of standards-based vuln management data represented using the Security Content Automation Protocol (SCAP)
      - this data enables automation of vuln management, security measurement, and compliance
      - includes databases of security checklist references, security related sw flaws, misconfigurations, product names, and impact metrics

      - NVD is the CVE List augmented with additional analysis, a database, and a fine-grained search engine
      - is synchronized with CVE such that any updates to CVE appear immediately on the NVD

    * Common Weakness Enumeration (CWE)
      - community-developed list of common sw and hw security weaknesses
      - serves as a common language, a measuring stick for security tools, and as a baseline for weakness identification, mitigation, and prevention efforts
      - each individual CWE represents a single vulnerability type
      - currently maintained by the MITRE Corporation

      - top 25 Most Dangerous Software Errors (CWE Top 25) is a demonstrative list of the most widespread and critical weaknesses that can lead to serious vulns in sw
      - these weaknesses are often easy to find and exploit
      - they are dangerous because they will frequently allow adversaries to completely take over the execution of sw, steal data, or prevent the sw from working
      - top 25 is a community resource that can be used by sw developers, sw testers, sw customers, sw project managers, security researchers, and educators to provide insight into some of the most prevalent security threats in the sw industry

    * Common Vulnerabilities and Exposures (CVE)
      - dictionary that provides definitions for publicly disclosed cybersecurity vulnerabilities and exposures
      - goal is to facilitate the sharing of data across separate vulnerability capabilities (tools, databases, and services) with these definitions
      - CVE entries are comprised of an identification number, a description, and at least one public reference
      - CVE List is built by CVE Numbering Authorities (CNAs)
      - every CVE entry added to the list is assigned by a CNA

      - CNAs are organizations authorized to assign CVE IDs to vulns affecting products within their distinct, scope, for inclusion in first-time public announcements of new vulns
      - these CVE IDs are provided to researchers, vulnerability disclosers and information technology vendors
      - CVE list feeds the US NVD

    * Common Attack Pattern Enumeration and Classification (CAPEC)
      - developed by leveraging CWE and CVE
      - comprehensive dictionary and classification taxonomy of known attacks
      - can be used by analysts, developers, testers, and educators to advance community understanding and enhance defenses

    * Open Web Application Security Project (OWASP)
      - nonprofit foundation that works to improve the security of sw
      - provides numerous resources to address the needs of various sw security stakeholders in each phase of the sw-development life cycle

      - providing an exhaustive list of these resources is not possible in this course, but a partial list (in no specific order) includes:
        -- OWASP Software Assurance Maturity Model (SAMM)
        -- OWASP Application Security Verification Standard (ASVS)
        -- OWASP Cheat Sheet Series
        -- OWASP Code Review Guide
        -- OWASP Testing Guide

      - top 10 lists from OWASP (in no specific order) include:
        -- Top 10 Web Application Security Risks
        -- Top 10 Proactive Controls
        -- Top 10 Mobile Risks
        -- API Security Top 10


  -- OWASP Top 10 Web Application Security Risks
    * OWASP Top 10 is a standard awareness document for developers and web application security
      - represents a broad consensus about the most critical security risks to web applications

    * at the time of this writing, the Top 10 Web Application Security Risks included the following:
      - 2021-Broken Access Control
        -- 34 CWEs mapped to risk were the most prevalent defect
      - 2021-Cryptographic Failures
        -- failures related to cryptography can lead to sensitive data exposure or system compromise
      - 2021-Injection
        -- 33 CWEs mapped to injection and represent the second most frequent defect
        -- cross-site scripting is now included
      - 2021-Insecure Design
        -- new in 2021, focuses on risks related to design flaws
        -- highlights the importance of sw security throughout the lifecycle
      - 2021-Security Misconfiguration
        -- most applications tested suffered from misconfiguration which correlates with business drivers for configurability and reuse of modern sw
        -- XML External Entities (XXE) are now included in this category
      - 2021-Vulnerable and Outdated Components
        -- changed from “Using Components with Known Vulnerabilities” and is a known issue that security practitioners struggle to test and assess risk
      - 2021-Identification and Authentication Failures
        -- changed from “Broken Authentication” and now includes CWEs that are related to identification failures
      - 2021-Software and Data Integrity Failures
        -- new in 2021, focuses on assumptions related to sw updates and CI/CD pipelines without guaranteeing integrity
      - 2021-Security Logging and Monitoring Failures
        -- changed from “Insufficient Logging & Monitoring” and now includes types of failures which directly impact visibility, incident alerting, and forensics
      - 2021-Server-Side Request Forgery
        -- included based on member surveys indicating importance despite data indicating low incidence rate and above average test coverage

  -- Other Common Application Vulnerabilities
    * CWE Top 25 is a list of the most widespread and critical weaknesses that can lead to serious vulnerabilities in sw


  -- Buffer Overflows
    * common sw vuln in numerous products, both commercial and open source
    * have led to exploits including malicious code execution or denial-of-service attacks
    * described in CWE:
      - “A buffer overflow condition exists when a program attempts to put more data in a buffer than it can hold, or when a program attempts to put data in a memory area outside of the boundaries of a buffer. The simplest type of error, and the most common cause of buffer overflows, is the classic case in which the program copies the buffer without restricting how much is copied”

    * good example of this classic case is the function “strcpy (...)” from the standard library of C/C++, which will copy a string from one memory location to another without regard to the destination buffer size (there are many more functions like this in the standard library)

    * from a discoverability and exploitability perspective, OWASP suggests that buffer overflows are not easy to discover, and even when one is discovered, it is generally extremely difficult to exploit
    * nevertheless, attackers have managed to identify buffer overflows in a staggering array of products and components
    * variations of this problem exist in terms of stack overflow, heap overflow, and other processes
    * nonetheless, this issue is primarily associated with languages such as C/C++, where the language was designed with certain flexibility requirements and objectives in mind
      --> the ability for direct memory manipulation through pointers and pointer operations

    * can lead to severe memory issues; it may even crash the system
    * attackers may also use this vuln to execute arbitrary code, which can lead to compromise in CIA security objectives
    * from the basic description of this vuln described above, the following is a list of a few prevention strategies for buffer overflow:
      - validation of input
      - buffer boundary checking and enforcement
      - avoiding use of unsafe APIs


  -- Secure Code Reuse
    * OWASP Top 10 contains an entry on the risk of using components with known vulns
    * OWASP Dependency-Check and Dependency-Track tools can help identify publicly disclosed vulnerabilities and reduce the risk of using third-party and open-source components

    * OWASP Dependency-Check
      - Software Composition Analysis (SCA) tool that attempts to detect publicly disclosed vulnerabilities contained within a project’s dependencies
      - it does this by determining whether there is a Common Platform Enumeration (CPE) identifier for a given dependency
        -> if found, it will generate a report linking to the associated CVE entries

      - automatically updates itself using the NVD Data Feeds hosted by NIST

    * OWASP Dependency-Track
      - intelligent Supply Chain Component Analysis platform that allows organizations to identify and reduce risk from the use of third-party and open-source components
      - takes a unique and highly beneficial approach by leveraging the capabilities of Software Bill-of-Materials (SBOM)
      - this approach provides capabilities that traditional Software Composition Analysis (SCA) solutions cannot achieve

      - monitors component usage across all versions of every application in its portfolio in order to proactively identify risk across an organization
      - platform has an API-first design and is ideal for use in CI/CD

      - can help identify multiple forms of risk including:
        -- components with known vulnerabilities
        -- out-of-date components
        -- modified components
        -- license risk


  -- Software Assurance Tools (SwA)
    * sw assurance is the level of confidence that sw is free from vulns, either intentionally or accidentally inserted during its life cycle, and that the sw functions as intended
    * security practitioners have several tool capabilities to analyze sw source code for risks

    * the following provides an overview of tool types

      - architectural analysis
        -- configuration analysis
          * good SCM (sw config mgmt) plan includes the secure, baseline configurations of sw
          * documenting and enforcing these secure baseline standards help to reduce the attack surface of the sw

        -- component decomposition
          * many sw products have dependencies, existing components, and coupled modules that are delivered as part of the app, often not readily transparent to the user
          * in these cases, component decomposition is necessary prior to other forms of analysis

        -- software supply chain analysis
          * some manufacturers have a pedigree of fixing issues, maintaining community transparency, and making wise security decisions
          * by analyzing sw manufacturer’s history, a proper level of iterative assessment of its products can be established

        -- software assurance cases
          * assurance case includes top-level claim(s) for a property of product, systematic argumentation regarding this claim, evidence & explicit assumptions underlining argumentation

      - dynamic analysis
        -- specialized application testing
          * many types of apps can be attacked in ways that are unique from other types
          * specialized app testing includes
            - Web Application Analysis
            - Database Analysis
            - Functional Testing
          * each of these levels of testing must optimally, include testing for potentially exploitable vulnerabilities in the app

        -- dynamic binary analysis
          * when an app binary is executed, usually in sandboxed environment, dynamic binary analysis can record and verify the behaviors of the binary, specifically for potential vulns

        -- dynamic source analysis
          * many interpreted languages, especially those that are popular for use in web apps and maintenance scripts, can be debugged and analyzed dynamically

      - static analysis
        -- static binary analysis
          * app binaries that are not executed directly can be decompiled and disassembled into control flow graphs and processor instructions that can then be analyzed

        -- statis source analysis
          * when source code is available, it can be reviewed for defects and weaknesses



  -- Architectural and Design Analysis
    * architectural analysis refers to the system architecture, requirements, and design that comprise an application development life cycle
    * while the system’s mission and environment are more closely aligned with mission assurance, the specific sw components that make up a mission must be secured
    * thorough architectural analysis is conducted in four main areas
      - configuration analysis
      - component decomposition
      - sw supply chain analysis
      - open sources analysis

    * configuration analysis
      - sw components can be configured in an insecure way, so it is important to monitor the configuration on a regular basis
      - it is also important to verify that a secure baseline for an application exists
      - this can prevent attackers from using legitimate sw features to enact exploits on a system
      - many sw components include hardening guidelines that should be followed and evaluated during security testing
      - SCM policy should include all hardening guidelines and application-specific configuration options to operate the sw in a known, secure state

    * component decomposition
      - in the realm of composition analysis, component decomposition extracts dependencies from even cryptic pieces of sw
      - good composition analysis platform will perform sw supply chain analysis and open sources analysis after unpacking/decomposing the sw

    * software supply chain analysis
      - determines the lineage (sukujuuri) of a sw product
      - many techniques use a component’s popularity and history of fixing security issues to determine a pedigree score for a manufacturer or product
      - if the sw hasn’t been used or trusted before, has an unverifiable origin, or has not been maintained with security updates, it imposes higher risks to the app
      - also includes verifying that the dependency in use within the sw product is the correct one coming from the origin under analysis
      - component under analysis is also checked for license compliance by making sure that OSS licenses are not violated and that commercial licenses are being used appropriately
      - often performed as an extra step after component decomposition

    * software assurance cases
      -  assurance case is a practical alternative to an overwhelming list or other unstructured methods for recording what was done or not done in a way that can lead to greater confidence in the result
      - assurance case includes a top-level claim for a property of a system or product (or set of claims), systematic argumentation regarding this claim, and the evidence and explicit assumptions that underlie this argumentation
      - arguing through multiple levels of subordinate claims, this structured argumentation connects top-level claims to evidence and assumptions


  -- Dynamic and Static Analysis
    * dynamic analysis
      - DAST tools analyze applications in their dynamic running state during testing or operational phases
      - they simulate attacks against an application (typically web-enabled applications, services, APIs), analyze the application’s reactions, and determine whether it is vulnerable
      - DAST tools go one step further than SAST and spin up a copy of the production environment inside the CI job to scan the resulting containers and executables
      - dynamic aspect helps the system catch dependencies that are being loaded at launch time, such as those that would not be caught by SAST
      - when sw is scanned during execution, even if only partial portions of the code are executed, a dynamic analysis is being conducted
      - hybrid, concolic (suppea), symbolic, and simulated execution methods are included as dynamic analysis techniques
      - while dynamic analysis is typically associated with binary analysis, interpreted languages can be executed to perform certain analysis

        -- Specialized Application Testing
          * many applications provide specialized tools for penetration testing and analysis
          * when developers or business stakeholder constructs use cases, they create specialized testing tools for their application
          * these testing tools and environments must include testing that is specifically designed to elicit the behaviors that may occur when secure coding best practices are not optimal, and a potential vulnerability were to be exploited
          * application that has unit tests, functional test cases, and acceptance criteria should document how to perform tests to verify not only the functionality of the code, but evidentiary (todisteena) behavior were a vulnerability in the code to be exploited
          * when a test fails, each test should be documented for its potential security impact on the sw

        -- Dynamic Binary Analysis
          * scanning a binary for weaknesses by executing it (usually in a sandboxed environment) can help an analyst quickly observe and determine anomalous behavior in app
          * is useful whether or not access to an application’s source code is available

        -- Dynamic Source Analysis
          * some tools interpret source code for analysis or analyze source code while performing a dynamic analysis of an application

    * static analysis
      - SAST tools analyze application source, bytecode, or binary code for security vulnerabilities, typically at the programming and/or testing software life cycle (SLC) phases
      - this technology involves techniques that look through the application in a commit and analyze its dependencies
      - if any dependencies contain issues or known security vulnerabilities, a commit will be marked as insecure and will not be allowed to proceed to deployment
      - this can also include finding hardcoded passwords/secrets in code that should be removed
      - enables an application to be assessed without executing it
      - use sw source code, build processes, binaries, and metadata to perform testing

        -- Static Binary Analysis
          * operates on a binary without utilizing the application’s source code
          * looks for programmatic weaknesses in executable code, often using the experience of a subject matter expert

        -- Static Source Analysis
          * scans software code for weaknesses
          * static source analysis tools may require the sw to build in order to reveal the composition of data structures, control flow graphs, and dependencies with linked components
          * once the analysis platform exposes the structure of these resources, it can analyze their usage directly in source code without executing the sw


  -- Interactive Application Security Testing
    * IAST tools combine elements of DAST with instrumentation of the application under test
    * they are typically implemented as an agent within the test runtime environment (instrumenting the Java Virtual Machine [JVM] or .NET CLR) that observes operations or identifies and attacks vulnerabilities
    * best suited for web applications and web APIs, IAST aims to perform behavioral analysis
    * it relies on a combination of SAST and DAST to achieve its goal through its visibility into both the source code and the execution flow during the runtime


  -- Software Assurance Tools Mapped to SDLC
    * while the consequences of exploitable vulns are realized during the operation and maintenance phases of the SDLC, the actions of anticipating, preventing, identifying, mitigating, and remediating exploitable vulns must take place throughout the entire process
    * to ensure exploitable vulns are identified and mitigated prior to sw deployment, SwA (sw assurance) processes must be applied throughout the entire SDLC
    * the following provides a list of recommended SwA capabilities to be applied during each phase of the SDLC
    * each SDLC phase is further defined in ISO/IEC 15288: Systems and Software Engineering - System Life Cycle Processes
    * the four SDLC phases are
      1) architectural design
      2) implementation, integration, verification, and validation
      3) operation
      4) maintenance



      - architectural design
        -- configuration analysis
          * if the foundation of the system is bad, then it will be nearly impossible to make the final product secure
          * that is why it is critical during the design phase to ensure certain baseline security standards are followed

      - implementation, integration, verification, validation
        -- Configuration Analysis
        -- Component Decomposition
        -- Software Supply Chain Analysis
        -- Software Composition Analysis
        -- Open-Source Analysis
        -- Specialized Application Testing
        -- Dynamic Binary Analysis
        -- Dynamic Source Analysis
        -- Static Binary Analysis
        -- Static Source Analysis

          * significant portion of the SwA work will be performed during these middle phases of development
          * to prevent exploitable vulns from reaching production, the system should be thoroughly tested and analyzed

      - operation
        -- Configuration Analysis
        -- Software Composition Analysis
        -- Specialized Application Testing

          * while sw is in operation, the opportunity for SwA activities includes analyzing for misconfigurations and running specialized tests that don't interfere with operation

      - maintenance
        -- Configuration Analysis
        -- Component Decomposition
        -- Software Supply Chain Analysis
        -- Open Sources Analysis
        -- Specialized Application Testing
        -- Dynamic Binary Analysis
        -- Dynamic Source Analysis
        -- Static Binary Analysis
        -- Static Source Analysis

          * SwA during the maintenance phase should look similar to the implementation through validation phases, with each new bug fix, updates and feature release being thoroughly tested and analyzed to ensure no exploitable vulnerabilities are introduced



  -- Security Code Review
    * combination of human effort and technological support
    * when resources are limited, it becomes essential to prioritize what features and components must be securely reviewed with a risk-based approach
    * secure code review is probably the single most effective technique for identifying security bugs early in the system development life cycle
    * when used together with automated and manual penetration testing, code review can significantly increase the cost-effectiveness of an application security verification effort

    * objective of code review is to detect development errors that may cause vulns and hence give rise to an exploit
    * code review can also be used to measure the progress of a development team in their practice of secure application development
    * can pinpoint areas where the development practice is weak, areas where secure development practice is strong, and give a security practitioner the ability to address the root cause of the weaknesses within a developed solution
    * it may give rise to investigation into sw development policies and guidelines and the interpretation of them by the users; communication is the key

    * manual security code review
      - relying on tools in lieu of (sijasta) manual checks is not advised because tools cannot completely emulate human experience and decision-making capabilities
      - true indication of security maturity implies that the tool is part of a more holistic security program and not just the sole measure to secure sw
      - ability of human reviewers to understand context can lead to more accurate estimates of the likelihood and impact of vulnerabilities’ discovery and exploitation

      - much work goes into manual security code reviews; thus, conducting this activity requires preparation to make it effective
      - examples of the type of preparation involved include
        -- developing an understanding of the business purpose of the application and the most critical business impacts
        -- understanding of threat agents
        -- motivations
        -- threat targets (assets)
        -- understanding the attack surface

    * peer review
      - code reviews are also frequently referred to as peer reviews
      - peer reviews involve the examination of source code authored by one developer by peer developers
      - historically, development teams have relied on peer reviews to
        -- produce sw with fewer defects
        -- ensure compliance with coding standards
        -- share knowledge among team members
        -- perform verification of consistency in implementations


  -- Looking for Malicious Code
    * sw code, particularly when not developed in house (3rd-party code), must be subjected to secure code reviews and inspections where possible, to identify embedded malicious code
    * this includes logic bombs and even backdoors that may have been left in place for legitimate reasons by developers

    * OWASP Application Security Verification Standard (ASVS) points out the challenge with respect to finding malicious code:
      - “finding malicious code is proof of the negative, which is impossible to completely validate
      - best efforts should be undertaken to ensure that the code has no inherent malicious code or unwanted functionality”

    * ASVS malicious code verification requirements offer guidance in this regard, following are a few examples:
      - verify the use of a code analysis tool that can detect potentially malicious code, such as time functions, unsafe file operations and network connections
      - verify the app does not ask for unnecessary or excessive permissions to privacy related features or sensors, such as contacts, cameras, microphones, or location
      - verify the app source code and third-party libraries do not contain time bombs by searching for date and time-related functions
      - verify the app source code and third-party libraries do not contain malicious code, such as salami attacks, logic bypasses, or logic bombs
      - verify the app source code and third-party libraries do not contain easter eggs or any other potentially unwanted functionality

    * backdoors
      - maintenance hooks are implanted intentionally in development or by error, usually by an insider
      - maintenance hook may be deliberate and useful
      - sw developers often introduce backdoors in their code to enable reentry to the system and to perform certain functions; this is known as a maintenance hook
      - backdoor can be left in a fully developed system either by design or accident

    * logic bombs
      - malicious code hidden within sw that is set to activate under certain conditions
      - triggered when a specific event (or series of events) occurs within the system
      - represents embedded malicious code inside the source code of the sw and is normally associated with an insider threat

    * exercise: use bandit tool to analyze legacy.py code

      How many results did you get?
      Total of 39 issues:
        Total issues (by severity):
          Undefined: 0
          Low: 14
          Medium: 16
          High: 9
      Total issues (by confidence):
        Undefined: 0
        Low: 0
        Medium: 14
        High: 25


      If there are too many, could you set a threshold for result reporting?
      You can filter the results by using the following options:

        -t TESTS, --tests TESTS
                        comma-separated list of test IDs to run

        -s SKIPS, --skip SKIPS
                        comma-separated list of test IDs to skip

        -l, --level     report only issues of a given severity level or higher
                        (-l for LOW, -ll for MEDIUM, -lll for HIGH)

        --severity-level {all,low,medium,high}
                        report only issues of a given severity level or
                        higher. "all" and "low" are likely to produce the same
                        results, but it is possible for rules to be undefined
                        which will not be listed in "low".

       -i, --confidence report only issues of a given confidence level or
                        higher (-i for LOW, -ii for MEDIUM, -iii for HIGH)

       --confidence-level {all,low,medium,high}
                        report only issues of a given confidence level or
                        higher. "all" and "low" are likely to produce the same
                        results, but it is possible for rules to be undefined
                        which will not be listed in "low".



      What potential defects did you find?
      I found issues in these categories: [B413:blacklist], [B324:hashlib], [B303:blacklist], [B324:hashlib], [B105:hardcoded_password_string], [B107:hardcoded_password_default], [B106:hardcoded_password_funcarg]

      Why do you think the phrasing of the previous question was “potential defects”?
      The findings should be verified by the author, because there could be false-positives.

      Does the tool attempt to provide guidance on Type I errors?
      The following finding could be thought as false-positive:

      >> Issue: [B324:hashlib] Use of weak SHA1 hash for security. Consider usedforsecurity=False
      Severity: High   Confidence: High
      CWE: CWE-327 (https://cwe.mitre.org/data/definitions/327.html)
      More Info: https://bandit.readthedocs.io/en/1.7.8/plugins/b324_hashlib.html
      Location: ./legacy.py:20:0
      19
      20  hashlib.sha1(1)
      21

      However, in the line 22, it's configured like this:
      hashlib.sha1(usedforsecurity=False)

      Therefore, the finding is in fact uncalled-for

      As can be seen above, the tool does provide guidance for it ("Consider usedforsecurity=False")




- Security Controls
  -- prevention, detection, and correction of vulnerabilities require implementation of various mitigation measures
  -- OWASP Proactive Controls represent a starting point for sw developers and offer concrete guidance around sw security

  -- controls, safeguards, and countermeasures (proactive or reactive) are used to mitigate risks to assets
  -- controls can be defined by category as technical, administrative, or physical, or defined by functionality (preventive, detective, corrective, compensating)

  -- OWASP Proactive Controls can help developers build secure sw
  -- security in every tier of the architecture must be addressed
  -- security in languages and frameworks used by developers must also be addressed


  -- Technical Software Security Controls
    * technical safeguards and countermeasures must be designed and implemented in sw in terms of various layers of defense to address sw vulns
    * no single safeguard or countermeasure is enough to address common sw security vulns
    * sw security vuln is a flaw, glitch, or weakness found in sw that can lead to security compromise
    * in this section, we present mitigation strategies for each of the OWASP Top 10 risks that were presented in the previous module


  -- Controls to Prevent Common Web Application Vulnerabilities

    * preventing injection
      - preventing injection requires keeping data separate from commands and queries
        -- preferred option is to use a safe API, which avoids use of interpreter entirely or provides parameterized interface or migrate to use of Object Relational Mapping Tools (ORMs)
        -- even when parameterized, stored procedures can still introduce SQL injection if PL/SQL or T-SQL concats queries and data or executes hostile data with EXEC IMMEDIATE/exec()
      - use positive or “approved list” server-side input validation
        -- this is not a complete defense as many applications require special characters, such as text areas or APIs for mobile applications
      - for any residual dynamic queries, escape special characters using the specific escape syntax for that interpreter
        -- note that SQL structure such as table names and column names cannot be escaped, and thus user-supplied structure names are dangerous
        -- this is a common issue in report-writing sw
      - use LIMIT and other SQL controls within queries to prevent mass disclosure of records in case of SQL injection

    * preventing broken authentication
      - where possible, implement multifactor authentication
      - do not ship or deploy with any default credentials, particularly for admin users
      - implement weak-password checks
      - align password length, complexity, and rotation policies with reputable standards
      - limit or increasingly delay failed login attempts
      - use a server-side, secure, built-in session manager that generates a new random session ID with high entropy after login

    * preventing sensitive data exposure
      - classify data processed, stored, or transmitted by an application
      - apply controls as per the classification
      - don’t store sensitive data unnecessarily -> discard it as soon as possible
      - encrypt all sensitive data at rest
      - ensure up-to-date and strong standard algorithms, protocols, and keys are in place; use proper key management
      - encrypt all data in transit with secure protocols such as TLS
      - disable caching for responses that contain sensitive data
      - store passwords using strong adaptive and salted hashing functions

    * preventing XML external entities (XXE)
      - use fewer complex data formats such as JSON, and avoid serialization of sensitive data
      - patch or upgrade all XML processors and libraries in use by the application or on the underlying operating system
      - disable XML external entity and DTD processing in all XML parsers in the application
      - implement positive (“approved listing”) server-side input validation, filtering, or sanitization to prevent hostile data within XML documents, headers, or nodes
      - verify that XML or XSL file upload functionality validates incoming XML using XSD validation or similar

    * preventing broken access control
      - except for public resources, deny by default
      - implement access control mechanisms once and reuse them throughout the application
      - model access controls should enforce record ownership, rather than accepting that the user can create, read, update, or delete any record
      - unique application business limit requirements should be enforced by domain models
      - disable web server directory listing and ensure file metadata (e.g., git) and backup files are not present within web roots
      - log access control failures, alert admins when appropriate (e.g., repeated failures)
      - rate limit API and controller access to minimize the harm from automated attack tooling
      - JWT (json web tokens) tokens should be invalidated on the server after logout

    * preventing security misconfigurations
      - repeatable hardening process that makes it fast and easy to deploy another environment that is properly locked down
        -- development, QA, and production environments should all be configured identically, with different credentials used in each environment
        -- this process should be automated to minimize the effort required to set up a new secure environment
      - minimal platform without any unnecessary features, components, documentation, and samples
        -- remove or do not install unused features and frameworks
      - task to review and update the configurations appropriate to all security notes, updates, and patches as part of the patch management process
      - segmented application architecture that provides effective, secure separation between components or tenants, with segmentation, containerization, or cloud security groups (ACLs)
      - sending of security directives to clients (e.g.,Security Headers)
      - automated process to verify the effectiveness of the configurations and settings in all environments

    * preventing cross-site scripting (XSS)
      - requires separation of untrusted data from active browser content, which can be achieved by:
        -- using frameworks that automatically escape XSS by design, such as the latest Ruby on Rails, React JS
          * learn the limitations of each framework’s XSS protection and appropriately handle the use cases which are not covered
        -- escaping untrusted HTTP request data based on the context in the HTML output (body, attribute, JavaScript, CSS, URL) will resolve Reflected and Stored XSS vulns
        -- applying context-sensitive encoding when modifying the browser document on the client side acts against DOM XSS
          * when this cannot be avoided, similar context sensitive escaping techniques can be applied to browser APIs
        -- enabling a Content Security Policy (CSP) as a defense-in-depth mitigating control against XSS

    * preventing insecure deserialization
      - only safe architectural pattern is not to accept serialized objects from untrusted sources or to use serialization mediums that only permit primitive data types
      - if that is not possible, consider one of more of the following:
        -- implementing integrity checks such as digital signatures on any serialized objects to prevent hostile object creation or data tampering
        -- enforcing strict type constraints during deserialization before object creation, as the code typically expects a definable set of classes
          * bypasses to this technique have been demonstrated, so reliance solely on this is not advisable
        -- isolating and running code that deserializes in low privilege environments when possible
        -- log deserialization exceptions and failures, such as where the incoming type is not the expected type, or the deserialization throws exceptions
        -- restricting or monitoring incoming and outgoing network connectivity from containers or servers that deserialize
        -- monitoring deserialization, alerting if a user deserializes constantly

    * preventing use of components with known vulnerabilities
      - there should be a patch management process in place to:
        -- remove unused dependencies, unnecessary features, components, files, and documentation
        -- continuously inventory the versions of both client-side and server-side components (frameworks, libraries) and their dependencies using tools such as
          * version
          * DependencyCheck
          * retire.js
        -- continuously monitor sources like CVE and NVD for vulns in the components
        -- use sw composition analysis tools to automate the process
          * subscribe to email alerts for security vulnerabilities related to components used
        -- only obtain components from official sources over secure links
          * prefer signed packages to reduce the chance of including a modified, malicious component
        -- monitor for libraries and components that are unmaintained or do not create security patches for older versions
          * if patching is not possible, consider deploying a virtual patch to monitor, detect, or protect against the discovered issue
        -- every organization must ensure there is ongoing plan for monitoring, triaging, and applying updates or configuration changes for the lifetime of the app or portfolio

    * preventing insufficient logging and monitoring
      - as per the risk of the data stored or processed by the application:
        -- ensure all login, access control failures, and server-side input validation failures can be logged with enough user context to id suspicious accounts and held for enough time to allow delayed forensic analysis
        -- ensure that logs are generated in a format that can be easily consumed by a centralized log management solution
        -- ensure high-value transactions have an audit trail with integrity controls to prevent tampering or deletion, such as append-only database tables or similar
        -- establish effective monitoring and alerting such that suspicious activities are detected and responded to in a timely fashion
        -- establish or adopt an incident response and recovery plan, such as NIST 800-61 rev. 2 or later
        -- use commercial and open-source application protection frameworks and web application firewalls with custom dashboards and alerting



  -- Common Controls
    * each category and type of vulnerability requires a specific set of safeguards and countermeasures
    * nonetheless, frequently used controls from sw security perspective also include:
      - access controls
      - change management controls
      - administrative controls, such as segregation of duties
      - audit and logging controls
      - malware detection/prevention controls

    * discussions around access control, change management, segregation of duties, audit and logging were presented in previous sections
    * discussion on malware detection/prevention follows

    * malware detection and prevention controls
      - malware remains a significant threat
      - controls discussed in this area include file integrity monitoring (FIM) and anti-malware


  -- File Integrity Monitoring (FIM)
    * objective of FIM can be related to the need for the implementation of specific safeguards to detect, alert, and report illicit or suspicious activities and unauthorized modifications to critical security files, including those of os, db and apps

    * various regulations and standards (GLBA, HIPAA, PCI DSS) have mandates and compliance requirements in this regard
    * various aspects of files including creation, modification, size, permissions, security settings and other attributes may be monitored
    * FIM solutions and tools are commonly used to detect the presence of malware

    * Anti-Malware
      - NIST publication dedicated to malware incident prevention and handling (NIST SP 800-83) highlights the prevalence (yleisyys) of current blended attacks and usage of multiple infection or transmission methods
      - it also points out the heavy reliance on social engineering methods for the spread of malware

      - malware customization causes significant problems for malware detection, because it greatly increases the variety of malware that antivirus software and other security controls need to detect and block
      - when attackers are capable of sending a unique attack to each potential victim, it should not be surprising that largely signature-based security controls, such as antivirus sw, cannot keep up with them
      - mitigation involves a defense in depth approach, using several different detection techniques to increase the odds that at least one of them can detect the malicious behavior of the customized malware

      - in addition to customization, another important characteristic of today’s malware is its stealthy nature
      - unlike most malware several years ago, which tended to be easy to notice, much of today’s malware is specifically designed to quietly, slowly spread to other hosts, gathering information over extended periods of time and eventually leading to exfiltration of sensitive data and other negative impacts
      - the term advanced persistent threats (APTs) is generally used to refer to such types of malwares


  -- OWASP TOP 10 Proactive Controls, order of priority:
    * C1: Define Security Requirements
    * C2: Leverage Security Frameworks and Libraries
    * C3: Secure Database Access
    * C4: Encode and Escape Data
    * C5: Validate All Inputs
    * C6: Implement Digital Identity
    * C7: Enforce Access Controls
    * C8: Protect Data Everywhere
    * C9: Implement Security Logging and Monitoring
    * C10: Handle All Errors And Exceptions



- Security Risks
  -- organizations operate under resource-constrained equation
  -- there is rarely enough time, money, or business justification for perpetuating (ikuistaa) the prevention of every sw defect
  -- organizations must be strategic in the pursuit of fixing or accepting technical and security debt (risks)

  -- every organization has a different risk appetite, tolerance, and landscape that make the tasks of identifying and ameliorating (parantaa) risks unique to each organization
  -- this concept remains true when we focus on more specific areas of concentration such as secure sw development
  -- for example, sw development projects will accept a certain level of risk by configuring DevSecOps pipelines to accept certain categories of issues while rejecting others
  -- perhaps a SAST tool is configured to detect only critical defects while potential moderate or warning-level findings are suppressed
  -- in this case, the organization accepts any technical debt in the form of potential security defects within the source code by refraining from a more comprehensive scan
  -- analysis tools can produce many potential findings that may include many false positives which could be prohibitively expensive to review in entirety

  -- Software Security Strategy Flow Down
    * organizations must define a security strategy that includes considerations for sw risks
    * strategy should capture the risk appetite and tolerance of the organization
    * there are several key sw documents that, when drafted correctly, incorporate considerations for security based on sw engineering life cycle (Software Security Strategy Flow Down ):


Development Phase        Description                                                          Supporting Document
---------------------------------------------------------------------------------------------------------------------------------------------------------
Requirements             Information item that identifies, in a complete, precise,            Draft SoW,
Specification            verifiable manner, the requirements, design, behavior, or            Draft Requirements Description Document (RDD)
                         other expected characteristics of system, service, or process
---------------------------------------------------------------------------------------------------------------------------------------------------------
Analysis                 Examination of acquired data for its significance and                Leverage SoW,
                         probative value to the case                                          Leverage Requirements Description Document (RDD),
                                                                                              Draft Software Requirements Specification (SRS)
---------------------------------------------------------------------------------------------------------------------------------------------------------
Design                   Process to define the architecture, system elements, interfaces,     Leverage Software Requirements Specification (SRS),
                         and other characteristics of a system or system element              Draft Software Design Document (SDD),
                                                                                              Draft Software Development Plan (SDP)
---------------------------------------------------------------------------------------------------------------------------------------------------------
Implementation           Specific requirements or instructions for implementing software      Draft Software Test Plan (STP),
                                                                                              Leverage Software Design Document (SDD),
                                                                                              Leverage Software Development Plan (SDP)
---------------------------------------------------------------------------------------------------------------------------------------------------------
Test                     Determination of one or more characteristics of an object of         Leverage Software Test Plan (STP)
                         conformity assessment, according to a procedure



  -- Creating a Software Security Strategy
    * before organization can mitigate sw risks it must identify reasonable threats and important infrastructure
    * not all threat sources possess the same capability to deliver negative impacts
    * likewise, not all organizational assets are of the same importance
    * standard risk approach starting with asset classification and protection needs can be applied to sw development or even production sw infrastructure

    * first, organizations should define the critical sw functions or components of the system when CIA (any of them) are degraded, which result in unacceptable disruptions to the continuity of sw operations
    * this exercise establishes priorities for which components may require a higher level of development scrutiny—prioritization by component criticality

    - further, organizations may use threat intelligence information and a framework like MITRE ATT&CK to identify which adversarial events may be of higher likelihood of occurrence or greater impact to an organization—prioritization by adversarial forces
    - prioritization may also be based on security budget or resources
    - if an organization has limited resources, it may only conduct static code analysis for critical defects while utilizing peer code reviews
    - each sw has different security and safety requirements based on the use case and operating environment
    - you would like the firmware on the airplane you’re traveling in to work every time
    - if you cannot log in to pay your bills today, you might not mind waiting until service is restored tomorrow

    - each organization’s decision to manage sw risk is anchored on aggregation of priorities as related to risks with the goal of reducing risk to acceptable level for production sw envs
    - strategies should facilitate addressing defects in the highest criticality components starting with the highest impact items first to maximize value



- Evaluation and Integration of Components
  -- modern applications are typically built with dependance on third-party components and libraries
  -- sw security practitioners must understand the great and real risk of using these libraries with known vulnerabilities
  -- sole task of identifying these commercial or open-source components could present a great challenge
  -- these libraries may be updated and patched over time, and this highlights the need for the continuity of the risk assessment processes using automated and manual processes

  -- development teams frequently rely on third-party components and APIs for specific functions within their overall sw products
  -- these components provide a plug-and-play opportunity for development teams and eliminate the need to reinvent and develop the capabilities from scratch
  -- for all the benefits of third-party components, there are also concerns and drawbacks, including
    * relinquishing control over the development processes of the code
    * relying on third parties to update and maintain the security of their components


  -- Third-Party Code
    * securely reuse third-party code or libraries
      - third-party code or libraries fall in the category of sw whose security risk must be managed
      - using components with known vulnerabilities is one part of the problem
      - obtaining sw components from unverified sources, acquiring components over insecure links, or even using older versions of components that are not maintained (unpatched) are a few other issues among a much longer list of issues pertaining to the use of third-party code or libraries
      - static analysis tools that are traditionally used with sw developed in-house, may also be used with third-party components

      - third-party code
        -- increased standardization of sw functions, protocols, and interfaces has enabled a large amount of code that would have been custom developed in the past to instead be written once by someone and reused by many
        -- frequently, the code is not written by the developers of the system or application in which the code will be used, or even by developers in the same organization

        -- practice of sw reuse, which began as a means of helping developers leverage their own well-engineered custom code and that of other developers in their organization, has long since evolved into use of third-party and open-source libraries of interfaces, protocols, and functions
          * sw development kits (SDKs)
          * wrappers
          * application frameworks
          * variety of other third-party sw components - the pedigree and provenance of which is often obscure and sometimes unknowable

        -- library or SDK that is commercially sold or extremely popular may still contain vulnerabilities
        -- original developers could have been careless or malicious, or it could have been dangerously tampered with at some point between its original release and its acquisition by the development team that plans to reuse it


  -- Third-party and Open Source Components - Risk Factors
    * while it is certainly important to consider the possibility and the risk of malicious code being inserted in sw at some point (particularly when sw is developed in a geographically distributed supply chain), security flaws and vulnerabilities introduced in sw inadvertently (vahingossa) due to insecure design and coding practices or errors and mistakes represent a higher risk
    * risk factors associated with third-party and open-source components include:
      - absence of an accurate inventory of all third-party/open-source components
      - outdated technology
      - unsupported components (component age)
      - repository trust
      - license



  -- Open-Source Policy
    * according to OWASP, open-source policies provide guidance and governance to organizations looking to reduce third-party and open-source risk
    * policies typically include:
      - restrictions on component age
      - restrictions on outdated and EOL/EOS components
      - prohibition of components with known vulnerabilities
      - restrictions on public repository usage
      - restrictions on acceptable licenses
      - component update requirements
      - blocked list of prohibited components and versions
      - acceptable community contribution guidelines

    * while oss policy is usually filled with restrictions, it provides an organization’s security, development, and legal teams an opportunity to create solutions for healthy oss usage



  -- Continuous Integration
    * given the current industry attention to Continuous Integration (CI), and the relevance of integration to this module, a brief overview of continuous integration is presented below
    * when sw developers work in isolation, the task of integrating their code with other team members’ code may present a challenge that can lead to merge conflicts, hard-to-find bugs, duplicated efforts, and other challenges
    * this issue may subsequently slow down the processes of software release

    * continuous integration can offer advantages over a traditional end-of-life-cycle approach, including:
      - more agility to respond to business requirements as the tempo of sw change and release increases
      - early detection of issues
      - reduction of the integration risks due to a decreased complexity
      - continuous feedback from sw users to sw development teams due to increased and incremental releases of the sw

    * continuous integration may rely on the integration of code that is checked into shared code repositories on a frequent basis
    * by automating build processes, various security checks and tests can be orchestrated using commercial and oss tools and technologies that have been introduced for this purpose

    * various tools and technologies for orchestrating the chain of actions needed for continuous integration have been introduced
    * Jenkins is a good example of oss CI server used for this purpose
    * regardless of the tools or technologies utilized, best practices for securing the CI/CD pipelines must be followed


  -- Systems-of-Systems Integration
    * collection of task-oriented or dedicated systems that pool their resources and capabilities together to create a new, more complex system which offers more functionality and performance than simply the sum of the constituent systems
    * organizations in military, civilian government, and commercial domains need to engineer systems of systems rather than stand-alone systems to meet challenges such as:
      - collaboration across independently funded and managed organizations
      - migration to a service-oriented environment
      - testing and compliance verification processes for systems of systems
      - interface particulars of systems to be integrated
      - degree of uncertainty
      - change in constituent (ainesosa) systems and interfaces
      - assurance and dependability/supportability requirements

    * ensure enough robustness and regression testing and analysis activities are incorporated into the development phase, including those that exercise quality related scenarios
    * require a risk-based verification plan that focuses on robustness of critical system services in the face of uncertainty, complexity, and change



  -- Security During the Build Process
    * sw build process must be standardized and repeatable in a secure manner
    * compilation and build utilizing automation within the development pipeline can help achieve this objective
    * security controls and checks should be factored into automated builds wherever possible
    * version control has a direct impact on the state of sw assurance
    * compiler switches may help with the enhancement of security
    * developers must investigate and address compiler warnings

    * Build Automation
      - automating tasks and activities that software development teams complete regularly, including:
        -- compiling the code
        -- packaging the binaries
        -- running tests
        -- deploying to production
        -- preparing release notes

      - developers have historically used build automation to call compilers and linkers from inside a build script rather than attempting to make the compiler calls cli
      - it is simple to use the command line
      - when passing a single source module to a compiler and then to a linker to create the final deployable object, developers find it simple enough to use cli
      - however, this is not effective for more complicated processes, such as when developers are attempting to compile and link many source code modules in a particular order
      - better alternative was required, and this was offered by the “make” scripting language
      - this allowed a build script to call in a series of compile and link steps to build a more complex sw apps
      - build automation has evolved, and contemporary automation tools now enable automation through workflows to address complex build processes

      - security controls and checks should be factored into the automated build scripts wherever possible
      - full builds and incremental builds have been common
      - builds may be triggered from within IDE, CLI, CI tools


  -- Secure Version Control
    * version control systems (CVS, SVN, GIT) are used to record changes to files over time
    * if centralized, the server contains the versioned files, and clients are used to check files out
    * if distributed, the repository is mirrored including its full history
    * regardless of whether svc is centralized or distributed, it has direct impact on the state of sw assurance and is applicable as part of development and deployment
    * changes to sw must be planned, purposeful, and documented
    * versioning, backups, check-in and check-out practices and management of sw configuration are all part of sw configuration management


  -- Anti-Tampering Techniques
    * protection of sw code in various formats (source or binary) and protection objectives (protection against disclosure of source code, against tampering with the binaries, or against reverse engineering) involves various tools, techniques, and technologies

    * there is no shortage of source code obfuscation tools; programmers may also choose to write their own
    * the objective is to introduce another layer of defense for protection against disclosure of intellectual properties, source code in this case
    * obfuscation of code in nonsource format (binary) is generally about introducing another layer of defense against reverse engineering to protect intellectual properties

    * code signing has been discussed in detail in previous domains and is a reference to digitally signing the executable code to provide assurance of the integrity of the code (no alteration or tampering) and provide confirmation of the sw’s author
    * object signing certificates may be used for this purpose
    * this requires the entity that signs the code to assure proper protection of the code signing key

    * compiler switches
      - leveraging compiler switches and techniques can be useful to provide memory protection and pointer integrity checking
      - this limits the chances of buffer overflow attacks and increases the overall security of sw
      - common compiler security switch is the /GS flag and the technique is StackGuard

      - other examples include build flag options available with gcc (where applicable), including:
        -D_FORTIFY_SOURCE          : Run-time buffer overflow detection
        -D_GLIBCXX_ASSERTIONS      : Run-time bounds checking for C++ strings and containers
        -fstack-clash-protection   : Increased reliability of stack overflow detection
        -fstack-protector          : Stack smashing protector

    * address compiler warning
      - in most cases, compilers generate object code from source code
      - they also conduct extensive analysis of code during the compilation process

      - compiler warnings contrast with compiler errors which prevent the code from compilation, until the issues are fixed
      - nonetheless, a developer should pay attention to the warnings that the compiler generates, and not discount them just because the code compiles
      - these warnings can potentially lead to other problems in sw that may be hard to identify
      - therefore, developers should take time to understand what the underlying issue is when a statement is flagged

      - examples of compiler warnings include:
        -- warnings that highlight possible usage of deprecated or unsafe functions
        -- warnings that indicate potentially unsafe type conversions
        -- warnings that indicate potential memory leaks






Audit/Auditing - 1) The process of reviewing a system for compliance against a standard or baseline. Examples include audits of security controls, configuration baselines, and financial records.  2) Independent review and examination of records and activities to assess the adequacy of system controls, to ensure compliance with established policies and operational procedures. Source: NIST SP 1800-15B
Buffer - A reference to an area (chunk) of memory. Normally set aside to temporarily hold data for processing or transfer from one place to another.
Buffer Overflow - A reference to a situation where a process tries to place more data in a buffer than it has the capacity to hold. Buffer overflow can be caused by various conditions and in various ways, both on the stack and on the heap.
Code Signing - A reference to the process of digitally signing executables/scripts with the objective of providing assurance of integrity and authenticity of the code. Object signing certificates may be used for this purpose.
Compiler - A reference to the software used to convert/translate the program code (source code) into machine-understandable format (i.e., object code).
Concurrency - A reference to an environment where multiple execution paths within the same process can exist at the same time. The objective is to have the ability to execute various parts of the program simultaneously and possibly out of order and yet not affect the expected outcome. Concurrency-related issues are not just quality issues, but also security relevant as they may introduce vulnerabilities. Time of check to time of use (TOCTOU) is an example.
Cryptographic Agility - A reference to the ability to evolve and adopt alternative cryptographic primitives with ease. This capacity to swap out one cryptographic service provider for another is an important architectural consideration during the design of cryptographic libraries/frameworks (e.g., when the original provider becomes vulnerable/broken and must be replaced with an alternative).
Dynamic Application Security Testing (DAST) - A process of testing an application or software product in an operating state.
Heap - A reference to the area of memory used for dynamic allocation of space. Failure to deallocate memory when it is no longer needed will result in memory leaks.
Injection Vulnerability - A reference to a flaw in software and a common application vulnerability that would allow externally influenced (malicious) input to be used as part of the construction and subsequent execution of a command. Variations include SQL injection, OS command injection, and LDAP injection.
Integrated Development Environment (IDE) - A reference to software development tools that at minimum facilitate editing, compiling, linking and debugging programs all in one place without having to constantly switch tools. Modern IDEs have become comprehensive and offer an array of capabilities beyond what is mentioned here (e.g., static code analysis capabilities).
Interactive AST (IAST) - Interactive AST (IAST) tools combine elements of DAST with the instrumentation of the application under test. They are typically implemented as an agent within the test runtime environment that observes operations or identifies and attacks vulnerabilities. Source: NIST SP 800-204c
Linker - Software used to combine compiled object files (e.g., yours and those of the third-party libraries) into one executable program. Static linking and dynamic linking are both common.
Log - A record of actions and events that have taken place on a computer system.
Obfuscation - A reference to the deliberate act of obscuring the code in various forms (e.g., source code, bytecode, object code). The objective is to either make the code difficult for humans to understand or to protect against decompiling and reverse-engineering. Ultimately, it is about the protection of intellectual properties.
Software Assurance - The level of confidence that software is free from vulnerabilities either intentionally designed into the software or accidentally inserted at any time during its lifecycle and that it functions in the intended manner.
Software Composition Analysis (SCA) - Automated process that identifies the open-source software in a codebase. Third-party/open-source components are an essential part of contemporary software development.
Stack - A reference to an area of memory that is used for static allocation of space for program variables and used during function calls.
Static Application Security Testing (SAST) - Also known as static source code analysis, these are tools which examine the source code for a variety of errors such as data type errors, loop and structure bounds violations, and unreachable code. Since SAST tools do not attempt to execute or simulate the execution of the code being analyzed, it’s a bit of a misnomer to call them “testing” tools.
Type Safety - A reference to the capacity of a programming language to discourage/prevent data type related issues such as unsafe type conversions/operations. Java is considered an example of a type safe language. C is an example of a type unsafe language.



QUIZ


Question 1    1 / 1 point
Which of the following is the most important use of input validation? (D5.1, L5.3)

A) Ensure input is readable.
B) Facilitates elegant interface design.
C) Ensure all required fields are filled and conform to business rules.
--> D) Prevents the execution of common injection attacks.

Correct. Input validation is a fundamental and critical layer of defense to protect against many common types of software attacks.


Question 2    1 / 1 point
Which of the following BEST represents the fundamental benefit of sandboxing?  (D5.1, L5.2)

A) Restricting access to code repositories.
B) Authentication and authorization.
C) Identification of vulnerabilities in code with no false positives.
--> D) Isolation and containment to reduce risk.

Correct. Isolation, physical or virtual, has value to security. Sandboxing increases security in most cases.


Question 3    1 / 1 point
Jeong is conducting a peer code review on Java source code and uncovers the function shown below. Which of the following best describes the security concern? (D5.1, D5.4, L5.1, L5.4, L5.10)

public boolean isNeeded(int id, int rate) {
    int minCnt = 10;
    int days = 0;
    boolean isReorder = false;
    // get inventory
    int inventoryCnt = inventory.getInventoryCnt(id);
    // number of days until minimum inventory
    while (inventoryCnt > minCnt) {
        inventoryCnt = inventoryCnt - rate;
        days++;
    }
    // set reorder true
    if (days > 0 && days < 5) {
        isReorder = true;
    }
    return isReorder;
}


A) CWE-787: Out-of-bounds Write
--> B) CWE-835: Loop with Unreachable Exit Condition ('Infinite Loop')
C) CWE-476: NULL Pointer Dereference
D) CWE-798: Use of Hard-coded Credentials

Correct. If int rate is the value of zero, an infinite loop will occur.


Question 4    1 / 1 point
Which of the following is considered a method to protect against attacks that attempt to exploit an injection vulnerability in software? (D5.1, D5.2, L5.2, L5.7)

--> A) Performing proper input validation using allow lists and block lists
B) Disallowing configuration changes
C) Handling exceptions/errors
D) Obfuscating the source code

Correct. Validating and sanitizing inputs will prevent injection related attacks.


Question 5    1 / 1 point
Which of the following BEST describes the intent of Common Attack Pattern Enumeration and Classification (CAPEC)? (D5.2, L5.6)

A) The root cause of a vulnerability
B) Specific instances of a weakness type that are exploitable
--> C) How weaknesses could be exploited
D) None of these

Correct. CAPEC describes how a weakness could be exploited. CAPEC attack pattern is a method of using a CWE to execute an attack.


Question 6    1 / 1 point
What do software maturity models measure? (D5.2, L5.6)

A) Maturity of software products, including operating systems
--> B) Maturity of software development processes and practices
C) Trustworthiness of third-party software
D) Software project milestones

Correct. Software maturity models such as SAMM and BSIMM measure the refinement of software processes and practices.


Question 7    1 / 1 point
Which of the following statements is TRUE regarding software configuration parameters? (D5.1, D5.3, L5.4, L5.8)

A) Configuration parameter access should be limited to authorized users.
B) Configuration parameters should be change controlled for integrity.
C) Typically, the most common value of a parameter is available as a default.
--> D) All of the above

Correct. Configuration parameters should be protected by limiting access to appropriate roles such as system admins. Configuration parameters should be enforced through file integrity technical controls and change board administrative controls. Generally, best practice is to provide a reasonable default configuration value.


Question 8    1 / 1 point
Diego is the project manager for your software team. He overhears part of a conversation you are having with a software developer. He hears the phrasing "cryptographic agility" mentioned several times, so he asks, "What's that?". Which of the following phrasings would you most likely use to help Diego understand? (D5.1, D5.3, L5.1, L5.8, L5.9)

--> A) It is the capacity for a software system to easily evolve and adopt alternatives to the cryptographic primitives it was originally designed to use.
B) It is the concept of using Agile software development methodologies to develop cryptographic solutions.
C) It is the theory of the relative speed of a symmetric encryption algorithm such as AES, as compared to an asymmetric algorithm such as Diffie–Hellman.
D) It is the concept of using shortcuts to develop cryptographic solutions.

Correct. Cryptographic agility is the capacity for a software system to easily evolve and adopt alternatives to the cryptographic primitives it was originally designed to use. The X.509 certificate format is an example of cryptographic agility, supporting various hash algorithms, signature algorithms, key sizes and the like to address future needs. Cryptographic agility is a common theme during the architecture and design of cryptographic libraries, APIs, and frameworks, including those found in .NET and Java.


Question 9    0 / 1 point
During an assessment of a web application that does not utilize a framework with built-in CSRF protection, you determine it is vulnerable to CSRF (cross-site request forgery). Which of the following actions is likely to mitigate the risk and should be included in your final report? (D5.2, D5.4, L5.10, L5.7)

A) Annual software security training
B) Enable built-in CSRF protection
--> C) Implement token-based mitigation
D) Implement GET requests for state changing operations

This question is difficult and likely requires a process of elimination for most learners. Annual training will not bolster the production application's security posture—eliminate. We cannot enable CSRF protection because the problem states it is not supported—eliminate. If you know about CSRF, then you know token-based mechanisms are a viable solution. Otherwise, if you are familiar with HTTP verbs, you can determine GET requests should not be used for state changing operations.


Question 10   1 / 1 point
Which of the following is an injection attack? (D5.2, D5.4, L5.6, L5.10)

A) SQL injection
B) OS command injection
C) LDAP injection
--> D) All of the above

Correct. SQL injection, OS command injection, and LDAP injection are examples of attacks that attempt to exploit injection vulnerabilities.


Question 11   1 / 1 point
What executive document captures risk appetite and tolerance for an organization and would inform the creation of a mission-level software development plan (SDP)? (D5.4, L5.10)

--> A) Security Policy
B) Statement of Work (SoW)
C) Requirements Description Document (RDD)
D) Software Test Plan (STP)

Correct. A security policy is a management generated document that describes organizational security philosophy. It typically includes a set of rules for data, assets, and networks. Security policy establishes enterprise direction, scope, and tone for security efforts.


Question 12   1 / 1 point
What software engineering phase is most likely to include drafting the software development plan (SDP)? (D5.4, L5.10)

A) Analysis
--> B) Design
C) Implementation
D) Test

Correct. The SDP will need to be created prior to use. During implementation, source code is being produced and is hopefully guided by best practices contained within the SDP.


Question 13   1 / 1 point
Which of the following statements is TRUE regarding application programming interfaces? (D5.5, L5.12)

A) APIs are inherently secure.
B) APIs are inherently insecure.
--> C) APIs are secure or insecure based on architecture, design, and implementation.
D) API security depends primarily on the programming language used.

Correct. APIs are secured based on design and implementation details


Question 14   1 / 1 point
Landon is a junior security analyst who works for a large telco company in the United Kingdom. His boss passes him in the hall and requests he propose a solution for better "SCA" during software implementation and integration of open-source components. What does SCA probably reference? (D5.1, D5.5, L5.11, L5.12)

--> A) Software Composition Analysis
B) Security Control Assessment
C) Software Control Account
D) Software Control Assessment

Correct. SCA in the context of implementation (i.e., building software) and dealing with open-source components would most likely be Software Composition Analysis.


Question 15   1 / 1 point
What is the most important aspect of code signing? (D5.6, L5.14)

A) Ensuring that the code is in the correct format for the signing process
B) Ensuring that the correct public keys are available to sign the code
--> C) Ensuring the origin and integrity of software
D) Ensuring hashing algorithms can perform both encryption and decryption

Correct. Code signing is the process of digitally signing executables and scripts to confirm the software author and guarantee that the code has not been altered or corrupted since it was signed. The process involves creating a digest by hashing the file's contents and encrypting it with the private key of the software publisher. This is done to validate authenticity and integrity. This requires the entity that signs to code to assure proper protection of the code signing key.


Question 16   1 / 1 point
An online store provides a listing page with financial charts for their items. An attacker inspects the browser requests and identifies the API endpoints used as a data source for the charts. The API pattern is: /store/{itemName}/sales.json. Using another API endpoint, the attacker can get the list of all hosted item names. The attacker writes a script to manipulate the names in the list, replacing {itemName} in the URL, and thus gains access to all your sales data. Which of the following OWASP API Security Top 10 is described here? (D5.3, D5.6, L5.13, L5.14)

--> A) Broken Object Level Authorization
B) Unrestricted Resource Consumption
C) Server-Side Request Forgery
D) Improper Inventory Management

Correct. APIs tend to expose endpoints that handle object identifiers, creating a wide attack surface of Object Level Access Control issues. Object level authorization checks should be considered in every function that accesses a data source using an ID from the user.


Question 17   1 / 1 point
Which of the following is a key advantage of continuous integration (CI) over a traditional "at end of life cycle" approach? (D5.5) (L5.12)

A) Increased complexity of software integration
B) Reduced agility to respond to business requirements
--> C) Early detection of issues
D) Decreased and infrequent releases of the software

Correct. One of the key advantages of continuous integration (CI) is the early detection of issues. By integrating code frequently and automating build processes, CI allows developers to identify and address issues sooner, reducing the likelihood of merge conflicts, bugs, and duplicated efforts. This leads to faster software release and improved software quality.


Question 18   1 / 1 point
What is the primary purpose of a separation kernel in the context of security? (D5.1, L5.5)

A) To handle complex operating system responsibilities
B) To provide virtualization features for virtual machines
--> C) To ensure the correctness of the separation function
D) To support device drivers and shell access

Correct. The primary purpose of a separation kernel is to ensure the correctness of the separation function. By factoring out the separation function from a complex operating system and creating a kernel solely focused on separation, it becomes possible to verify the correctness of the separation mechanism. This is important for maintaining the security and isolation of sensitive information in high-security systems.


Question 19   1 / 1 point
What is the difference between static and dynamic linking? (D5.6, L5.13, L5.14)

A) Static linking involves combining source code files, while dynamic linking involves running tests on the compiled code.
--> B) Static linking links all library dependencies at compile time, while dynamic linking links library dependencies at runtime.
C) Static linking is used for large-scale software projects, while dynamic linking is suitable for small-scale projects.
D) Static linking is more vulnerable to attacks than dynamic linking due to its reliance on external libraries.

Correct. In static linking, all library dependencies are resolved and linked with the executable during the compile-time process. The resulting executable contains all the necessary code from the libraries. On the other hand, dynamic linking resolves library dependencies at runtime, allowing the executable to load and link with the required libraries when the program is launched.


Question 20   1 / 1 point
Which of the following is a prevention strategy for buffer overflow vulnerabilities? (D5.2, L5.7)

A) Increasing the buffer size to accommodate more data
--> B) Avoiding the use of unsafe APIs
C) Allowing direct memory manipulation through pointers
D) Encouraging unrestricted copying of buffers

Correct. One of the prevention strategies for buffer overflow vulnerabilities is avoiding the use of unsafe APIs. Unsafe APIs, such as "strcpy," can lead to buffer overflows if not used with proper buffer size checks. By using safer alternatives or implementing buffer size checks and validation, the risk of buffer overflow vulnerabilities can be mitigated.




DOMAIN 6: Secure Software Testing
---------------------------------

- Security Testing Strategy and Plan
  -- each organization’s sw testing strategy must address functional and nonfunctional testing
  -- security testing plans will need to leverage manual and automated tools, techniques, and processes
  -- development processes and methodologies adopted by the organization (i.e., Agile and DevOps) may have a direct impact on the level of test automation

  -- ad hoc security testing is neither efficient nor effective
  -- sw enterprises must define a testing strategy that informs functional and nonfunctional test coverage based on
    * attack vectors
    * framework such as MITRE ATT&CK
    * threat types
    * intelligence sources

  -- regardless of methodology, the security testing strategy should define the organization's overall approach

  -- once an approach is established at the enterprise level, development organizations should create test plans focused on executing sw testing strategy
  -- sw testing plan should be facilitated by automated testing tools when processes are repeatable and manual processes when automation is not viable or a process will not be repeated
  -- test plan must include provisions for measuring results
  -- without pass/fail criteria, the sw test results cannot be objectively evaluated



  -- Security Testing Techniques
    * app security testing techniques are numerous
    * commonly used security testing techniques include white box, black box, and gray box testing

    * white box testing
      - technique that involves the direct analysis of source code
      - keep in mind that static analysis security testing (SAST) tools for white box testing scale well
      - testers are given knowledge of how the sw is implemented and how it works
      - focuses on how the sw does what it does

      - performed based on the knowledge of how the system is implemented
      - to test the intended and unintended sw behavior, it includes analyzing
        -- data flow
        -- control flow
        -- information flow
        -- coding practices
        -- exception and error handling within the system

      - can be performed to validate whether code implementation follows intended design, to validate implemented security functionality, and to uncover exploitable vulns
      - requires knowing what makes sw secure or insecure, how to think like an attacker, and how to use different testing tools and techniques

      - first step is to comprehend and analyze available design documentation, source code and other relevant dev artifacts; knowing what makes sw secure is a fundamental requirement
      - second, to create tests that exploit sw, a tester must think like an attacker
      - third, to perform testing effectively, testers need to know the different tools and techniques available for white box testing

      - the three above requirements do not work in isolation, but together

    * black box testing
      - although appropriate for all levels of testing (component through acceptance), is best suited for uncovering sw issues where specifications exist
      - with no knowledge of components/structures, testing remains focused on inputs and outputs and what the sw does, rather than how it is done
      - should be used primarily to assess
        -- security of individual high-risk compiled components
        -- interactions between components
        -- interactions between the entire application or application system with its users, other systems, and the external environment

      - black box penetration tests
        -- while black box penetration test results can be impressive and useful to demonstrate how vulns are exposed in a production environment, they are not the most effective or efficient way to secure an application
        -- it is difficult for dynamic testing to test the entire code base, particularly if many nested conditional statements exist
        -- if the source code for the application is available, it should be given to the security staff to assist them while performing their review
        -- it is possible to discover vulns within the application source that would be missed during a black box engagement


    * gray box testing
      - between black box and white box testing lies gray box testing, which uses limited knowledge of the program internals
      - in principle, this could mean that the tester knows about some parts of the source code and not others
      - in practice, it usually just means that the tester has access to design artifacts more detailed than specifications or requirements
      - for example, the tests may be based on an architecture diagram or a state-based model of the program’s behavior



  -- Environment
    * there are several types of environments and trust boundaries (the points where privilege levels change) that demarcate (rajata) one environment from the other
    * the threat model should include a logical and physical, end-to-end scenario, and when you are performing environment testing, all the components should be thoroughly tested

    * interoperability
      - when conducting environmental testing, certain aspects of sw need to be tested
      - this includes upstream and downstream dependency checks and configuration mismatch checks

      - upstream and downstream dependencies check
        -- it is critical to consider the context of the app or sw that you are testing as it relates to the other apps, both upstream and downstream in the environment
        -- this is particularly true in cases that address cryptography
          * for example, if an app encrypts data, then testing should make sure that the downstream application that consumes that data has a way to decrypt it and that in all upstream and downstream levels, the key to encrypt and decrypt is appropriately secured

      - configuration mismatch check
        -- to prevent cases where the sw runs perfectly in development and testing environments but has issues in production or user environments, it is critical to check for configuration mismatches when conducting environment testing

      - test harness
        -- typically used for test automation and increased productivity
        -- consists of many items such as drivers, stubs, and other tools needed to successfully run a test
        -- benefits include
          * ability to schedule a test
          * handling complex conditions that could be difficult to simulate by testers
          * keying in inputs for sw under test
          * capturing outputs generated during the test
          * flexibility/support for debugging



  -- Standards and Guidelines
    * examples of organizations that have produced documented standards and guidelines relevant to software security include:
      - International Organization for Standardization (ISO)
      - National Institute of Standards and Technology (NIST)
      - Open-Source Security Testing Methodology Manual (OSSTMM)
      - Software Engineering Institute (SEI)
      - OWASP Testing Guide

    * International Organization for Standardization (ISO) creates documents that provide requirements, specifications, guidelines, or characteristics that can be used consistently to ensure that materials, products, processes, and services are fit for their purpose

    * two of ISO standards pertinent to software security:
      - ISO/IEC 27000 series: information security management systems
        -- helps organizations keep information assets secure
        -- helps your organization manage the security of assets such as financial information, intellectual property, employee details, or information entrusted to you by 3rd parties
      - ISO/IEC 15408: information technology, security techniques, evaluation criteria for IT security

    * ISO/IEC 27001
      - best-known standard in the family, this provides requirements for an information security management system (ISMS)
        -- ISMS is a systematic approach to managing sensitive company information so that it remains secure
        -- includes people, processes, and IT systems by applying a risk management process
        -- can help small, medium, and large businesses in any sector keep information assets secure

      - ISO/IEC 27001 control sets include a group of controls related to system acquisition, development, and maintenance
      - controls in this group aim to ensure that information security is an integral part of information systems throughout the life cycle

    * ISO/IEC 27034
      - standard for app security that offers guidance on information security to those specifying, designing, and programming or procuring, implementing, and using app systems

    * ISO/IEC 15408
      - establishes the general concepts and principles for the Common Criteria for Information Technology Security Evaluation
      - Common Criteria is an international framework and standard for security specification and evaluation of a variety of IT products (e.g., operating systems, TPM chips)
      - Common Criteria ratings can be used when evaluating products and building confidence levels that certain products can meet an organization’s needs and requirements
      - Evaluation Assurance Levels (EAL) of 1 through 7 are defined
      - selecting the right protection profile is the key to assurance and confidence building

    * ISO/IEC 29119
      - defines a set of standards for sw testing that can be used by any organization when performing any form of sw testing and using any life cycle
      - provides context for the role of sw testing in quality management and as part of validation and verification of sw systems
      - details implementations for tools including static and dynamic

        -- ISO/IEC/IEEE 29119-1: Software test definitions and concepts
        -- ISO/IEC/IEEE 29119-2: Software test process model for any SDLC
        -- ISO/IEC/IEEE 29119-3: Software test documentation and templates
        -- ISO/IEC/IEEE 29119-4: Software test design
        -- ISO/IEC/IEEE 29119-5: Software test case development

    * ISO/IEC 25000
      - defines a standard known as Software Quality Requirements and Evaluation (SQuaRE) aimed at organizing and enhancing sw quality requirements and evaluation processes



  -- Additional Standards and Guidelines
    * NISTIR 8397
      - guidelines on Minimum Standards for Developer Verification of SW provides guidance for complying with the US Executive Order (EO) 14028, Improving the Nation’s Cybersecurity
      - although focusing on US EO compliance, it provides excellent general coverage of secure sw testing processes from threat modeling to memory-safe source code compilation

    * Open-Source Security Testing Methodology Manual (OSSTMM)
      - offers guidelines for conducting comprehensive security testing that results in consistent and measurable results
      - as published and described by ISECOM:
        -- "Open-Source Security Testing Methodology Manual (OSSTMM) provides a methodology for a thorough security test, herein referred to as an OSSTMM audit
        -- OSSTMM audit is an accurate measurement of security at an operational level that is void of assumptions and anecdotal evidence
        -- as a methodology it is designed to be consistent and repeatable
        -- as an open-source project, it allows for any security tester to contribute ideas for performing more accurate, actionable, and efficient security tests

    * OWASP Testing Guide
      - covers the procedures and tools for testing the security of applications and can be used as part of a comprehensive application security verification
      - can be used as a reference and as a methodology to help determine the gap between existing practices and industry best practices
      - allows organizations to compare themselves against industry peers, to understand the magnitude of resources required to test and maintain sw, or to prepare for an audit
      - can benefit developers, software testers, and security specialists



  -- Crowdsourcing
    * historically, the concept of turning to the public to seek individuals’ ideas, opinions, feedback, and even contributions has produced good results for organizations
    * crowdsourcing is in general about involving a diverse group of talent in producing creative work

    * as an emerging form of outsourcing sw development, it relies on collective power of the development community and specialized skills within this community to achieve innovation
    * relies on the dissemination of tasks among considerably large and diverse groups of independent workers

    * Crowdsourced Security
      - crowdsourcing has created a business model, and many services/activities may subsequently be crowdsourced
      - given the current shortage of cybersecurity talent, with respect to security testing, communities of ethical vulnerability researchers have been established

    * Bug Bounty
      - publicly available, community-driven activities with the objective of finding vulnerabilities or exploits in sw
      - penetration testers, who are sometimes called bug hunters, dedicate their time and efforts to this task in return for recognition and compensation
      - leveraging a community of researchers and active testers that commit their efforts to identifying sw issues can benefit all organizations in various industries




- Security Test Cases
  -- testing sw involves the development of both functional and nonfunctional test scenarios and cases
  -- functional testing aims to assess the functionality of the sw in respect to functional specifications
  -- nonfunctional testing aims to assess nonfunctional aspects of sw, including its resilience to misuse or malicious acts

  -- considering that the primary objective of security testing is to ensure that sw is free of vulns and that it can withstand attacks, the task of creating security test cases to achieve that objective is a delicate one that requires specific skills and know-how

  -- testing may be functional or nonfunctional
  -- security testing focuses on detecting pre-compilation weaknesses or post-compilation vulns in sw systems
  -- objective of security testing is to identify and remediate issues within the SDLC before the organization suffers degradation in capabilities and continuity
  -- from a business perspective, security test activities are often conducted to reduce overall project costs, protect brand, reduce litigation expenses, conform to regulatory requirements, or enhance the confidence of receivers



  -- Guidelines for Security Testing
    * before delving into different security testing methods and techniques, review the following list of practices that can improve security testing in general and should be represented in an organizational security testing strategy:

      - test early & often
      - have balanced approach
      - implement proper tools & techniques
      - develop metrics to measure progress

    * comprehensive security testing should include testing of every component of sw system, not only the web interface components (e.g., web portals), but also where applicable, the mobile apps (e.g., iOS and Android versions), APIs (e.g., RESTful APIs), database components, and so on

    * testing goals must identify not only those vulns that are due to insecure coding but also those that are due to flaws in design and architecture
    * sw security testing must ensure that sw meets the security objectives of confidentiality, integrity, and availability
    * third-party frameworks and libraries should also be included where applicable

    * in the case of web applications in particular, testing should include authentication, authorization, session management, data validation, logging, configuration, and cryptography, among other functions

    * test environment
      - one way to understand the importance of setting up a proper test environment is to think of it as a platform that has been specifically built for executing test cases
      - test environment should be separate and isolated from the development environment
        -- at a minimum, it should involve hw, sw, properly configured network, and test execution tools
        -- additionally, the test environment should simulate the real production environment
        -- configuration drifts between environments can be problematic.



  -- Functional and Nonfunctional Security Testing
    * functional
      - typically performed by QA team, this security testing focuses on activities that will validate that a particular security feature indeed works as expected
        --> thus, enabling users to perform a certain function as specified by the requirements

      - testing may be performed from different perspectives:
        -- by prioritizing the requirements per risk criteria and executing the tests accordingly
        -- by relying on scenarios that represent the day-to-day business use and constructing them based on knowledge of the business processes

      - example in this category would be the testing conducted by QA team to ensure that the password reset function in the app works as specified by the requirements

    * non-functional
      - as the term implies, this security testing consists of testing the nonfunctional security aspects of sw
      - example would be the testing that is conducted to ensure that sw fails in a secure state, and the stack trace is not exposed in the case of encountering runtime anomalies

      - besides security testing, this testing is also used in the industry to imply an umbrella for several test methods
      - this type of testing typically includes scalability, interoperability, and performance:

        -- scalability testing primarily measures the ability of the sw to scale as necessary and demanded by business/users
          * it is important to understand that scalability of sw system must be supported by the scalability of its various components (e.g., the database component)

        -- interoperability testing aims to provide assurance of end-to-end functionality between communicating systems, as most sw systems do not operate in a silo

        -- performance testing may be an umbrella to include other types of testing
          * however, it primarily ensures that acceptable levels of performance can be achieved under various conditions.

      - from a general security perspective, security practitioners should ensure that testing provides adequate coverage of critical components with respect to CIA
      - non-security-centric examples above for scalability and performance map to the security goal of availability


  -- Attack Surface Validation
    * security testing can support the validation of the attack surface of an app
    * for example, OSS/commercial dynamic and vulnerability assessment tools can be used to enumerate an app structure and identify parts of it that are accessible over the web
    * iterative attack surface analysis is important to ensure that security testing strategy, policies, and procedures are aligned with organizational risk management needs
    * security, including testing, must adapt to maintain alignment with organizational risk posture


  -- Penetration Tests
    * NIST defines pen testing as security testing in which evaluators mimic real-world attacks to identify ways to circumvent the security features of an app, system, network
    * pen testing often involves issuing real attacks on real systems and data, using the same tools and techniques used by actual attackers
    * most pen tests involve looking for combinations of vulns on a single or multiple systems that can be used to gain more access than could be achieved through a single vuln
    * more succinctly, OWASP defines sw pen testing as the art of testing a running app remotely to find security vulns without knowledge of the app inner workings
    * regardless of definition, pen testing involves simulating attacks to uncover vulns

    * typically, pen test team can access an app as if they were users
    * testers act as though they are attackers and attempt to identify and exploit vulns
    * in many cases, testers may also receive standard or administrator user accounts as well
    * pen tests, like other test methods, frequently involve a combination of manual testing techniques and specialized automated pen testing tools

    * objective of pen testing is to observe whether the sw resists or tolerates the attacks and how it behaves when it cannot do so
    * pen testing pays particular attention to those aspects of sw behavior, interactions, and vulns that cannot be observed using other test techniques during the sw development, integration, and staging for deployment

    * as described in NIST Special Publication 800-115: Technical Guide to Information Security Testing and Assessment, penetration tests should answer the following questions:
      - how cyber resilient is the software system?
      - what is the level of sophistication or effort required to execute a compromise?
      - can countermeasures to adequately mitigate software vulnerabilities be identified?

    * according to this NIST publication, penetration testing involves four phases: planning, discovery, attack, and reporting


  -- Penetration Testing Phases
    * in general, penetration testing may have many components and may also involve the security assessment of network, application, wireless, and even physical security
    * whereas NIST has four phases, most other penetration testing methodologies have five phases:

      - planning
        -- the senior management of the organization and the penetration testers will identify rules of engagement (approval to begin test, test windows), and set testing goals
        -- then, management approval is finalized and documented
        -- the planning phase sets the groundwork for a successful penetration test
        -- no actual testing occurs in this phase

      - discovery
        -- start of actual testing and covers information gathering and scanning
        -- network port and service identification is conducted to identify potential targets
        -- other techniques are used to gather information on host name, IP address, system information, and application and service information
        -- in some cases, techniques such as physical walk-throughs of facilities and examination of trash bins may be used to collect additional information on the targeted network
        -- these efforts may also uncover additional information to be used during the penetration tests, such as passwords written on paper

      - vulnerability mapping
        -- involves comparing the services, applications, and operating systems of scanned hosts against vuln databases, as well as the testers’ own knowledge of vulns

      - attack
        -- executing an attack is at the heart of any penetration test
        -- the process of verifying previously identified potential vulns by attempting to exploit them
        -- if an attack is successful, the vuln is verified, and safeguards are identified to mitigate the associated security exposure
        -- if an attack is unsuccessful, the tester should attempt to exploit another discovered vuln
        -- successful attack equates to verification of vuln
        -- unsuccessful attack does not mean that the vuln is not exploitable

      - reporting
        -- occurs simultaneously with the other phases of the penetration test
        -- in the planning phase, the assessment plan is developed, along with rules of engagement
        -- report has various sections, including an executive summary, summary and/or details of identified vulns, attack scenarios and results, and mitigation recommendations


  -- Fuzzing
    * black box testing technique
    * done through automated means (using fuzz tools) and is intended to find implementation bugs by injecting malformed/semi-malformed data
    * NIST relates fuzz testing to fault injection in that invalid data is input into the app via the environment, or input by one process into another process
    * implemented by tools commonly referred to as “fuzzers,” which are programs or scripts that submit some combination of inputs to the test target to see how it responds

    * attack vector is the I/O of the system being fuzzed and may include the user interface, command line, import/export capabilities, URLs, forms, etc
    * fuzzing programs fall into two different categories:

      - generation-based
        -- there is an understanding of the file format or protocol
        -- involves “generating” the fuzz inputs from scratch based on the specification or format
        -- also known as “smart” or “intelligent” fuzzing

      - mutation-based
        -- injecting the fault values is done blindly
        -- also known as “dumb” fuzzing, as in lacking understanding of the format or structure of the data
        -- one example can be just replacing or appending a random section of data

    * various fuzzing tools are designed to be used at different points in the sw development life cycle
    * some should be used in tandem with unit testing, others with integration testing, and still others in acceptance testing

    * advantages
      - when an automated process or tool fuzzes various entry points of the sw indiscriminately with no assumptions, quickly, repeatedly, and with ease, issues such as memory leaks or buffer overflows that otherwise may have gone undetected could surface
      - this makes fuzz testing useful for regression tests

    * challenges
      - no single method of testing can uncover all the security-relevant issues in sw
      - each testing method is best suited for uncovering types of vuln
      - however, the two main challenges with fuzzing are setup and data analysis
        -- fuzzing tends to require special setup to include a testing harness
        -- fuzz testing is noisy and produces large amounts of data, which often include false positive results



  -- Scanning
    * scanning hosts, networks, and apps for vulns is common practice, and in many cases required
    * PCI-DSS is an example of an industry standard that has requirements in this regard

    * vulnerability scanning
      - can help identify outdated sw versions, missing patches, and misconfigurations as well as validate compliance with or deviations from an organization’s security policy
      - this is done by identifying the OS and major sw apps running on the hosts and matching them with information on known vulns

      - vuln scanner is a relatively fast and easy way to quantify an organization's exposure to surface vulns
      - surface vuln is a weakness that exists in isolation, independent from other vulns
      - system behaviors and outputs in response to attack patterns submitted by the scanner are compared against those that characterize the signatures of known vulns, and the tool reports any matches that are found.

      - tools available for automated vuln scanning at the app-level focus on web applications, web services, and mobile apps
      - in addition, other scanning products are available for assessing vulns in various components of software’s typical execution environment, including scanners for popular web servers, database management systems, virtual machines, and OS

      - many scanners combine simulated actions of client or browser to submit inputs and observe the target’s resulting behavior, scanning for configuration errors, dangerous static conditions, and violations such as sensitive data leaks or communications over prohibited network ports
      - like virus scanners and intrusion detection systems, most vuln scanners are signature-based and need to be continuously updated with new signatures from their vendors


    * application vulnerability scanners
      - scan executing sw for vuln patterns
      - although vuln scanners can detect simple vulns because they are essentially automated pattern-matching tools, they cannot pinpoint risks associated with aggregations of vulns or vulns that emerge from unfamiliar, anomalous combinations of input and output patterns

      - some web app vuln scanners attempt to overcome the limitations of signature-based scanning by performing combination of simulated reconnaissance attacks and fuzz tests to probe the app for additional known and common vulns that typically elude signature-based scans

      - many vulns detected by signature-based scanners may be located in sections of code that are unused, or dead
      - the removal of dead code before compiling the application will often significantly reduce the number of vulns that would otherwise be found through automated vuln scanning
      - however, it can be very challenging to locate dead code in third-party software, especially when the third-party source code is not available


    * content scanning
      - may be done to achieve different objective
      - for example, it may be done to inspect traffic for either malicious threats or confidential data leakage
      - what we mean by confidential data leakage is data an employee should not be sending out, such as a company’s financial statement
      - encrypted traffic presents a challenge in this regard

      - one of the main concerns regarding using content scanning is that of privacy
      - in those cases, exceptions should be considered to exclude sites such as financial sites
      - another possibility is to alert users (through a warning page) if their content is being inspected



  -- Cryptographic Validation
    * testing, evaluation, and validation of cryptographic modules in accordance with a set standard
    * example: cryptographic module validation program established by NIST, where vendors work with designated security testing labs to test their modules against applicable standards

    * it should also be noted that as part of a comprehensive sw testing program, an organization should make the effort to ensure it is adhering to best practices and standards
    * automated static analysis tools and manual security code reviews as part of white box testing effort can help identify risky operations (usage of unsafe randomization functions for generating tokens) or deprecated and weak algorithms/APIs

    * validation of cryptographic modules is valuable for compliance purposes
    * FIPS 140-2 is a perfect example of such validation, which may be achieved through NIST specified laboratories

    * Pseudorandom Number Generator (PRNG)
      - in the context of cryptography, any weakness in the process of generating true random numbers could result in patterns in the cipher
      - this may result in a substantial reduction in the work factor for the adversaries to a point where a brute force attack may be economically feasible
      - sw is far from perfect when it comes to randomization, so where and when possible, hw-assisted cryptography is valued

      - NIST SP 800-90A: Recommendation for Random Number Generation Using Deterministic Random Bit Generators, states that:
        -- there are two fundamentally different strategies for generating random bits
        -- one strategy is to produce bits non-deterministically, where every bit of output is based on a physical process that is unpredictable
          * this class of random bit generators (RBGs) is commonly known as non-deterministic random bit generators (NRBGs)

        -- the other strategy is to compute bits deterministically using an algorithm
          * this class of RBGs is known as Deterministic Random Bit Generators (DRBGs)

      - pseudorandom number generators (PRNGs) are also deterministic random bit generators
      - PRNG is an algorithm for generating a random sequence of numbers
      - due to the static nature of the initial values used in sw-based random number generators, the use of hw is recommended for this purpose


    * Cryptographically Secure Pseudorandom Number Generator (CSPRNG)
      - PRNG with properties that make it suitable for use in cryptography
      - many aspects of cryptography require random numbers, such as:

        -- key generation
        -- nonces
        -- one-time pads
        -- salts

      - there are many methods for testing the randomness of binary sequences
      - NIST Test Suite is a statistical package consisting of 15 tests that were developed to test the randomness of (arbitrarily long) binary sequences produced by either hw or sw -based cryptographic random or pseudorandom number generators

      - one example from this test suite is Frequency (Monobit) Test:
        -- focus of the test is the proportion of zeroes and ones for the entire sequence
        -- the purpose of this test is to determine whether the number of ones and zeroes in a sequence are approximately the same as would be expected for a truly random sequence
        -- test assesses the closeness of the fraction of ones to ½, that is, the number of ones and zeroes in a sequence should be about the same
        -- all subsequent tests depend on the passing of this test


    * Cryptographic Validation
      - it’s important for secret values to be unpredictable
      - entropy is the amount of uncertainty that an attacker must face to determine the value of a secret
      - it is difficult to create sources of entropy for random bit generators; true randomization is challenging because we generally fall into creating patterns even if we try not to

      - testing, or validating, entropy sources is necessary to meet requirements
      - NIST recommends the following to validate an entropy source:
        -- entropy source may be submitted to an accredited lab for validation testing by the developer or any entity with an interest in having an entropy source validated
        -- after the entropy source is submitted for validation, the lab will examine all documentation and theoretical justifications submitted
        -- the lab will evaluate these claims and may ask for more evidence or clarification


    * Entropy-as-a-Service
      - due to the fundamental importance of entropy and cryptography in general for securing data at rest and in transit, discussions around the concept of Entropy-as-a-Service (EaaS) have also been presented, especially regarding the Internet of Things (IoT):
        -- standard deterministic computers have trouble producing good randomness, especially IoT-class devices that have little opportunity to build entropy locally before they begin network communications
        -- best sources of true randomness are based on unpredictable physical phenomena, such as quantum effects, but they can be impractical to include in IoT devices
        -- finding ways to unlock the full potential of cryptography to secure data on the IoT can offer hope for a better future



  -- Misuse and Abuse Test Cases
    * earlier, we discussed the relationship between use cases and misuse or abuse cases
    * we also highlighted the importance of creating misuse and abuse cases

    * in general, a test case answers the question, “What am I going to test?”
    --> you develop test cases to define the things that you need to validate to ensure that the system is working as expected

    * it is common for a test case to have an ID, test scenario, test case description, test steps, prerequisite(s), test data, and environment information, among other parameters

    * use case testing involves testing common scenarios where actors (users or other systems) interact with the system to complete a task and achieve a goal
    * for this reason, use cases can be utilized as the basis for the creation of useful test cases that can lead to uncovering defects based on real usage of the system

    * misuse and abuse cases can help with the specification of appropriate needed functions and controls to block the intended misuse and abuse
    * controls that were subsequently identified must have been incorporated into the design of the sw and implemented to provide the intended functions
    * during the testing phase of the life cycle, test cases should be created to verify the implementation and effectiveness of those identified controls


  -- Simulation
    * is the imitation of the operation of a real-world process or system
    * act of simulating something first requires that a model be developed
    * this model represents the key characteristics, behaviors, and functions of the selected physical or abstract system or process

    * simulating production environment and production data
      - sw should be tested in an environment that is configured as closely to the target production environment as possible using production-like data
      - otherwise, test results may not be very accurate
      - this may require the QA environment to mirror the production environment or simulate it closely
      - production data in non-production environments can be a problem

      - sw that works well in one environment may experience issues when deployed to a hardened production environment
      - configuration mismatch between environments historically has presented challenges to organizations that develop sw
      - however, this problem can be less of an issue in the cloud, as infrastructure management services can simplify the process
      - for example, using templates to set up resources consistently and repeatedly to replicate an environment is much easier and more efficient than manual and error-prone processes to replicate and configure all the resources needed by the app

    * synthetic workloads
      - effective testing and evaluation of sw may require workloads with certain characteristics for the purpose of analysis and benchmarking the performance of the sw
      - generation of these workloads for testing purposes (testing the performance of a hypervisor’s scheduling algorithm) may pose a challenge to the organization
      - synthetic workload generators may be used for this purpose


    * simulation may also be used for the purpose of monitoring
    * the primary objective of monitoring in this context is to have feedback (preferably continuous) from production, that is, receive information about the performance and usage patterns of an application in production

    * this information can then be used for various purposes, including achieving high availability or minimizing time to detect or mitigate
    * monitoring in this sense is a proactive measure

    * two common methods of monitoring include:
      - Real User Monitoring (RUM)
        -- concerns monitoring user interactions (with websites or applications)
        -- interactions are recorded for the purpose of collecting performance data and improving the user experience
        -- an example is monitoring by SaaS providers to manage service quality

        -- also used to perform tests before website or application changes are rolled out to production
        -- typically does provide repeatable measurements over time, which contrasts with synthetic monitoring

      - Synthetic Monitoring
        -- scripts are deployed for the purpose of simulating the path taken by an end user
        -- web browser emulation capabilities or scripts may be used to capture performance data during each run

        -- commercial or open-source tools for synthetic monitoring are available, or you may choose to develop your own if appropriate
        -- these tools can be set up to perform simple tests or more complex tests involving multiple steps to meet specific objectives and needs
        -- for example, tools may allow the user to set test frequency, test location, and test retries, and may allow for success criteria to be defined for each test

    * synthetic transactions
      - refer to transactions that serve no business value other than to exercise the system programming and infrastructure
      - can be passive or active

      - passive synthetic transactions create no residual impact to the system
      - in the case of an order status application, a synthetic transaction would be a status query on behalf of a “dummy” customer, which is maintained by the testing organization

      - in active synthetic transactions, the transaction itself is processed and stored within the application
      - an example of this would be a “dummy” order placed by a “dummy” customer to test the order-processing attributes of the system
      - care must be taken to prevent the transaction from being processed by later processes (such as picking and shipping), or from creating an impact within the financial subsystem
      - if active synthetic transactions are to be used, then special data setup is often required to prevent any impact across the production environment



  -- Failure Testing
    * under certain conditions, defects in sw may be executed and result in the sw failing to perform its required function(s)
    * not all defects result in failures; some may stay inactive in the code, and we may never notice them
    * for example, defects in dead code will never result in failures

    * it is not just defects that give rise to failures
    * failures can also be attributed to other causes:

      - the operational environment may cause conditions that might prevent or change the execution of sw
      - human error in interacting with the sw may result in a wrong input value being entered or an output being misinterpreted

    * these issues cause incidents to be reported
    * to deal with the incidents, the programmer needs to determine whether this incident has occurred because of the failure or for some other reason

    * during sw testing, errors, defects, and vulnerabilities may be identified
    * errors due to mistakes made by developers may result in flaws or defects in sw
    * these flaws may introduce vulns into the sw, threatening the security objectives of CIA
    * once again, it is important to note that vulns are not always due to coding errors and may be traced back to design flaws, configuration settings, etc.

    * fault injection
      - as a form of testing, fault injection is about purposely introducing errors to assess how the sw behaves in the presence of error conditions
      - may be done at compile time or run time

      - compile-time injections
        -- fault injection technique where source code is modified to inject simulated faults into a system

      - run-time injections
        -- makes use of sw trigger to inject a fault into a sw system during run time

      - when the instrumented code is compiled and executed, the tester can trace between the executing code and the instrumented source code to observe the sw behavior and state changes when execution reaches each instrumented code portion
      - in this way, the tester can observe and even quantify the secure and insecure state changes that result from the instrumented faults, as well as the ways in which such faults propagate throughout the program (as traced through its source code)

      - source code fault injection is particularly useful for detecting incorrect use of pointers and arrays, race conditions, and use of dangerous system calls
      - when new attack patterns and intrusion techniques emerge, the source code can be re-instrumented with representative faults
      - note that while source code fault injection achieves greater code coverage and a lower false positive rate than other testing methods, it also requires a deeper understanding of the sw that is being tested

    * stress testing
      - type of nonfunctional testing that involves testing beyond normal operational capacity (often to a breaking point) and observing the results
      - can help ensure that sw exhibits correct behavior under stressful conditions
      - although large volumes of data or many concurrent users can create stressful conditions for the sw, stress testing should not be confused with volume testing or load testing

      - basic differences between these three forms of testing are summarized below:
        -- volume testing uses a large amount of data
        -- load testing uses a large number of users
        -- stress testing may use too many users and too much data

      - in the case of a web application, here are some ways in which stress can be applied to a system:
        -- double the baseline number for concurrent users/HTTP connections
        -- randomly shut down and restart ports on the network switches/routers that connect the servers (e.g., via SNMP commands)
        -- take the database offline, then restart it
        -- run processes that consume resources (CPU, memory, disk, network) on the web and database servers

    * break testing
      - creating tests that will fail for various reasons, such as latency, invalid responses, and incomplete/unacceptable data
      - list of what break testing should test is long
      - examines the response of system in case of hw or sw failures, such as lost connections to database, network failures, and anything else that is unpredictable and breaks the normal path of execution
      - confirms the expected system behavior in case of any breakage
      - restart and recovery part of the overall architecture plays a very important role in defining the conditions for break testing
      - needs to be facilitated by the infrastructure support groups and development team (needed to create the scenario and analyze the results)



  -- Functional Testing
    * if one considers functional testing as a form of black box testing, then the logical conclusion is that our objective is to confirm that the sw meets its functional requirements and is behaving as expected

    * test automation through scripts and tools designed for this purpose can go a long way toward enhancing the testing process, whether testing web applications, mobile applications, web APIs, and so on
    * whereas considering opportunities for automation is a great idea, abandoning manual testing in favor of complete automation may not be the wisest choice
    * not all tests are good candidates for automation

    * various types of testing can be put under the umbrella of functional testing
    * common to nearly all sw development initiatives are the functional testing methods best known as unit testing, integration testing, and regression testing


    * unit testing and code coverage
      - unit testing is about developers testing small units of code (often functions/methods) in isolation using stubs and drivers to address dependencies of the unit of code tested
      - sw unit is a function, class, procedure, or interface that is testable
      - unit testing focuses on the input, processing, and output of the sw unit

      - by ensuring that the individual parts are working correctly, the developer ensures that when a specific set of inputs is supplied, the proper output values are returned
      - unit testing not only ensures that issues can be discovered in the early stages of the life cycle, but also is useful in maintaining and changing the code
      - since the issues are found early in unit testing, the cost of fixes is reduced, and the process of debugging is simplified

      - it is a commonly accepted principle within sw industry that sw bugs exposed earlier in the development process are much cheaper to fix than those discovered late in the process
      - for example, sw bugs uncovered by a developer during unit tests generally involve only the developer and require a relatively small amount of effort to diagnose and correct
      - conversely, the same sw bug identified after deployment may involve many personnel and internal processes to diagnose and correct, and therefore may cost significantly more
      - reference:  https://www.us-cert.gov/bsi/articles/best-practices/security-testing/risk-based-and-functional-security-testing#buscase

      - test cases must be created to provide adequate code coverage – that is, the percentage of the code covered by tests
      - even with complete code coverage, there is no guarantee that the code is free of bugs, as there may be bugs that are not identified by the existing test cases


    * integration testing
      - once units of code have been tested in isolation, the next logical step is to test the interfaces between these units
      - integration testing is done following unit testing
      - different approaches to integration testing have been proposed, each with its own pros and cons

      - systems seldom run in silos without dependencies on other sw systems and processes, both upstream and downstream
      - another form of integration testing is system integration testing, which focuses on the verification of proper execution and functionality of sw components and interfaces between modules within the solution


    * regression testing
      - when sw is touched (adding new features or bug fixes), there is always the possibility that things that used to work may experience issues after the introduction of the change
      - this makes regression testing necessary to ensure that the existing functionality remains intact

      - automation of regression testing is widely adopted to minimize the tedious and repetitive steps and tasks of testing
      - automation may rely on sets (libraries) of unit test cases that are automatically executed during each regression
      - depending on the nature of the sw change, test cases may have to be prioritized for testing purposes


  -- Continuous Testing
    * continuous delivery in DevOps environments requires the concept of testing that hovers around the process of testing early, often, and wherever possible
    * continuous testing methodology typically requires great degree of automation and contrasts with the traditional testing model of handoff from one team to another (dev to QA)




- Documentation Verification and Validation
  -- documentation and other non-source artifacts need to be validated and verified
  -- security practitioners should ensure documentation verification and validation are incorporated into the security testing strategy and that testing exists within the system engineering life cycle
  -- this is especially true for regulated industries or sensitive information, when lack of diligence (ahkeruus) may result in undo risk
  -- all supporting artifacts, including documents, must be accurate, complete, and compliant with organization-defined standards and requirements

  -- sw documentation includes installation procedures and setup instructions, error messages, and documents that inform customers of features after a new build
  -- almost all sw is accompanied and delivered with some level of documentation
  -- installation and setup instructions, documentation of error messages, user guides, and release notes are common artifacts that need to be verified and validated
  -- general tools and techniques are similar across various artifacts

  -- OWASP Application Security Verification Standard (ASVS)
    * lays out control objectives and security verification requirements for sw
    * may be used to assess degree of trust in web apps, provide developers with guidance on security controls, and basis for specifying app security verification reqs in contracts

    * has two main goals:
      - help organizations develop and maintain secure applications
      - allow security service vendors, security tool vendors, and consumers to align their requirements and offerings

    * application security verification levels
      - ASVS defines three security verification levels, with each level increasing in depth
      - each ASVS level contains a list of security requirements
      - each of these requirements can also be mapped to security-specific features and capabilities that must be built into sw by developers

        -- ASVS level 1
          * app achieves level 1 if it adequately defends against app security vulns that are easy to discover and included in OWASPTop10 and other similar checklists
          * bare minimum that all apps should strive for

        -- ASVS Level 2
          * app achieves level 2 (or standard) if it adequately defends against most of the risks associated with sw today
          * typically appropriate for apps handling significant b-to-b transactions, including those that process healthcare information or implement biz-critical or sensitive functions

        -- ASVS Level 3
          * highest level of verification within the ASVS
          * typically reserved for apps requiring significant security verification, such as those within areas of military, health and safety, critical infrastructure, etc

    * ASVS Requirements
      - requirements in more than a dozen areas
      - requirements were developed with the following objectives in mind:

        -- use as a metric
          * provide app developers and owners with a yardstick with which to assess the degree of trust that can be placed in their web apps

        -- use as a guidance
          * provide guidance to security control developers as to what to build into security controls to satisfy app security requirements

        -- use during a procurement
          * provide a basis for specifying app security verification requirements in contracts

      - requirement areas include:
        -- Architecture, Design and Threat Modeling
        -- Authentication
        -- Session Management
        -- Access Control
        -- Validation, Sanitization and Encoding
        -- Stored Cryptography
        -- Error Handling and Logging



- Undocumented Functionality
  -- when an organization is not aware of sw functionality or the intention of source code, the results are an intangible risk to the organization
  -- business decisions cannot be made about risk treatments or controls if the risks are unknown
  -- security practitioners cannot defend effectively against every unknown
  -- security through obscurity is never a solid strategy for organizational security or secure sw

  -- undocumented functionality and source code equate to undefined organizational risk
  -- sw development plans should define adequate documentation, and sw test plans should provide methods to verify proper documentation exists as an acceptance criterion

  -- comments and naming conventions
    * self-documenting source code
      - source code that is readable and easy to understand without extensive documentation is often considered self-documenting code
      - problem with this definition is that self-documenting code is subjective
      - perhaps the developer who writes the code finds tightly interlaced regular expressions, lambda functions, and ternary operators used together in a nested fashion to be self-documenting, but to everyone else, it looks like obfuscated nonsense

    * preventing unreadable code
      - readability issues and uncommented code can be prevented by proactively checking the source for compliance using a lint or static analysis tool during gated check-in
      - these tools should align with the SDP guidance and enforce consistent style
      - there are many static analysis tools that can be used for this purpose, most of which are language specific
      - for checking our C example above, a development process could use cpplint
      - the development pipeline could potentially force styling for readability using an auto-formatter such as clang-format
      - alternatively, manual peer review checklists could include concerns for improperly commented source code

    * identify undocumented functionality
      - undocumented functionality in apps is functionality that exists but is not officially documented or recognized
      - there are several problems associated with using undocumented functions
        -- who is responsible for maintaining them and managing associated security risks?
        -- what is the intended function?
        -- without knowing the intended function, how can the function be tested properly?

      - the absence of documentation means that you really do not know how the function should work
      - therefore, you cannot distinguish between a bug and intended behavior
      - additionally, the function may change in future versions, which may range from implementation changes to the function being totally removed and adversely impacting apps that invoke and rely on such undocumented functions

    * preventing undocumented code
      - several frameworks exist that can generate documentation based on source code
      - when leveraged correctly, these frameworks produce sw documentation from commenting, naming, and source code structure
      - additionally, when documentation is more closely integrated into the development process, it provides a higher level of assurance that functionality will be documented properly throughout the life cycle
      - each time sw is built, documentation could theoretically be generated simultaneously to reflect the changes to functions
      - swagger UI is one of many examples of an open-source tool for autogenerating documentation for functionality




- Security Implications of Test Results
  -- security test results can provide evidence in support of various specific objectives
  -- these objectives include enhancement of the security posture of the app (reduction of attack surface) and compliance with security regulations and information security standards.

  -- analyzing security test results can help provide an initial measure of risk
  -- test results impact product management and remediation prioritization
  -- security checks must be performed at various checkpoints during the build process


  -- Implications of Security Test Results
    * security test results provide an insight into the overall security posture of the sw
    * additionally, test results can produce evidence in support of compliance with policies, regulations, and standards

    * impact on product management
      - stakeholders with significant interest in and reliance on security test results include the product management team
      - this is true considering the broad pm responsibilities and the fact that security test activities are primarily performed to validate a system conformance to security requirements

    * remediation prioritization
      - when security testing results reveal security flaws and vulns in sw, the pm, in coordination with other stakeholders (business), needs to prioritize the remediation efforts
      - various factors could impact the prioritization of remedies/fixes with a few of the following:

        -- risk associated with likelihood and resulting impact from exploitation
        -- severity and criticality of a vulnerability
        -- affected users
        -- efforts involved in remediation

    * break build criteria
      - aside from the obvious reason for a break in the build process due to the inability to compile the code, the sw build process may be broken due to several other reasons
      - sw builds may also be broken when security checks are performed at various checkpoints during the build process
      - this necessitates the clear definition and establishment of pass/fail conditions and criteria

      - good example would be the breaking of the build in the CI/CD pipeline to treat security issues with the same level of importance as quality and business requirements, and requiring the issues to be addressed through appropriate activities (trigger code review)


  -- Types of Results
    * efficient and comprehensive testing relies on the understanding of the test data outputs
    * the following are some considerations when analyzing test results.

    * type I and type II errors
      - sw test plans (STP) should include criteria or thresholds that determine if the test is successful before it is executed
      - consider the example of executing a nonfunctional static analysis tool against a recently developed API
        -- first, an organization should provide policy guidance stating what should be scanned for and what conditions are unsatisfactory, perhaps something like the following:
        -- “all source code shall be subject to static analysis and determined reasonable free of MITRE CWE Top 25 before commits can be made into the main source branch”

        -- second, static analysis would need to be implemented into gated check-in and preconfigured to check for a known set of defects such as MITRE CWE Top 25

        -- finally, an attempt to commit the new API to the main branch in the repository would automatically execute the static analysis
        -- pending the results, the code would be committed or rejected

      - however, sometimes processes and testing are more complicated
      - how do we know there were no defects from the MITRE CWE Top 25
      - suppose "CWE-787: Out-of-bounds Write" is not included in the checks for the static analysis tool or worse yet, the tool fails to identify CWE-787 as indicated
      - in both cases, it would be possible to have false negatives, also known as type II errors in statistics
      - type II errors may be detected by validating the coverage of the present static analysis tool with alternate tools

      - likewise, it is possible that our static analysis tool produces many findings for "CWE-20: Improper Input Validation" that are all incorrect
      - when testing produces an issue that is a non-issue, the result is said to be a false positive, which is also referred to as a type I error
      - manual review of test results can determine the validity of findings
      - type II errors are typically considered worse
      - security practitioners cannot ameliorate (parantaa) unknown issues
      - another way to classify test results post review would be predicted vs actual


  -- Accuracy vs Precision
    * in addition to the potential for error within test results, there are also varying levels of accuracy and precision
    * accuracy and precision are unfortunately used interchangeably in communications, but they are distinctly different and have implications on test results

    * accuracy describes how close a test measurement is to correct value
    * precision describes how close a sample of test results are relative to each other

    * from earlier, consider static analysis results that always incorrectly report CWE-20 defects to be precise but not accurate
    * if three true positive defects were all reported as CWE-20 when they were really three different CWEs, that would be considered accurate but not precise


- Security Errors Classification
  -- security bugs and errors identified during testing should be classified and tracked
  -- severity of an identified vulnerability (from a technical perspective) is an important factor in determining the risk to the business, but not the only factor

  -- sw security defects need to be tracked and addressed
  -- security defects have a life cycle
  -- reporting vulns’ risk ratings in the findings and reports make it possible to prioritize vulns for remediation

  -- Common Vulnerability Scoring System (CVSS) is a published standard used by organizations worldwide to capture the principal characteristics of a vulnerability


  -- bug tracking
    * upon the identification and verification of a bug, defect, or issue, the problem needs to be tracked so it can be addressed accordingly
    * it is advisable to track all bugs or defects related to the sw in a centralized repository or defect tracking system
    * centralization makes it possible to have a comprehensive view of the sw functionality and security risk
    * it also makes it possible to ensure that no two individuals are working on the same bug or defect

    * bug or defect tracking system, commercial or open source, should have the ability to support the following requirements:

      - defect documentation
        -- all required fields from a defect report must be recorded
        -- in situations where additional information needs to be recorded, the defect tracking system must allow for the definition of custom fields

      - integration with authentication infrastructure
        -- defect tracking system that has the ability to fill the authenticated user information automatically via integration is preferred to prevent user entry errors
        -- it also makes it possible to track user activity as they work on a defect

      - customizable workflow
        -- sw defect continues to be a defect until it has been fixed or addressed
        -- each defect goes through a life cycle
        -- as the sw defect moves from one status to another, workflow information pertinent to that defect must be tracked and, if needed, customized

      - notification
        -- when sw defect state moves from one status to another, it is necessary to notify the appropriate personnel of the change so processes in SDLC are not delayed
        -- most defect tracking systems provide a notification interface that is configurable to alert the appropriate people upon each status change

      - auditing capability
        -- for accountability reasons, all user actions in tracking system must be audited, and it must allow for storing and reporting on the auditable information in a secure manner



  -- defects
    * “any unintended characteristic that impairs the utility or worth of an item, or any kind of shortcoming, imperfection, or deficiency”
    * actual definition of sw defect can vary from one source to another
    * irrespective of the definition, it is important to understand that sw defects, quality or security, must be tracked through their life cycle

    * from the root cause perspective, a security defect can be due to mistakes in design or coding

    * defect life cycle
      - begins with its detection during testing and ends when the defect is addressed and closed
      - nearly all bug/defect tracking systems use various states to track the bug/defect through its life cycle
      - these states include “New,” “Assigned,” “Open,” “Pending Retest,” and “Fixed” to signify the initial detection and logging of the defect, its assignment to developers, commencement of the analysis and fixing of the bug, readiness for retest, and passing the test respectively


  -- risk scoring
    * successful risk-based program can be based on effective scoring system that should ensure the system is understandable, consider all relevant risk factors, and avoid subjectivity

    * Common Vulnerability Scoring System (CVSS)
      - if you have looked at a vuln report generated by common vuln scanner recently, you have likely noticed that vulns presented in the report have a CVSS score
      - is a published standard used by organizations worldwide to capture the principal characteristics of a vuln and produce a numerical score reflecting its severity
      - numerical score can be translated into qualitative representation (low, medium, high, critical) to help organizations properly assess and prioritize their vuln mgmt processes

      - when vulns are scored per CVSS, three dimensions are taken into consideration for calculating a score from 0 to 10 (least severe to most severe)
      - these dimensions are:
        -- base metrics for qualities intrinsic (luontainen) to a vuln
        -- temporal metrics for characteristics that evolve over the lifetime of the vuln
        -- environmental metrics for vulns that depend on a particular implementation or environment

      - the National Vulnerability Database (NVD) provides CVSS scores for almost all known vulns


- Data Security Testing
  -- securing test data is no different from securing any other data in terms of reasons for doing it (confidentiality and integrity) and how (safeguards and countermeasures)
  -- test data must be precise and complete
  -- in most cases, using production data in a test environment is not acceptable, especially when dealing with privacy data
  -- safeguards must be put in place to protect nonconfidential information in every environment as deemed necessary

  -- Generate Test Data
    * test data is data that has been specifically identified for use in tests
    * test data must be managed carefully to ensure confidentiality and privacy

    * test data is often overlooked by those using it as well as those who are charged to protect it
    * often test databases are compiled from production data
    * this production database may contain specific, if not sensitive, data such as employee information, payroll data, sales forecasts, and customer information

    * large volumes of data
      - when large volumes of test data are needed to test an application (for performance testing, load testing, stress testing), manual insertion of data in db is not practical
      - test data generation tools may be used for this purpose

    * production data
      - if sensitive data is extracted from production databases and moved or used as test data, the security of the sensitive data may be compromised
      - this data must be sanitized by whatever means possible before it can be used
      - once data has been moved out of production, security controls are often not as restrictive
      - often it is moved to a less restrictive site, and access is given to a wide variety of developers and QA personnel
      - to avoid this situation, test data should be generated from generic data or sanitized to eliminate security issues regarding sensitive data

      - in cases where production data needs to be migrated, only nonconfidential information must be migrated or the data must be obfuscated or masked
      - referential integrity constraints may pose a challenge

    * referential integrity
      - relational database management systems (RDBMSs) must provide methods for preserving the consistency and integrity of data in db
      - nearly all RDBMS products on the market today, whether commercial or open source, have a way of enforcing the referential integrity rules

      - referential integrity relies on the concept of primary keys (unique identifiers) and foreign keys when relationships are set up in db
      - to maintain integrity, operations performed on one table may require operations to be performed on other tables as well

      - example: if a record is removed from one table, corresponding records may also have to be removed (through cascade delete operation) from related table to
        -- maintain the integrity of the db
        -- prevent records from becoming orphaned

------------------------

        Checking Code Maintainability
        Install Radon using PyPI. From inside Visual Studio Code, using the terminal, execute pip install radon
        Download NuclearFusion.py. The file is located at https://github.com/avenatti/ISC2-CSSLP/blob/main/Metrics/NuclearFusion.py .
        Run Radon helper function. From inside Visual Studio Code, using the terminal, execute radon -h to display options for gathering metrics.
        Gather software complexity metrics for NuclearFusion.py. From inside Visual Studio Code, using the terminal, navigate to the folder containing NuclearFusion.py and execute radon cc nuclearfusion.py
        Gather raw software metrics for NuclearFusion.py. From inside Visual Studio Code, using the terminal, navigate to the folder containing NuclearFusion.py and execute radon raw nuclearfusion.py.

        What is cyclomatic complexity?
        $ radon cc NuclearFusion.py
        NuclearFusion.py
          F 185:0 analyze - C
          F 96:0 _get_all_tokens - B
          F 62:0 _fewer_tokens - A
          F 72:0 _find - A
          F 82:0 _split_tokens - A
          F 176:0 is_single_token - A
          F 122:0 _logical - A
          F 54:0 _generate - A


        Is the metric per function, file, or reactor based on outputs?
        The Radon tool is able to break down the CC metric per function in this case.

        What do the letters mean? Use radon -h as needed.
        First letter indicates function, method or class. The last letter indicates ranking (A-F).

        What was the SLOC for NuclearFusion.py? What other metrics were gathered?
        SLOC is the number of source lines of code.
        Other metrics is also included:
         LOC: 245
         LLOC: 102
         SLOC: 111
         Comments: 33
         Single comments: 31
         Multi: 57
         Blank: 46
         - Comment Stats
             (C % L): 13%
             (C % S): 30%
             (C + M % L): 37%

        LOC: the total number of lines of code
        LLOC: the number of logical lines of code
        SLOC: the number of source lines of code - not necessarily corresponding to the LLOC [Wikipedia]
        comments: the number of Python comment lines (i.e. only single-line comments #)
        multi: the number of lines representing multi-line strings
        blank: the number of blank lines (or whitespace-only ones)

-----------------------

- Production Data Reuse
  -- prior to the use of production data for testing, security and privacy risks must be weighed against the potential benefits of reuse
  -- generally, the use of production data for testing requires altering or removing sensitive information
  -- obfuscation (i.e., masking), anonymization, and tokenization are different techniques for protecting data without encryption

    * obfuscation
      - data masking and data obfuscation are the processes of hiding, replacing, or omitting sensitive information from a specific data set
      - data masking is usually used to protect specific data sets such as PII or commercially sensitive data or to comply with certain regulations such as HIPAA or PCI-DSS
      - data masking and obfuscation are also widely used for test platforms (where suitable test data is not available)
      - both techniques are typically applied when migrating test or development environments to the cloud or when protecting production environments from threats such as data exposure

    * anonymization
      - process of removing identifiers to prevent identification of individuals or sensitive information
      - process of anonymization is similar to masking and includes identifying the relevant information to anonymize and choosing a relevant method for obscuring the data
      - since true anonymization is difficult to achieve, some organizations may consider the use of pseudonymization techniques, which involve replacing actual data with pseudonyms
      - GDPR has defined pseudonymization as the processing of personal data in such a way that the data can no longer be attributed to a specific data subject without the use of additional information
      - techniques to pseudonymize data have been proposed, including scrambling, encryption, tokenization, and data blurring
      - it is important to understand that pseudonymization does not equate to anonymization

    * tokenization
      - like anonymization, aims to prevent identification of individuals or leakage of sensitive information
      - unlike anonymization, tokenization involves swapping sensitive data with a generated identifier (i.e., token)
      - further, tokens cannot be reverse engineered to reveal the original data because the token was not derived from the data it represents
      - the system of origin must be used to perform de-tokenization
      - tokenization is used frequently in the payment card industry

    * sanitization
      - marks the end of the test data life cycle
      - more specifically, sanitization is a process to render access to data on the media infeasible for a given level of effort
      - clear, purge, destroy
        -- actions that can be taken to sanitize media
      - NIST SP 800-88 Guidelines for Media Sanitization provides techniques which can easily be adapted to support sw testing data


    * Consider the following progression for data privacy:
      - Personal Data:     Bernhard Avenretti (804-222-1111) had lipoplasty on 7/19/2022 and 1/19/2023. His provider was Dr. Fatima Ferreira.
      - Pseudonymization:  Patient X1 had lipoplasty on 7/19/2022 and 1/19/2023. The provider was Dr. Fatima Ferreira.
      - Anonymization:     A patient had repeat medical procedures on 7/19/2022 and 1/19/2023.

      - after the patient information is pseudonymized, it may still be possible to figure out the person of origin is Bernhard
      - after anonymization, it is not possible to logically deduce origin


- Verification and Validation Testing
  -- sw verification and validation are related, but each serves a distinct purpose
  -- independent verification and validation (IV&V) conducted by unbiased third parties is a valuable and common practice during the system engineering life cycle

  -- sw verification and validation (V&V) evaluates how well the sw is meeting its technical requirements and its safety, security, and reliability objectives relative to the system
  -- although sw verification and validation are related, they are not the same

  -- verification remains focused on the assurance that the product is built right
  -- validation remains focused on the assurance that the right product is built

  -- the earlier in the life cycle the related software V&V activities can be conducted, the better
  -- verification and validation conducted by independent, unbiased, third-party organizations are valuable, and in some cases may be considered a requirement for compliance purposes

  -- Verification and Validation Testing

    * verification
      - involve verifying and analyzing various artifacts, including those created during the design stage or those created during the implementation and coding stages
      - does not involve executing code
      - instead, it helps achieve a level of confidence that the sw built is of high quality through document checks, walk-throughs, and other related activities
      - in doing so, consistency, completeness, and correctness of sw at each phase of the life cycle can be verified
      - decreased number of defects, better understanding of the product, and enhanced quality/stability of sw can be enumerated as the benefits of verification
      - ensures the correct sw was constructed

    * validation
      - is completed after verification
      - involves executing the code to validate and test the actual product
      - uses various testing methods, such as black box testing, as discussed earlier
      - after verification is completed, at the end of the development process, that validation is undertaken
      - ensures the sw was constructed correctly
      - objective is to ensure that the final sw produces functions correctly and meets the user requirements/needs
      - benefits of software validation include:
        -- possibility of catching defects that may have been missed during the verification process
        -- possibility of detecting misunderstood specifications when the actual results (during execution) are observed and compared against the expected results


    * independent and internal verification and validation
      - verification and validation are formal, well-structured processes to evaluate sw within systems engineering
      - they may be performed by the organization or by an independent third party
      - independent testing may be used to address various aspects of sw quality assurance
      - this may include security testing, functionality testing, and performance testing by independent testers

      - lack of bias implies that test reports are more credible, objective, and neutral
      - independent third parties specialize in testing and utilize laboratories designed for this purpose; this can reduce the test time
      - third-party assessment of sw functionality and assurance is the process in which the sw is reviewed, verified, and validated by someone other than sw developer or the acquirer
      - this is mostly the case with sw that is purchased

      - this is also frequently referred to as independent third-party testing
      - third-party testing is very helpful in validating vendor claims and assists with the compliance oversight process as it transfers the liability inherent from the sw risks to the third party, which conducts reviews and tests should a breach occur once the sw has been accepted on grounds of the findings from the IV&V


    * acceptance test
      - may include user acceptance testing, operational acceptance testing, contract acceptance testing, or even compliance acceptance testing
      - user acceptance testing focuses mainly on functionality, thereby validating the fitness for use of the system by the business user
      - the users and application managers perform the user acceptance test

      - operational acceptance testing is commonly conducted by system administrators prior to the system release to validate fitness for operations
      - it could include testing of backups through restoration or other continuity exercises which determine the operational readiness of sw

    * alpha and beta testing
      - alpha testing may also be considered a form of acceptance testing that is conducted when development is completed
      - it is normally carried out by internal teams/staff that did not have a direct involvement in the development of the sw
      - beta testing, which is conducted after alpha testing, is completed by a sample of the intended audience (customers) prior to the release of the sw for public






Attack Surface - The set of points on the boundary of a system, a system element, or an environment where an attacker can try to enter, cause an effect on, or extract data from. Source: NIST 800-160 vol 2
Anonymization - The process of removing identifiers to prevent identification of individuals or sensitive information.
Black Box Testing - A test methodology that assumes no knowledge of the internal structure and implementation detail of the assessment object. Source: csrc.nist.gov/glossary
May also be referred to as zero-knowledge testing and is best suited for uncovering certain types of vulnerabilities in software. This method of testing can be applied to virtually every level of software testing: unit, integration, system and acceptance.
Functional Testing - Type of software testing focused on validating the behavior of the software, and the assurance that the software meets its functional specifications and requirements. The advertised security mechanisms of an information system are tested against a specification. Source: CNSSI 4009-2015
Fuzz Testing (software) - Type of testing that is best suited for uncovering certain types of memory-related issues/vulnerabilities that may be present in software (e.g., memory leaks, buffer overflows) by subjecting the entry points of the software to invalid or random input types, ranges, and input lengths. It is normally conducted using automated testing tools and techniques.
Gray Box Testing - A test methodology that assumes some knowledge of the internal structure and implementation detail of the assessment object. Source: NIST SP 800-53A rev 5
Interoperability Testing - Measuring the performance associated with the use of standardized biometric data records in a multiple vendor environment. This form of testing involves the production of the templates by N enrollment products and authentication of these against images processed by M others. Source: NIST SP 800-85B
Nonfunctional Testing - Testing that verifies aspects of the product, such as cyber resiliency, performance, safety, security, scalability, recoverability, and usability against organization-defined criteria.
Obfuscation - The processes of hiding, replacing, or omitting sensitive information from a specific data set.
Penetration Testing (software) - Type of security testing that is conducted from the perspective of the attackers and targets the application while it is running in its production or production-like environment. It is used in vulnerability analysis for vulnerability assessment, trying to reveal vulnerabilities of the system based on the information about the system gathered during the relevant evaluation activities. Source: ISO/IEC 19989-3
Pseudorandom Number Generator - A deterministic computational process that has one or more inputs called "seeds" and which outputs a sequence of values that appears to be random according to specified statistical tests. Source: csrc.nist.gov/glossary
Real user monitoring (RUM) - A monitoring tool/technology that aims to provide deeper insight into the end-user experience, performance, and general health of applications.
Regression testing - Reference to the type of software testing that remains concerned with the verification that new changes introduced to the software do not adversely affect the existing/old functionality.
Sanitization - A process at the end of the data life cycle that renders access to data infeasible for a given level of effort. Actions to achieve this can include Clear, Purge, and Destroy.
Scalability - Testing the ability of a system to handle an increasing amount of work correctly.
Software Assurance - A process and specialization within cybersecurity that seeks to establish the level of confidence that a software is free from vulnerabilities, either intentionally designed into the software or accidentally inserted at any time during its life cycle, and that the software functions as intended by the purchaser or user.
Stress testing - A type of software testing that remains focused on verifying the stability and reliability of the software under stress conditions (beyond the normal limits of operation). Stress may be expressed in terms of excessive loads/volumes or even unexpected anomalies at run time (e.g., loss of connection to the network or to the database).
Synthetic monitoring - A monitoring tool/technology that relies on scripting capabilities and consistent sets of transactions to assess performance and availability and aims to minimize the time to detect and mitigate production issues.
Testing - A type of assessment method that is characterized by the process of exercising one or more assessment objects under specified conditions to compare actual with expected behavior, the results of which are used to support the determination of security control or privacy control effectiveness over time.
 Test Case - A reference to a procedure that is created and executed by software developers/testers to determine whether the software under the test meets the specified requirement.
Testing strategy - A reference to a high-level document that describes an organization’s overall approach to testing the software.
Tokenization - A system to prevent identification of individuals or leakage of sensitive information by swapping sensitive data, with a generated identifier. This system is used frequently in the payment card industry.
Type I Error - Totality of protection mechanisms within a computer system, including hardware, firmware, and software, the combination responsible for enforcing a security policy. Source: NIST SP 800-12 Rev.1
Type II Error - Incorrectly classifying malicious activity or software defects as benign. Also referred to as False Negatives.
Unit Testing - A type of software testing that focuses on testing units of code in isolation and is conducted by software developers during the implementation (coding) phase of the life cycle. Dependencies of the code being tested in isolation are addressed through creation of drivers and stubs.
Vulnerability Scanner (software) - A software tool (commercial or open source) that is specially designed for uncovering weaknesses and vulnerabilities in software.
White Box Testing - A test methodology that assumes explicit and substantial knowledge of the internal structure and implementation detail of the assessment object. Reference:  csrc.nist.gov/glossary. Also known as clear box testing, glass box testing, transparent box testing, and structural testing, it tests internal structures or workings of an application, as opposed to its functionality (i.e. black box testing). Source: NIST SP 800-192. May also be referred to as “full-knowledge testing” and is best suited for uncovering certain types of vulnerabilities in software.



QUIZ


Question 1    1 / 1 point
Which term BEST relates to a security testing method involving the direct analysis of source code where the tester has complete knowledge of the inner workings of the software under test? (D6.1, L6.1)

A) Unit testing
B) Black box testing
C) Stress testing
--> D) White box testing

Correct. The description above matches white box testing.


Question 2    0 / 1 point
Which aspect of software testing involves checking the context of the application in relation to other applications, both upstream and downstream in the environment? (D6.1, L6.2)

A) Configuration mismatch check
B) Test harness validation
--> C) Interoperability testing
D) Dependency resolution testing


Dependency resolution testing focuses on identifying and resolving dependencies or conflicts between different components or modules within the software being tested, ensuring that all required dependencies are properly managed.


Question 3    0 / 1 point
Which of the following provides guidelines for conducting consistent and deterministic security testing? (D6.1, L6.3)

A) ISO 27001
B) NIST SP 800-53
C) CVSS
--> D) OSSTMM


ISO 27001 is used as a blueprint for establishing an Information Security Management System (ISMS), NIST SP 800-53 describes security controls for RMF, and CVSS is a standard for capturing the principal characteristics of vulnerabilities.


Question 4    1 / 1 point
What is the main objective of bug bounty programs in crowdsourced security? (D6.1, L6.4)

A) To outsource software development tasks to a diverse group of talent
--> B) To identify vulnerabilities or exploits in software and provide acknowledgement or compensation
C) To involve the public in producing creative work for organizations
D) To rely on the collective power of the development community to achieve innovation

Correct. Bug bounty programs in crowdsourced security aim to encourage individuals, often penetration testers or bug hunters, to identify vulnerabilities or exploits in software.


Question 5    0 / 1 point
Which of the following is the correct sequence of execution for penetration testing? (D6.2, L6.6)

A) Reconnaissance, Scanning, Attack, and Reporting
B) Planning, Discovery, Penetration, and Reporting
--> C) Planning, Discovery, Attack, and Reporting
D) Planning, Discovery, Vulnerability Mapping, and Reporting

For a refresher, review NIST SP 800-115 Section 5.2.


Question 6    1 / 1 point
Which of the following is NOT a correct statement regarding fuzz testing? (D6.2, L6.6)

A) Fuzz testing is easy to repeat once established.
--> B) Fuzz testing is only used to uncover injection vulnerabilities.
C) Fuzz testing can be helpful for uncovering memory-related issues, including buffer overflows.
D) Fuzz testing can generate a tremendous amount of output data.

Correct. Fuzzing can detect injection, XSS, and memory related issues.


Question 7    1 / 1 point
Which of the following security-focused verification tasks can identify overly complex, erroneous, or ambiguous information that leads users, administrators, operators, or maintainers to inadvertently place the system into a nonsecure state? (D6.2 L6.7)

--> A) Abuse and Misuse Cases
B) Penetration Tests
C) Correctness Analysis
D) Vulnerability Remediation

Correct. Security abuse and misuse procedures address the way the system can be utilized to produce unspecified behavior and outcomes. These procedures may target the security guidance, policies, procedures, and any other available information directed at users, operators, maintainers, administrators, and trainers.


Question 8    1 / 1 point
Bathsheba is reviewing a coworker's assessment of risk management controls. She is concerned that only Control Correlation Identifiers (CCIs) marked as technical controls have been verified as part of the internal audit. What advice would you provide? (D6.3, L6.8)

A) Security Content Automation Protocol (SCAP) only supports Security Technical Implementation Guide (STIG) technical controls. You cannot STIG a policy. The audit is fine.
B) Security Content Automation Protocol (SCAP) supports administrative and technical controls. The auditor needs to rerun the tool correctly and include policy results.
C) CCIs identified as policies are not technical controls and cannot be assessed. How would we even manage to audit a software development plan (SDP)? It would be good, if possible, but the technical audit of controls is sufficient.
--> D) CCIs identified as policies, although not technical, are administrative controls which must be audited to ensure compliance. They should be checked to ensure timely revisions. Some continuity documents such as Disaster Recovery Plan (DRP) can be tested using strategies such as a tabletop or simulation. The auditor should be able to find evidence of such tests and should include them in reporting.

Correct. Bathsheba is correct in her gap assessment of the control audit. SCAP can be used for supported technical checklists. Manual review is for non-supported technical controls and administrative controls.


Question 9    1 / 1 point
Malik works for a company that is producing a software service that will be leveraged by critical national infrastructure. What OWASP Application Security Verification Standard (ASVS) level will he likely want to achieve? (D6.3, L6.9)

A) Level 1
B) Level 2
--> C) Level 3
D) Level 4

Correct. ASVS Level 3 is the highest level of verification within the ASVS. This level is typically reserved for applications that require significant levels of security verification, such as those that may be found within areas of military, health and safety, critical infrastructure, etc.


Question 10   1 / 1 point
What does "undocumented functionality" in the context of an application generally mean? (D6.4, L6.10)

A) The functionality existed at some point, but it was commented out in code and is no longer executed.
B) The code in the application that provides the specified functionality is unreachable during normal execution of the software.
C) The functionality should be treated as embedded malicious code.
--> D) The functionality exists but it is not officially documented and/or recognized.

Correct. Undocumented functionality in applications is functionality that exists but is not officially documented and/or recognized.


Question 11   1 / 1 point
A real software weakness, CWE-1242: Inclusion of Undocumented Features or Chicken Bits most closely aligns to which of the following terms based on the title? (D6.4, L6.10)

--> A) Backdoor
B) Botnet
C) Remote Access Trojan (RAT)
D) Distributed Denial of Service (DDoS)

Correct. A backdoor is typically a covert method of bypassing normal authentication which can be supported by the inclusion of undocumented features.


Question 12   1 / 1 point
Ming receives the results from running Pysa, a Meta produced open-source static analysis tool for Python, and is surprised that the test results include more than a thousand reported issues for ten thousand lines of code. What advice might you provide Ming? (D6.5, L6.11)

A) The tool is probably not acceptable for testing your application. Find another tool.
B) The application has many issues. Get busy and fix them starting with most severe.
--> C) Test results can produce false positives when tuned to reduce false negatives. You will likely be able to confirm that some test results do not require fixes.
D) Test results can produce false negatives when tuned to reduce false positives. You can refrain from fixing all false negatives logged by the test.

Correct. The tool likely produced many false positives that will be trivial to review and dismiss, making the review of a thousand results much more achievable.


Question 13   1 / 1 point
Conor, a very technically focused security engineer, was instructed by his manager to prepare the results of several vulnerability assessments for the company board of directors. Which of the following options would be MOST effective? (D6.5, L6.11)

A) Prepare a presentation showing the quantity of each of the CVEs and the estimate to fix.
B) Prepare a document that aggregates and sorts all the findings, so the board can see all the details.
--> C) Prepare a presentation showing qualitative categories of issues, potential risk impact if unfixed, and cost to fix.
D) Prepare a document that includes tools to find future vulnerabilities and security controls to mitigate. Include tool features and licensing costs.

Correct. A quick five-slide presentation with backup slides highlighting required funding and risks is likely to resonate best.


Question 14   1 / 1 point
What is the best way to describe the Common Vulnerability Scoring System (CVSS)? (D6.6, L6.14)

A) Vulnerability detection standard used by vulnerability scanners.
--> B) Standard used to produce a numerical score reflecting the severity of a vulnerability.
C) Standard used to measure the attack surface of the software in relation to a vulnerability.
D) Standard used to describe TOCTOU vulnerability.

Correct. CVSS is a published standard used by organizations worldwide to capture the principal characteristics of a vulnerability and produce a numerical score reflecting its severity.


Question 15   1 / 1 point
The life cycle of a bug begins with discovery and ends when the underlying defect is ameliorated. Nearly all bug tracking systems use various stages to track the progression of system fixes through the application life cycle. Which of the following is NOT a typical stage for bug tracking? (D6.6, L6.13)

A) Open
B) Closed
--> C) Cloned
D) Duplicate

Correct. Open, closed, and duplicate are all reasonable labels for bugs within the tracking life cycle.


Question 16   1 / 1 point
Referential integrity rules and constraints serve which primary purpose in relational database management systems? (D6.7, L6.15)

--> A) Ensure that data changes in one place are reflected in other related records.
B) Ensure that data in the database is encrypted.
C) Ensure that passwords are always hashed before they are stored in the database.
D) Allows primary key reference to be null temporarily to support security.

Correct. Relational database management systems (RDBMSs) must provide methods for preserving the consistency and integrity of data in the database.


Question 17   1 / 1 point
HealthiestClinic is building a new patient application, PatientiestPower, that will provide access to user profile, treatment history, and payments from a convenient mobile interface. Are there any concerns with using other production system data for testing PatientiestPower? (D6.7, L6.15)

A) Production data will provide the most accurate test results. There is no issue.
B) Production data will provide accurate test data, but names will need to be anonymized or obfuscated.
--> C) Production data may carry risks with fair use and requirements for PHI, PII, PCI-DSS, etc.
D) Production data may carry risks with fair use and requirements for PHI and FERPA.

Correct. Production data that includes user profile (PII), treatment history (PHI), and payments (PCI-DSS) carries risks.


Question 18   0 / 1 point
Which of the following is correct regarding software validation? (D6.8, L6.16)

A) Validation follows verification.
  Validation should answer the question, "Are we building the software correctly?"
B) Verification follows validation.
  Validation should answer the question, "Are we building the software correctly?"
--> C) Validation follows verification.
  Validation should answer the question, "Are we building the correct software?"
D) Verification follows validation.
  Validation should answer the question, "Are we building the correct software?"



ve-va   (VErification - VAlidation)
pr-rp   (Product is built Right - Right Product is built)
cs-sc   (Correct Sw was constructed - Sw was constructed Correctly)
st-dy   (STatic - DYnamic)


For a deeper understanding of Verification and Validation from a secure system engineering perspective, review NIST SP 800-161 volume 1 Appendix H.9 & H.11.

Question 19   1 / 1 point
Who does NOT generally conduct acceptance testing? (D6.8, L6.16)

--> A) Software developers
B) Specialized testers
C) Customers
D) None of these

Correct. Apart from the Alpha testing, acceptance test types are performed by different stakeholders— specialized testers, customers, the customer's customers…


Question 20   1 / 1 point
Which of the following BEST represents a process that determines to what degree an application meets end users' approval? (D6.8, L6.16)

--> A) Acceptance Test
B) Performance Test
C) User Integration Test
D) Exploratory Test

Correct. Acceptance testing is a formal test that verifies systems against business and user requirements.





DOMAIN 7: Secure Software Deployment, Operations, and Maintenance
-----------------------------------------------------------------

- Operational Risk Analysis
  -- after sw and systems have been built, they must be managed and maintained within an organization’s production environment
  -- security posture changes over time as components and sw are replaced and the threat landscape changes
  -- operational risk analysis should be conducted on an organizationally defined frequency to ensure the organization has an up-to-date understanding of risks

  -- Operational Risks
    * synonymous with production risks for sw systems
    * security practitioners have differing security mechanisms for sw development than sw deployment and sustainment (ylläpitäminen)
    * when developing secure sw, practitioners may focus on safeguards including
      - administrative controls (e.g., Software Development Plan, Software Test Plan)
      - technical controls (e.g., gated check-in with static analysis)

    * however, production sw has a different security emphasis
    * safeguards must be implemented in depth to include
      - deployment environment
      - training
      - compliance
      - business dictated interoperability

    * each of these product protections needs to be monitored and assessed throughout the sw useful life

    * risk analysis frameworks
      - NIST SP 800-30 provides high level guidance for conducting information system assessments
      - ISO 31000 series also describes risk management, especially ISO 31010, which focuses on specific risk assessment techniques
      - generally, the steps involved in production risk analysis and amelioration (parantaminen) include:
        -- risk identification. What is the risk? Why would it occur?
        -- risk impact. If the risk is realized, what is the organizational impact?
        -- risk likelihood. How likely is the risk to be realized?
        -- risk treatment. What safeguards should be used to address the risk?

    * deployment environment
      - staging and conducting sw assurance (the level of confidence that sw is free from vulns either intentionally designed in or accidentally inserted at any time during its life cycle, and that sw functions as intended) are important to ensure that sw will function as desired before deploying to production

      - ISO 31000 describes risk analysis as one part of the risk assessment process
      - risk analysis should aid an organization in determining the nature of a risk
      - staging sw for quality and functional tests and conducting sw assurance activities allow organizations to evaluate the level of risk before moving sw into production

    * personnel training
      - providing appropriate training to personnel is an opportunity for risk treatment by increasing awareness within the organization
      - risk treatments such as awareness training should be made in accordance with objectives, risk criteria, constraints, and resources
      - risk treatments are not mutually exclusive and can be combined with other treatments such as user access controls, screen timeout, and IT user awareness training
      - options for treating risks within ISO 31000 include
        -- avoiding risk
        -- removing the source of risk
        -- lowering likelihood
        -- lowering impact
        -- sharing the risk through contracts or insurance
        -- accepting the risk formally

    * system integration
      - when dealing with complex sw solutions or systems of systems, integration of component sw or systems is unavoidable
      - system integration describes the process of bringing together sub-systems (sw or other components) into a unified system
      - there are many business reasons that dictate requirements for a unified system
      - for similar reasoning, security practitioners also seek to unify an approach to risk management where processes for analyzing and treating risk are integrated into other organizational processes

      - ISO 31004 defines guidelines for implementing risk management and highlights the importance of integrating risk management with business processes
      - when integrating sw systems, care should be given to develop a process that ensures regressions do not occur
      - likewise, changes to business processes to support risk management should be monitored to ensure integration is successful

    * legal compliance
      - sw application context matters
      - application under test with synthetic data has different compliance considerations once moved into production environment
      - geography of hosting environments, sensitivity of the data (e.g., PII, PHI, financials) being processed, nationality of users, and business sector and corresponding regulatory bodies matter

      - ISO 31022 provides guidance to enable organizations to manage the complexities of legal compliance
      - ISO 31022 describes operations and activities for managing contractual risk, strategic decision-making for legal issues, and framework for handling legal environments
      - organizations can implement legal risk management processes by tailoring the framework to mission needs


- Secure Configuration and Version Control
  -- controlled configurations and versioning assist in the secure deployment, execution, and maintenance of sw systems using a structured change management process
  -- this process is sometimes generally referred to as Security Configuration Management (SecCM) or the management and control of configurations for an information system to enable security and facilitate risk management
  -- sw system is typically composed of multiple components, which are updated and reconfigured at differing rates in response to business needs for enhancements or sustainment

  -- Security Configuration Management (SecCM)
    * NIST 800-128 (Guide for Security-Focused Configuration Management of Information Systems) describes the need for configuration management to protect sw and systems
    * the need for changes may arise based on hw updates, defect amelioration, new security hardening needs, or changing business functions
    * sw updates that require configuration changes on the host system or within the sw configuration are common
    * the combination of system changes at multiple levels (OS config, app, app config) increases complexity and the likelihood of unintentionally introducing production issues
    * to safeguard against such an occurrence, config management should be a well-defined, controlled process

    * configuration management (CM)
      - encompasses establishing and maintaining the integrity of sw products and systems by controlling processes for adding, modifying, and monitoring configs
      - each item under change control is essentially a configuration item
      - config item (CI) such as sw, fw, hw, or documentation is a discrete unit of the organization’s configuration control processes
      - CM plan describes roles, policies, and procedures required to effectively execute CM for CI updates

      - CM plans typically include:
        -- Change Control Board (CCB)
          * composed of parties responsible for controlling and approving changes throughout the life cycle of sw systems
        -- Configuration Item Identification
          * covers selection and the naming method for CI
        -- Configuration Change Control
          * process for managing baseline configuration changes
        -- Configuration Monitoring
          * process for assessing baseline compliance that includes reporting

    * baseline configuration
      - set of reviewed and accepted configuration items for a system that becomes the basis for builds, releases, and production conformance
      - baselines are formally reviewed and accepted before being adopted
      - any new changes to the baseline must undergo change control procedures
      - baselines and corresponding CIs can change based on time and the system-development life cycle (SDLC) phase

      - early in the SDLC system, baselines may be functional requirements
      - as the system matures, the baseline will likely include sw design artifacts and component system configurations
      - baselines may be organizationally established for various computing environments that are distinguished by
        -- classification level (Secret vs. Unclassified) or
        -- life cycle phase (test vs. production)

      - when a new baseline for a computing environment or system is established, the old baseline is deprecated but archived
      - the CCB may reverse an approval decision if quality or security issues are discovered
      - if this occurs, a previous baseline would be used to rollback as needed
      - NIST SP 800-53 security control CM-2 Baseline Configuration describes the SecCM phase for identifying and implementing secure configurations

    * information systems and hw configuration
      - secure configurations are implemented to reduce operational system risk
      - configuration may enforce trusted sw installation, patch maintenance, or security-minded configuration values
      - secure configurations for sw systems commonly reference integrated supplier sw products
      - the concept of secure configuration is also relevant to hw
      - organization must track and label assets
      - wthin the context of a traditional on-premises server room, a rack of servers might have a particular configuration
      - the baseline configuration of the server rack might enforce equipment redundancy and support effective cable management and other organizationally defined requirements
      - NIST SP 800-53 security controls CM-6 Configuration Settings and CM-7 Least Functionality also support identifying and implementing secure configurations within SecCM

    * configuration change control
      - formally documented process for controlling additions, modifications, and removal of system configurations or associated CIs
      - configuration change control for sw and systems typically includes:
        1. Proposal
        2. Justification
        3. Implementation
        4. Test/Evaluation/Review
        5. Change Disposition (luovutus, sijoittaminen, järjestely)

      - should apply to all change scenarios, whether an emergency or unscheduled business need or routine updates
      - changes should be managed from proposal through all steps including change disposition
      - each step should be clearly defined and documented
      - NIST SP 800-53 Security Control CM-3 Configuration Change Control and CM-5 Access Restrictions for Change, both support the concept of controlling configuration changes within SecCM

    * configuration documentation
      - SecCM policies should establish expectations for the organization using specific, measurable, and testable objectives
      - in general, configuration management policies explicitly define configuration management organizational goals and requirements
      - SecCM policy documents describe organizational objectives informing the creation of the steps required to execute change management policy goals
      - SecCM procedures specifically focus on the methodology to propose, justify, implement, and test changes utilizing the concepts of CCB, baseline, and CI

      - CM plan is operational and describes how SecCM policy (strategy) should be implemented
      - SecCM plan might be incorporated into the CM Plan or be kept separate
      - SecCM plan may be enterprise-wide or apply to a specific group (sw development, DevSecOps, infrastructure) within the organization
      - bottom line, organizations should tailor configuration document implementation to integrate with and support business processes
      - SecCM plan and other documents are initiated during SecCM planning and support the implementation of both NIST SP 800-53's security controls
        -- CM-1 Configuration Management Policy and Procedures
        -- CM-9 Configuration Management Plan


- Secure Software Release
  -- proper deployment ensures the security of sw and its operating environment
  -- sw security and integrity may be adversely affected if appropriate controls are not incorporated into the integration and deployment processes

  -- Secure Software Deployment
    * one of the final stages in delivering secure sw is ensuring that neither the security nor integrity of developed applications is compromised during deployment
    * secure deployment (SD) practice focuses on this
    * to this end, the practice’s first stream focuses on removing manual error by automating the deployment process as much as possible and making its success contingent upon the outcomes of integrated security verification checks
    * it also fosters separation of duties by making adequately trained non-developers responsible for deployment

    * actions involved in deploying sw securely depend on the maturity of organizational processes
    * secure deployment may require the organization to:
      - formalize the deployment process and secure the used tooling and processes
      - automate the deployment process over all stages and introduce sensible security verification tests
      - automatically verify integrity of all deployed sw regardless of whether it's internally or externally developed

    * secure continuous integration and continuous delivery (CI/CD) pipeline
      - DevOps requires tasks such as builds, testing, and deployment to occur frequently and naturally
      - for that to happen, such tasks must be automated
      - CI/CD practice relies on the automation of much of the routine work of transforming code changes into working sw, including delivering tested code into production

      - CI is a reference to the build and test cycle
      - CI servers build the project from scratch when developers merge their changes into a shared version control repository
      - CD is a reference to the code movement from one environment to another (dev to QA to UAT) [UAT = User Acceptance Testing]
      - CI and CD typically go hand in hand, and the same CI server will likely also handle CD

      - traced back to build servers like Hudson, Jenkins, and Microsoft Team Foundation Server, CI/CD has become a collection of technologies and practices that support the mission of releasing new code changes while keeping things stable

     - in practice, CI/CD would rely on a system to trigger the process of compilation and run all tests and checks that have been automated

     - inconsistent methods, lack of automation and integration of security testing tools, and excessive FPs can contribute to the app security challenges in CI/CD workflows

     - distinction between continuous delivery and continuous deployment must be made
       -- whereas CI is part of both continuous delivery and continuous deployment (the step that involves deployment to production is manual in the case of continuous delivery), continuous deployment goes one step further than continuous delivery
       -- with this practice, every change that passes all stages of the production pipeline is released to customers with no human intervention, and only a failed test will keep a new change from being deployed to production



  -- Application Security Toolchain
    * implementing an entire security toolchain into an existing pipeline is challenging
    * hooks for pre-commit checks, commit-time checks, build-time checks, test-time checks, and deploy-time checks may all be needed

    * for example:
      - using IDE security plugins to run scans before code is checked into the shared repository
      - performing static code analysis with limited rulesets (top three vulnerabilities) to provide developers with quick feedback upon committing changes to the repository
      - running even more in-depth security checks (OWASP Top 10), and software composition analysis of open-source components at build time
      - performing dynamic analysis or fuzzing

    * note that references above to IDE Plugins, SAST and DAST tools, SCA tools, and fuzz tools were only used as examples among other tools in the toolchain



  -- Build Artifact Verification
    * builds with external dependencies may require safeguards to ensure the legitimacy of artifacts (binaries) that are used during the build
    * this is especially true when sw relies on artifacts that are published on third-party repositories

    * sw composition analysis (SCA) tools can also be useful in this regard
    * keep in mind that these tools can generate an inventory report of all open-source components in sw, including all direct and transitive dependencies

    * verification in terms of checksums, hashes, and signatures may be used to avoid integrating compromised dependencies

    * verified and trusted mobile apps
      - app vulns are caused by several factors, including design flaws and programming errors, which may have been inserted intentionally or inadvertently
      - unfortunately, in the app marketplace, apps containing vulns are prevalent (yleisiä) due in part to the submission of apps by developers who may trade security for functionality to reduce cost and time to market

      - platform vendors provide various mechanisms to enforce security, and there is much to discuss about the security models for each platform

      - iOS
        -- with security at the core of iOS platform design, there is a tight integration of hw, sw, and services that protect the entire ecosystem
        -- iOS only allows apps that it considers safe
        -- first, it must verify the source of the app and its integrity
        -- the only executable code that iOS will allow apps to run must be signed with an Apple-issued certificate
        -- this is true regardless of whether the app came with the device or is a third-party app
        -- this certificate validates the iOS app developers’ identity and tells iOS that they are known to and verified by Apple
        -- iOS developers are required to join the Apple Developer Program
        -- iOS apps are also reviewed to ensure adherence to certain requirements for operations and bugs
        -- as part of iOS security, every third-party app is sandboxed
        -- this is done for several reasons, including preventing misbehaving apps from compromising the system or other apps and accessing user data

      - Android
        -- Android was designed to be open, with an eye toward creating a developer-friendly platform
        -- securing an open and developer-oriented platform involves various layers of security and a strong security architecture
        -- for starters, although some developers are security experts, some are not
        -- approach was to cater to both types of devs, offering flexible controls to security-savvy and protecting not-so-savvy through secure defaults and built-in controls
        -- Android relies on mechanisms to protect against hacks and corruption through a chain of trust
          * for example, during the verified boot process, each stage ensures the integrity and authenticity of the next stage, before execution gets handed over

        -- like iOS, Android sandboxes its apps
        -- this keeps apps separate from each other and protects apps and the system from misbehaving and malicious apps
        -- Android also has a read-only system partition that contains the kernel, OS libs, app runtime, platform apps, and other features
        -- this partition protects the most crucial parts of the OS from malicious apps, and from users who might accidentally delete or change vital part of the OS

      - anti-tamper
        -- publication of malicious versions of an app by bad actors on third-party markets may be possible
        -- antitamper techniques, tamper-detection mechanisms, or other safeguards that can help thwart bad actors’ attempts to reverse engineer an app and introduce malicious functionality should be a consideration


- Storage and Management of Security Data
  -- credentials, secrets and keys, certificates and config data are among extremely valuable and sensitive sw assets that always require protection in all places
  -- these highly sensitive assets may be stored in key management solutions or sw-based vaults, or backed by tamper-resistant hardware

  -- Storage and Management of Security Data
    * security data that comes with app will be related to accessing or protecting it in some way, so care must be exercised to protect the security data from potential attackers
    * security data can take the form of credentials, secrets, keys/certificates, and configuration files
    * key vaults and secret managers offer reasonable protection for the secrets

    * credentials
      - bound in some way to subjects are used to provide a proof of identity during an authentication transaction and as part of access control
      - electronic credentials can come in the form of PINs, passwords, biometric data, and more
      - in general, credentials used for authentication that can be easily spoofed are a source of concern
      - the special case of anonymous credentials (not associated with a known personal identity) must be approached with caution

    * secrets
      - secure deployment of sw also needs to address protection of secrets (passwords and tokens) required for apps to operate in production environments
      - depending on the maturity of organizational processes, you may need to:
        -- introduce protection measures to limit access to production secrets
        -- inject secrets dynamically during the deployment process from hardened storages and audit all human access to them
        -- improve the life cycle of app secrets by regularly generating them and by ensuring proper use

      - in its simplest form, suitable production secrets are moved from repositories and configuration files into adequately managed digital vaults
      - in more advanced forms, secrets are dynamically generated at deployment time and routine processes detect and mitigate the presence of any unprotected secrets in the environment

    * secrets injection
      - when building apps using containers (Docker), the management of secrets (e.g., passwords, OAuth tokens, and SSH keys) must be addressed
      - one possibility is to store the secrets in the source code itself; another is to store the secrets in the container image (e.g., Docker file)
      - there are several issues with this approach, as access to the source code should not yield access to the secret or changes in code
      - for example, a password should not require rebuilding and redeploying the code
      - there are alternatives to the approach previously mentioned, if supported by the container management tool and technology
      - one alternative is to inject (pass) the secrets into the container using environment variables or files and make the secrets available to the containerized code at runtime
      - this, in turn, may incur other risks such as logging the entire environment, which could potentially expose the secrets to those with access to logs

    * keys and certificates
      - both keys and certificates need the same type of protection, since they mostly perform the same type of functions
      - public key cryptography (also known as asymmetric encryption algorithms) involves a pair of keys: one public, one private
      - anyone can know the public key, but the private key can be known only by authorized subjects
      - if the private key is compromised, the associated public key should be revoked

      - any lapses, flaws, or weaknesses in the key management process could be detrimental to the encryption process and security of the data encrypted with that key
      - adversaries may exploit vulns in sw (e.g., buffer overflow and memory dump) to obtain the encryption key

      - hw security modules (HSMs) can offer additional security for key protection, so it’s best to use hardware-assisted cryptography rather than encrypting with software alone
      - encryption also requires robust physical and network security for any device used to generate or store private keys

    * configurations
      - configuration files can be used to configure the parameters and initial settings for all kinds of sw, including operating systems, web server sw, and user apps
      - tools to create, modify, and verify the syntax of the configuration files may be provided with the applications, and may either be CLI or GUI driven
      - some computer programs only read their configuration files at startup, whereas others periodically check the configuration files for changes
      - potential risk with configuration files is that users can instruct some programs to reread the configuration files and apply the changes to the current process, or indeed to read arbitrary files as a configuration file

      - further, configuration files generally contain various types of sensitive information, so they must be protected
      - CWE-260: Password in Configuration File provides an example:
        -- “The software stores a password in a configuration file that might be accessible to actors who do not know the password. This can result in compromise of the system for which the password is used. An attacker could gain access to this file and learn the stored password or worse yet, change the password to one of their choosing”

    * key vault
      - deploying encryption and other security solutions across the enterprise requires managing secrets
      - vaults are services that securely manage secrets for apps
      - they store, retrieve, and optionally rotate secrets such as encryption keys, certificates and API keys, among others

      - as an example, consider the encryption key used by Transparent Data Encryption (TDE) to encrypt the data in the database
      - the encryption key used by TDE would be stored in the key vault for secure access, and the applications’ interaction with the key vault would be managed through APIs

      - key vaults may be implemented on premises or in the cloud infrastructure
      - in addition to security, fault tolerance and scalability are necessary for any such service
      - for stronger security and/or compliance, key vaults may use FIPS 140-2 validated HSM

      - examples of solutions used to store and manage secrets include Azure Key Vault, AWS Secret Manager, and HashiCorp Vault
      - there are many considerations when choosing solutions for this purpose. A few are listed here:
        -- are secrets safeguarded using industry standard algorithms?
        -- does the service provider have access to the keys?
        -- are key access and usage logged?
        -- can secrets be updated without disrupting applications?
        -- are keys stored in the same data center as the application?
        -- can customers manage their own keys?



- Secure Installation
  -- secure installation and secure bootstrapping are essential components of secure sw deployment activities
  -- the least-privilege principle is relevant to deployment and operations of sw
  -- least privilege can help with overall system stability
  -- hardening is necessary to protect the system and its critical components against accidental or intentional misuse and attacks
  -- various sw activation mechanisms may be used to verify compliance with sw license agreements
  -- sw security policy must address the security-relevant aspects of sw installation and deployment

  -- Secure Installation
    * proper installation processes are necessary for the security and functionality of sw, as well as the environment in which the sw will operate
    * secure installation can include the following activities:
      - appropriate security hardening across all parts of the application stack
      - disabling/removing unnecessary features
      - disabling/changing default accounts and their passwords
      - enabling or configuring latest security features for upgraded systems
      - choosing appropriate security settings in application server, application frameworks, libraries, and databases
      - adhering to the principle of least privilege
      - appropriate permissions and access control lists (ACLs)
      - tamper-resistant distribution media

    * bootstrapping/secure boot
      - with advancements in hw and sw technologies, there are new possibilities for building a trustworthy computing platform
      - trusted computing base (TCB), collection of hw, sw, fw, and basic os services and processes, must provide security guarantees and computing env where security policies are enforced
      - unfortunately, attacks and compromise of the TCB may occur as early as the boot time, and that makes it essential to address the security of the bootstrap mechanism
      - from a technical perspective, security may involve an immutable (muuttumaton) bootstrap process, checking digital signatures, or taking other steps
      - bootstrap mechanism typically involves:
        -- performing self-diagnostics
        -- loading the configuration settings
        -- loading BIOS, hypervisor, operating system, etc.

    * key generation
      - the encryption key represents a valuable resource whose protection throughout its life cycle is critical to the effectiveness of protective measures
      - life cycle of the key begins with key generation
      - device or program used to generate keys is called a key generator and can be an integral part of many bootstrapping processes

      - with Trusted Platform Module (TPM) it is possible to encrypt the encryption key (wrapping process) in a way that decryption requires TPM, providing protection of encryption key
      - with that said, it should be noted that vulns with TPM chips have also been discovered and publicly discussed

      - hw-assisted cryptography provides more protection than a sw-only solution, because hacking hw is more involved than hacking sw
      - there are many reasons for this: most obviously, hacking hw generally requires physical access, whereas sw does not
      - additionally, there aren’t as many known and publicized vulns associated with hw as there are with sw


  -- Least Privilege
    * fundamental security principle of least privilege has been discussed in previous domains
    * according to NIST AC-610, “Organizations employ least privilege for specific duties and information systems. The principle of least privilege is also applied to information system processes, ensuring that the processes operate at privilege levels no higher than necessary to accomplish required organizational missions/business functions. Organizations consider the creation of additional processes, roles, and information system accounts as necessary, to achieve least privilege. Organizations also apply least privilege to the development, implementation, and operation of organizational information systems.”

    * this principle is relevant to deployment and operations of sw
    * least privilege can help with
      - overall system stability (damage that malicious or misbehaving processes can inflict)
      - enhanced security (limiting access of a process to certain resources)

    * implementing least privilege with precision can be challenging in practice, with respect to processes and their needed privileges for accessing and operating on resources


  -- Environment Hardening (EH)
    * involves implementing controls for the operating environment surrounding an organization’s sw to bolster the security posture of applications deployed
    * EH practice focuses on building assurance for the runtime environment that hosts the organization’s sw
    * since secure operation of an app can be deteriorated by problems in external components, hardening this underlying infrastructure improves the overall security posture of the sw

    * security hardening is an iterative process
    * hardening is necessary to protect the system and its critical components against accidental or intentional misuse and attacks

    * hardening eliminates certain attack vectors, reducing the overall attack surface and risk
    * where and when possible, unnecessary programs, accounts, access, ports, and permissions should be removed or disabled

    * hardening to the highest levels possible (at least for critical components) is desirable but may not be feasible or practical
    * when the operating environment, os, web servers, and app servers are hardened, it is likely that apps will experience problems or not function at all
    * this is why hardening and proper testing should be first attempted in a nonproduction environment (one that mirrors the production environment) to avoid business disruptions

    * secure installation processes can help address a common app vuln that was mentioned earlier, namely Security Misconfiguration in OWASP Top 10
    * secure installation processes should be implemented, including:

      - use a repeatable hardening process that makes it fast and easy to deploy another environment that is properly locked down
        -- development, QA, and production environments should all be configured identically, with different credentials used in each environment
        -- this process should be automated to minimize the effort required to setup a new secure environment

      - minimal platform should not have any unnecessary features, components, documentation, or samples
        -- remove or do not install unused features and frameworks

      - review and update the configurations appropriate to all security notes, updates, and patches as part of the patch management process
      - where applicable, segmented app architecture provides effective, secure separation between components or tenants, with segmentation, containerization, cloud security groups (ACLs)
      - follow an automated process to verify the effectiveness of the configurations and settings in all environments


    * note that because of risk analysis and its outcome, in addition to environment hardening, app hardening may also be necessary
    * this could mean making changes to the code in a way that prevents vulns without altering the design, using stronger implementations of libraries, reengineering the app to integrate security features that were absent or insufficient, and other measures


  -- Secure Activation
    * common requirement for commercial sw is generating and validating license keys
    * sw activation is an anti-piracy technology that verifies that the product has been legitimately licensed in compliance with the software’s end-user license agreement
    * sw activation reduces a form of piracy known as casual copying, or the sharing of sw between people in a way that violates the license agreement
    * activation helps protect the intellectual property that lies at the heart of the sw industry
    * it is designed to have minimal impact on users
    * there are various software activation mechanisms

    * for example:
      - hashing product keys and hw serial numbers may be used to generate unique installation keys and numbers
      - these are then sent to the sw publisher for verification of authenticity
      - sw manufacturer provides the user with a unique product serial number that is used during the product installation and verified with the vendor over the internet
      - any applicable limitations pertaining to the sw license will then be applied

    * sw that is not activated will typically remind users to activate at startup or at certain frequencies when limitations are reached
    * after activation, it is also possible for the sw to make activation validity verification at certain frequencies
    * if verification cannot be made, it is possible that sw may stop working


  -- Security Policy Implementation
    * security policy should set the tone for hardening the entire environment for security
    * sw installation and deployment may require creating accounts, directory structures, and configuration files, copying binaries, generating keys, or installing certificates
    * security policy should address the following sw installation and deployment issues:
      - password complexity, aging, and expiration
      - granting privileges to roles and NOT users while adhering to the least-privilege principles
      - secure generation and protection of encryption keys
      - secure configuration settings for the sw


- Security Approval to Operate
  -- when security or operational integrity is a concern, planning the deployment of an IT product or service within an existing system (sw that is developed and planned for deployment within the enterprise network) may involve a formal process to obtain permission, an authorization to operate (ATO) for a specified period
  -- risk acceptance is the responsibility of an authorizing or accepting official while considering various factors

  -- Authorization to Operate (ATO) and Security
    * whereas authorization to operate (ATO) is primarily used in the federal government, private companies may also use it
    * ATO is issued by the body that certifies the service or product only after it determines that product works correctly and is free of flaws that may jeopardize the network and information assets

    * application for an ATO may not be approved
    * possibilities include a denial of ATO or an ATO for an interim period (likely short term, until approval or denial)
    * at a high level, conducting risk management activities to receive an ATO would involve:
      - categorization of the information system based on potential adverse impact
      - selection of baseline security controls
      - implementation of the selected controls
      - assessment of the controls for effectiveness
      - authorization of the system
      - monitoring the system

    * authorization decisions
      - are based on the content of the authorization package, including inputs from the organization’s risk executive (function) and any additional supporting documentation required by the authorizing official
      - the security authorization package provides comprehensive information on the security state of the information system

    * there are two types of authorization decisions that can be rendered by authorizing officials:
      - authorization to operate
      - denial of authorization to operate

    * authorizing official
      - “Security authorizations are official management decisions, conveyed through authorization decision documents, by senior organizational officials or executives (i.e., authorizing officials) to authorize operation of information systems and to explicitly accept the risk to organizational operations and assets, individuals, other organizations, and the nation based on the implementation of agreed-upon security controls,” states NIST.12


  -- Risk Acceptance
    * explicit acceptance of risk and is the responsibility of an authorizing or accepting official who holds a senior management position with the authority and technical knowledge to make such a decision

    * authorizing or accepting official considers many factors when deciding whether the risk to organizational operations (including mission, function, image, reputation) is acceptable
    * balancing security considerations with mission and operational needs is paramount to achieving an acceptable authorization decision
    * risk-related information includes
      - the criticality of organizational missions and/or business functions supported by the information system
      - the risk management strategy for the organization

    * risk management strategy typically describes the following:
      - risk assessment (e.g., tools, techniques, procedures, and methodologies)
      - risk evaluation (e.g., severity or criticality)
      - risk aggregation and categorization from information systems and other sources
      - risk response approaches
      - risk tolerance
      - risk monitoring

    * after a risk determination, organizations can respond to risk in a variety of ways
    * risk-handling strategies could include accepting the risk, avoiding the risk, mitigating the risk, or transferring the risk

    * risk acceptance
      - decisions on the most appropriate course of action for risk response include some form of prioritization
      - some risks may be of greater concern than other risks
      - in that case, more resources may need to be directed at addressing higher-priority risks than other lower-priority risks
      - this does not necessarily mean that the lower-priority risks are ignored
      - rather, it could mean that fewer resources are directed at the lower-priority risks (at least initially), or that the lower-priority risks are addressed later

      - key part of the risk decision process is the recognition that regardless of the risk decision, there typically remains a degree of residual risk
      - organizations determine acceptable degrees of residual risk based on organizational risk tolerance
      - the authorizing official issues an authorization decision for the information system and the common controls inherited by the system after reviewing all the relevant information and, where appropriate, consulting with other organizational officials

      - security authorization decisions are based on the content of a security authorization package and, where appropriate, any inputs received from key organizational officials
      - the authorization package provides relevant information on the security state of the information system, including the ongoing effectiveness of the security controls employed within or inherited by the system



- Information Security Continuous Monitoring (ISCM)
  -- continuous monitoring provides for better risk management decision making, and it helps with
    * anomaly detection
    * incident response
    * compliance with regulatory requirements
    * forensic analysis

  -- continuous monitoring can present challenges, but automated solutions, tools, and technologies support its implementation
  -- logs contain information about events and are generated by various sources within the IT infrastructure

  -- ISCM was introduced previously for maintaining ongoing awareness of information security, vulnerabilities, and threats to support organizational risk management decisions
  -- monitoring information system security has long been recognized as sound management practice

  -- security monitoring helps achieve various objectives, including:
    * better understanding of the current security posture, resulting in better security decision making
    * early detection of attacks, better response to attacks, and reduction of impact from attacks
    * detection of violations of internal security policies or standards
    * compliance with regulatory requirements such as HIPAA, SOX, and GDPR
    * digital forensics analysis
    * reauthorization (as highlighted when discussing ATOs earlier)

  -- robust and comprehensive continuous monitoring strategy integrated into organization system-development life cycle process promotes risk management on an ongoing basis and can significantly reduce the resources required for reauthorization

  -- before implementing security monitoring:
    * ensure that governance has been addressed and that policy and procedures are in place
    * ensure that goals and objectives of monitoring are clear, and that the infrastructure needed to achieve those objectives is or can be put in place
    * understand the requirements imposed by regulations or standards with respect to monitoring
    * identify high-value assets, roles and accounts, and vulnerable systems

  -- implementing a security-monitoring and attack-detection system may require:
    * coping with the high volume of security events that will be generated
    * the storage and management of event information in a central repository
    * identification of and response to attack signatures (patterns of activities that can signal an attack)
    * restricting staff from circumventing security audit controls



  -- Automation
    * automated security testing facilitates test execution repeatedly, consistently, and rapidly at scale
    * earlier in this course, automation of security testing in environments with short release cycles was discussed
    * commercial and open-source tools allow automation of security testing pre- and post- deployment/production release

    * for example:
      - performing automated DAST after release to production
      - verifying that a website is opting into security-relevant HTTP headers such as x-frame-options, content security policy, HSTS, secure and HttpOnly cookie flags
      - verifying that a server’s SSL/TLS configuration is free of protocol versions and cipher suites known to be insecure

    * NIST states that, when possible, organizations should look for automated solutions to lower costs, efficiency, and reliability of monitoring security-related information
    * security is implemented through a combination of people, processes, and technology
    * automation of information security deals primarily with automating aspects of security that require little human interaction
    * automated tools are often able to recognize patterns and relationships that may escape the notice of humans, especially with large volumes of data
    * this includes items such as verifying technical settings on individual network endpoints or ensuring that sw on a machine is up to date with organizational policy
    * automation serves to augment the security processes conducted by security professionals within an organization and may reduce the amount of time a security professional must spend on redundant tasks, thereby increasing the amount of time the trained professional may spend on tasks requiring human cognition


  -- Collect and Analyze Security Observable Data
    * security information and event management, also known as SIEM, solutions were introduced previously
    * SIEM tools, which act as data aggregators and search and reporting systems, have become part of the enterprise security ecosystem
    * regardless of the benefits or shortcomings, SIEMs are ultimately intended to facilitate the identification of abnormal behavior or potential cyberattacks
    * the process involves:
      - collecting data from various sources
      - Normalizing and aggregating the collected data
      - Analyzing the data to detect threats
      - Pinpointing security breaches

    * threat intelligence
      - combining threat intelligence feeds with SIEM providing data collection, consolidation, and analysis can help establish of a robust cyber threat intelligence framework
      - in turn, this can result in knowledge about existing or potential threats and taking steps to prevent hacking attempts or cyberattacks

    * event
      - if an event is an observable occurrence in a network or system, then event management involves monitoring and responding to such occurrences
      - variety of tools and technologies exist to monitor events, such as IDS and logging mechanisms
      - some tools may detect events based on known attack signatures, while others detect anomalies in behavior or performance that could indicate an attack
      - certain events may signal that an incident has occurred, which is a violation or imminent threat of violation of computer security policies, acceptable use policies, or standard computer security practices

    * logs
      - log is a record of the events occurring within an organization’s systems and networks
      - logs are composed of individual entries; each containing information related to a specific event that has occurred within a system or system component

      - originally, logs were used primarily for troubleshooting problems, but logs now serve many functions within most organizations, such as optimizing system and network performance, recording the actions of users, and providing data useful for investigating malicious activity
      - logs have evolved to contain information related to many different types of events occurring within networks and systems
      - within an organization, many logs contain records related to computer security; common examples of these computer security logs are audit logs that track user authentication attempts and security device logs that record possible attacks

    * security logs
      - many logs within an organization contain records related to computer security
      - these computer security logs can be generated by many sources, including security sw such as malware protection sw, firewalls, intrusion detection and prevention systems, operating systems on servers, workstations, networking equipment, and applications

      - the number, volume, and variety of security logs have increased greatly, which has created the need for information system security log management—the process of generating, transmitting, storing, analyzing, and disposing of security log data
      - log management is essential for ensuring that security records are stored in sufficient detail for an appropriate period
      - logs are a key resource when performing auditing and forensic analysis, supporting internal investigations, establishing baselines, and identifying operational trends and long-term problems
      - routine log analysis is beneficial for identifying security incidents, policy violations, fraudulent activity, and operational problems, and as such, supports an ISCM capability


  -- Intrusion Detection and Response
    * intrusion detection is the process of monitoring the events occurring in a computer system or network and analyzing them for signs of possible incidents, which are violations or imminent threats of violation of computer security policies, acceptable use policies, or standard security practices
    * intrusion prevention is the process of performing intrusion detection and attempting to stop possible incidents as they are detected

    * intrusion detection and prevention systems (IDPS) are typically used to record information related to observed events, notify security administrators of important observed events, and automatically generate reports
    * after human review of the report, remediation actions are manually performed
    * many IDPSs can also be configured to respond to a detected threat using a variety of techniques, including changing security configurations or blocking the attack

    * within the context of an ISCM program, IDPSs can be used to supply evidence of the effectiveness of security controls (policies, procedures, and other implemented technical controls), document existing threats, and deter unauthorized use of information systems

    * the key difference between IDS and IPS is that although they appear similar, an IDS is a passive security system, much like a warning light in a car
    * when the sensor that controls the light detects an anomaly, it alerts the driver -> similarly, when an IDS detects an anomaly, it alerts the administrator
    * IPS system is more like an airbag
    * when the car’s sensors detect a collision, they activate the airbag -> similarly, the IPS activates some means of protective action to keep the network safe


  -- Application Performance Monitoring
    * apps are monitored with the objective of enhancing user experience and assuring availability and performance
    * this involves observing app behavior, collecting data on the sources of any app performance issues, alerting, and analyzing data to assess the adverse impact of identified issues
    * solutions today provide performance metrics for apps, allow for the establishment of metrics baseline, and provide capability to monitor apps for variances from the baseline


  -- Post-Deployment Security
    * checks are important when building a level of confidence in the security posture of the app
    * issues identified in post-deployment testing must be tracked for resolution
    * automated security testing facilitates test execution repeatedly, consistently, and rapidly at scale

    * post-deployment security verification should take into consideration missed opportunities in pre-deployment tests to identify vulns and issues arising from build deployment processes
    * post-deployment testing also ensures issues are not introduced due to production changes

    * post-deployment verification
      - testing is performed prior to deployment, but problems could still arise after production release, which may stem from corner-case issues having been missed in test environment
      - this could be because testing was performed using test data as opposed to real production data
      - there may also be issues resulting from manual build deployment processes
      - this could be due to different configuration settings or missing configuration files, missing database scripts, dependencies that were installed incorrectly, or other factors
      - post-deployment verification and validation activities such as pen testing and vuln assessments are crucial
      - monitoring post-deployment in production is another important activity

    * post-deployment issues tracking
      - clearly, any issues or bugs identified as part of the release verification activities should be reported and tracked for resolution
      - the same issue or defect tracking system that was used during the testing phase of the life cycle is also used at this stage
      - issues identified during post-deployment testing may be categorized under a separate issue type (post-deployment issue) to simplify reporting and filtering
      - the case of critical issues is of special importance here, as decisions may be required regarding the necessity of a build rollback

    * post-deployment testing constraints
      - limitations may apply to post-deployment testing
      - the scope of planned testing must consider such factors as:
        -- sensitive live production user/client data
        -- potential impact of testing on the system
          * for example, consider a workload that is strictly used for testing purposes, and which results in records being inserted into the database
          * if the data in the database is aggregated for reporting, test records must be removed prior to the aggregation; otherwise, reports are influenced as a side-effect of tests


- Incident Response Plan
  -- effectively responding to computer security incidents requires planning and preparation
  -- digital forensics is the field of forensic science that is concerned with retrieving, storing, and analyzing electronic data that can be useful in criminal investigations
  -- incidents have precursors (esiaste) and indicators
  -- alerts and logs must be monitored in support of incident response

  -- incidents
    * according to NIST, a computer security incident is defined as:
    “An occurrence that actually or potentially jeopardizes the confidentiality, integrity, or availability of an information system or the information the system processes, stores, or transmits or that constitutes a violation or imminent threat of violation of security policies, security procedures, or acceptable use policies"

    * not every event is considered an incident
    * there are different definitions for events (any observable occurrence in an information system, a significant change in the state of a service)
    * regardless of definition, an event signifies something that is unusual, something that is an exception
    * two examples of events include a network switch going down or a particular service starting on a server

    * incident triage
      - incident is high-magnitude event or series of events that significantly affects organizational assets and requires organization to respond to prevent or limit organizational impact
      - triaging event artifacts is the first step in an analysis process through which an organization recognizes that an incident is underway
      - in the triage process, an organization determines how to categorize an event(s), how to evaluate it, and whether the event reaches the threshold of a declarable incident

      - important activities in the triage and analysis include:
        -- event categorization
        -- events prioritization
        -- event data correlation and analysis
        -- incident declaration
        -- incident analysis and response determination


  -- incident response
    * effective response to computer security incidents can be a complex undertaking and involve substantial planning
    * as with any other significant undertaking, governance—including policy, procedures, roles, and responsibilities—must be first addressed

    * there are several phases to incident response, ranging from initial preparation to post-incident analysis
    * major phases of incident response include preparation, detection and analysis, containment, eradication (hävittäminen), and recovery

    * incident response life cycle as described by NIST SP 800-61 Rev. 220 is depicted in this figure:


                                                  +---------------+
                                                  |               |
                                                  |               V
    Preparation  ----------->   Detection & Analysis             Containment, Eradication, Recovery  ------------->   Post-incident Activity
          ^                                       ^               |                                                     |
          |                                       |               |                                                     |
          |                                       +---------------+                                                     |
          +-------------------------------------------------------------------------------------------------------------+




  -- incident precursors and indicators
    * NIST states:
    “For many organizations, the most challenging part of the incident response process is accurately detecting and assessing possible incidents - determining whether an incident has occurred and, if so, the type, extent, and magnitude of the problem... Signs of an incident fall into one of two categories: precursors and indicators
      - precursor (esiaste) is a sign that an incident may occur in the future
      - indicator is a sign that an incident may have occurred or may be occurring now”

    * example of a precursor might be web server log entries that show the usage of a vulnerability scanner, and an example of an indicator is when antivirus software alerts when it detects that a host is infected with malware


  -- monitor alerts and logs
    * precursors and indicators are identified using many different sources
    * given the context of this discussion, the focus will be on computer security sw alerts and logs
    * alerts may be generated through various sources including IDPS, antivirus and antispam software, and file integrity checking software, among others

    * as mentioned earlier, SIEM products are like IDPS products, but they generate alerts based on analysis of log data

    * logs from OS, services, and apps (particularly audit-related data) are frequently of use when an incident occurs
      - for example, recording which accounts were accessed and what actions were performed
    * organizations should require a baseline level of logging on all systems and a higher baseline level on critical systems
    * logs can be used for analysis by correlating event information
    * depending on the event information, an alert can be generated to indicate an incident

    * other types of logs (firewall and router logs) can also be sources of precursors and indicators

    * incident management platforms and tools, commercial and open source, are available for response teams that can help ensure incidents are resolved effectively
    * search engines used for log analytics on-premises or in the cloud can provide visualization and dashboard capabilities


  -- Root Cause Analysis (RCA)
    * distinction must be made between incident management and problem management
    * incident management has historically referenced fixing an issue as soon as possible, whereas problem management refers to fixing the underlying issue or the root cause

    * RCA is a method of problem solving that looks past the symptoms to identify the underlying events and problems
    * this approach must be structured and methodical

    * among available RCA methodologies, the Five Whys is commonly chosen
    * by repeatedly asking the question “Why?” (five times is a good rule of thumb), you can peel away the layers of symptoms that can lead to the root of a problem
    * here is a simple example of the Five Whys technique:

      Q: Why won’t my computer turn on?
      A: Because it’s not plugged in.

      Q: Why isn’t my computer plugged in?
      A: Because the power cord was pulled out of the electrical socket.

      Q: Why was the power cord pulled out of the socket?
      A: Because I tripped over it earlier.

      Q: Why did I trip on the cord?
      A: Because the cable is on the floor, in a place where I can trip on it.

      Q: Why is the cable in a place where I can trip on it?
      Solution: Find a way to tie up the power cord so that it is out of the way of my feet.

    * although it's called Five Whys, you may find that you will need to ask the question fewer or more than five times before you find the issue that reveals how to solve a problem


  -- Forensics
    * number of crimes that involve computers has grown, spurring an increase in companies and products to assist law enforcement’s use of computer-based evidence to determine the who, what, where, when, and how of crimes
    * as a result, computer and network forensics has evolved to ensure proper presentation of computer crime evidentiary data into court
    * forensic tools and techniques are most often thought of in the context of criminal investigations and computer security incident-handling in response an event
    * this is achieved by investigating suspect systems, gathering and preserving evidence, reconstructing events, and assessing the current state of an event

    * the forensic process includes:

      - collection
        -- involves identifying, labeling, recording and acquiring data from possible sources of relevant data while following guidelines and procedures that preserve data integrity
        -- collection is typically performed asap so as not to lose dynamic data (current network connections), as well as data from battery-powered devices (cell phones)

      - examination
        -- involves forensically processing large amounts of collected data using a combination of automated and manual methods to assess and extract data of particular interest while preserving integrity of data

      - analysis
        -- analysis of the results of the examination, using legally justifiable methods and techniques, to derive useful information that addresses the questions that were the impetus for performing the collection and examination

      - reporting
        -- reporting the results of the analysis, which may include
          * describing the actions used
          * explaining how tools and procedures were selected
          * determining what other actions need to be performed (forensic examination of additional data sources, securing identified vulns, improving existing security controls)
          * providing recommendations for improvement to policies, guidelines, procedures, tools, and other aspects of the forensic process
        -- formality of the reporting step varies greatly depending on the situation


- Patch Management
  -- organizations must establish and maintain security strategies and procedures for patch and vuln management
  -- patch management process consists of activities including those for identifying, acquiring, installing, and verifying patches
  -- patch management should ensure updates are free from defects that hinder business continuity
  -- updates to firmware and software must be tested before they are introduced into production infrastructure
  -- testing should include security and functional considerations

  -- patch and vuln management can be challenging
  -- timing, prioritizing, and testing patches are important activities

  -- “Patch management is the process for identifying, acquiring, installing, and verifying patches for products and systems,” states NIST
  -- “Patches correct security and functionality problems in software and firmware. From a security perspective, patches are most often of interest because they are mitigating software flaw vulnerabilities; applying patches to eliminate these vulnerabilities significantly reduces the opportunities for exploitation. Also, patches are usually the most effective way to mitigate software flaw vulnerabilities and are often the only fully effective solution"
  -- patch management is required by various security compliance frameworks, mandates, and other policies

  -- Timing, Prioritization, and Testing
    * are intertwined issues for enterprise patch management
    * ideally, an organization would deploy every new patch immediately to minimize the time that systems are vulnerable
    * however, this is often not possible because organizations have limited resources, which makes it necessary to prioritize which patches should be installed before other patches
    * further complicating this is the significant risk of installing patches without first testing them, which could cause serious operational disruptions, potentially even more damaging than the corresponding security impact of not pushing the patches out

    * testing patches
      - patches may be complicated, affecting many portions of a system
      - they may replace system files and alter security settings and may also include fixes for multiple vulns
      - this makes it necessary to test patches on nonproduction systems and, subject to organizational policy and procedures as remediation, can easily produce unintended consequences

  -- Patch Management Challenges
    * aside from the timing, prioritization, and testing challenges mentioned above, other patch management challenges include:
      - multiple mechanisms for applying patches (e.g., manual update, automatic update, third-party patch-management solutions)
      - dependency on software inventory, without which correct patches cannot be identified
      - patch side-effects (e.g., patch may alter existing security configuration settings)
      - patch implementation verification (e.g., reboot required)
      - conflicts between application allow-list technologies and patch management technologies

  -- Supply Chain Risks
    * patch management also involves supply chain risk-management issues
    * specifically, it is important to verify that patches can be trusted
    * this includes determining the patch came from legitimate third-party vendor or dev team rather than malicious actor, such as malware developer masquerading as a legit patch source
    * it also means verifying the integrity of the patch - assuring that the patch was not tampered with en route from the supplier
    * before installing any patch, its code signature and hash or checksum should be validated

    * in this regard, not only patches, but also all workarounds, code replacements, component substitutions, and procedural mitigations need to be tested and assessed to verify whether they are effective, do not introduce new vulns or weaknesses, and avoid or minimize interruption or degradation of service
    * third-party developers who have generated patches in response to a vuln then generally announce and provide the patch for those using their sw



- Vulnerability Management
  -- sw vuln management involves various activities, including vuln assessment
  -- vuln scans are normally conducted based on frequencies determined by organizations

  -- involves managing conditions that, if exploited by a threat, render an entity susceptible to a risk
  -- the threats can be natural or man-made, and the entity could be an entire organization or any of its constituent parts
  -- at a high level, vuln management involves:
    * defining a vuln analysis and resolution strategy
    * developing a plan for vuln management
    * implementing the vuln analysis and resolution capability
    * assessing and improving the capability

  -- vuln management is a key component in planning for and determining the appropriate implementation of controls and the management of risk
  -- it is reasonable to say that vulnerability management is central to cyber resilience

  -- sw vulns
    * vulns can be defined (by NIST Information Technology Laboratory’s National Vulnerability Database) as a “weakness in the computational logic (code) found in sw and hw components that, when exploited, results in a negative impact to confidentiality, integrity, or availability
    * mitigation of the vulns in this context typically involves coding changes but could also include specification changes or even specification deprecations (removal of affected protocols or functionality in their entirety)”

    * vulns, whether in custom sw or third-party products, may be identified or detected at any time and under various circumstances
    * they are commonly identified through security analyses and tests performed in the SDLC because of failures reported by users or through security incidents involving the sw during its operation

  -- vulns may exist at any tier of the architecture and at any layer of the sw, including middleware or even the runtime environment
  -- security analyses and tests that developers perform throughout the SDLC should be augmented by ongoing security monitoring and vul assessment of the sw after deployment
  -- this requires the establishment of processes and mechanisms for the ongoing assessment of vulns in sw at installation (baseline assessment) and throughout its operational deployment
  -- post-deployment vuln assessments should be performed:
    * on a regular periodic basis
    * whenever a new, relevant threat is discovered (through ongoing risk management)
    * whenever a security incident involving the sw occurs or is suspected

  -- from a mission/business perspective, not all apps are equally important
  -- further, not all vulns are of the same criticality
  -- as stated previously, the CVSS communicates vuln characteristics and severity
  -- to perform triage on a security vulnerability, CVSS score can help organizations prioritize vulns based on severities determined by vendors


  -- Vulnerability Scanning
    * organizations determine the frequency and comprehensiveness of vuln scans
    * they may also be guided by the security categorization of information systems when applicable
    * policies should ensure that scans include all potential sources of vulns

    * vuln analyses for custom sw apps may require additional approaches such as static analysis, dynamic analysis, binary analysis, or a hybrid approach

    * vuln scanning can include:
      - scanning for patch levels
      - scanning for functions, ports, protocols, and services that should not be accessible to users or devices
      - scanning for improperly configured or incorrectly operating information flow-control mechanisms

   * web app vuln scanning
     - scanners are automated tools that scan web apps, normally from outside, to look for security vulns such as XSS, SQL injection, cmd injection, path traversal, insecure server config
     - tools in this category are frequently referred to as DAST tools
     - many commercial and open-source tools of this type are available, and all tools have their own strengths and weaknesses
     - examples of tools that include web app vuln scanning capability include Zed Attack Proxy and Burp Suite


  -- Vulnerability Data and Notification
    * vuln data is highly sensitive, so vuln management and recording systems need to be protected against unauthorized or arbitrary access
    * vuln notifications to affected stakeholders should document the immediate mitigation actions required and the specific patches or workarounds that need to be implemented
    * notifications should also identify any planned longer-term remediation to be provided later by sw dev (or maintenance) team, with anticipated timelines for delivery/implementation

    * when mitigation or remediation of a vuln cannot be achieved through a simple patch or workaround and necessitates a change to or removal of functionality from the sw, the potential impacts of the sw modification need to be discussed with users and other affected stakeholders to ensure that their conflicting goals are adequately addressed (required functionality versus lack of security assurance or cost versus speed of mitigation/remediation)


- Runtime Protection
  -- tools and technologies can afford applications protection against attacks that exploit common web application vulns
  -- operating systems offer protection mechanisms for applications at runtime

  -- RASP
    * threat intelligence
      - not all vulns are known, as described definitionally by zero-day
      - TI via RASP provides code execution level insights that assist security practitioners with addressing previously unknown security issues

    * application protection
      - provides a powerful medium for detecting known vulns and ameliorating them without disrupting business continuity through automatic runtime response

  -- address space layout randomization (ASLR)
    * makes it difficult to create buffer-overflow attacks by randomizing the memory address for executables at system boot
    * this helps defeat a well-understood attack called “return-to-libc,” where exploit code attempts to call a system function such as the socket() function in wsock32.dll to open a socket, or LoadLibrary in kernel32.dll to load wsock32.dll in the first place
    * the job of ASLR is to move these function entry points around in memory, so they are in unpredictable locations
    * DLL or EXE could be loaded into any of 256 locations, which means an attacker has a 1/256 chance of getting the address right
    * in short, this makes it harder for exploits to work correctly
    * it should be noted that exploit techniques in the presence of ASLR have been introduced (return 0 programming), so ASLR should only be considered as part of layered defenses

  -- DEP (Data Execution Prevention)
    * enforced by both hw and sw
    * Unix and Linux equivalent of Windows DEP is Executable Space Protection (ESP)
    * exploit techniques for DEP are similar to ASLR


- Continuity of Operations
  -- organizations must concentrate on the continuity of operations for mission-critical services
  -- continuity planning is often subject to internal and external compliance requirements and may involve initial business impact analysis (BIA) to identify those mission-critical services
  -- stakeholders’ tolerance to the outage of those services or data loss in case of disruptions must be determined subsequently
  -- business/mission requirements will drive disaster recovery (DR) strategies adopted by the organization

  -- must be planned for
  -- business continuity and disaster recovery (BCDR) planning is a requirement by various regulations and standards and significant part of many information security frameworks
  -- business impact analysis should yield
    * maximum tolerable downtime (MTD)
    * recovery time objective (RTO)
    * recovery point objective (RPO)

  -- this will drive adopted strategies for backup and recovery
  -- DR strategies must meet an organization’s recovery requirements while remaining in compliance with applicable laws and regulations

  -- minimizing the effects of outages and disruptions on business operations is the key driver for continuity planning
  -- organizations may have separate business continuity (BC) and DR plans or may have a BCDR plan that addresses both

  -- information systems are vulnerable to a variety of disruptions, ranging from mild (short-term power outage, disk drive failure) to severe (e.g., equipment destruction, fire)
  -- much vulnerability may be minimized or eliminated through management, operational, or technical controls as part of the organization’s resiliency effort
  -- however, it is virtually impossible to eliminate all risks

  -- governance must be addressed first
  -- policy and procedures must be established, and a business continuity plan (BCP) developed
  -- BCP focuses on sustaining an organization’s mission/business processes during and after a disruption
  -- example of a mission/business process may be an organization’s payroll process or customer service process
  -- BCP may be written for mission/business processes within a single business unit or may address the entire organization’s processes
  -- BCP may also be scoped to address only the functions deemed to be priorities
  -- because mission/business processes use information systems (ISs), the BC planner must coordinate with IS owners to ensure that the BCP expectations and IS capabilities match

  -- DRP
    * applies to major, usually physical disruptions to service that deny access to the primary facility infrastructure for an extended period
    * is an IS-focused plan designed to restore operability of the target system, app, or computer facility infrastructure at an alternate site after an emergency

    * in general, as part of continuity planning, critical functions and key resources must be analyzed to avoid disruptions due to single point of failure
    * high availability is one of the primary aspects of continuity planning
    * single points of failure must be identified and addressed through various means (component redundancy) to keep the environment running


  -- Business Impact Analysis (BIA)
    * BCDR begin with the understanding of an organization’s mission, goals, and objectives
    * organizational BCDR must be planned in a manner that is appropriate to business requirements

    * through the process of business impact analysis (BIA), organizations can determine mission/business processes and recovery criticality
    * mission/biz processes supported by the system are identified and the impact of system disruption to those processes is determined along with outage impacts and estimated downtime
    * downtime should reflect the maximum time that an organization can tolerate while still maintaining the mission

    * maximum tolerable downtime (MTD)
      - represents the total amount of time the system owner/authorizing official is willing to accept for a mission/business process outage

    * recovery time objective (RTO)
      - defines the maximum amount of time that a system resource can remain unavailable before there is an unacceptable impact on other system resources, supported mission/business processes, and the MTD

    * recovery point objective (RPO)
      - represents the point in time, prior to a disruption or system outage, to which mission/business process data can be recovered (given the most recent backup copy of the data) after an outage


  -- Backup, Archiving, and Retention
    * relevant to the continuity of operations
    * data backup
      - for various reasons (corruption, accidental deletion of db) there may be a need to quickly recover data in current/recent use from data backup sources
      - restoration speed may be critical
      - choosing the right data backup strategy depends on many factors, but it is primarily determined by the business/mission/compliance requirements and available resources

    * common backup methods for performing system backups include
      - Full
        -- full backup captures all files on the disk or within the folder selected for backup
        -- because all backed-up files are recorded to a single media or media set, locating a particular file or group of files is simple
        -- however, the time required to perform a full backup can be lengthy
        -- maintaining multiple iterations of full backups of files that do not change frequently (such as system files) could lead to excessive, unnecessary storage reqs

      - Incremental
        -- incremental backup captures files created or changed since the last backup, regardless of backup type
        -- incremental backups afford more efficient use of storage media, and backup times are reduced
        -- however, to recover a system from an incremental backup, media from different backup operations may be required
        -- for example, consider a case in which a directory needs to be recovered
        -- if the last full backup was performed three days prior and one file had changed each day, then the media for the full backup and for each day’s incremental backups would be needed to restore the entire directory

      - Differential
        -- differential backup stores files created or modified since the last full backup
        -- therefore, if a file is changed after the previous full backup, a differential backup will save the file each time until the next full backup is completed
        -- differential backup takes less time to complete than a full backup
        -- restoring from a differential backup may require fewer media than an incremental backup because only the full backup media and the last differential media would be needed
        -- as a disadvantage, differential backups take longer to complete than incremental backups because the amount of data since the last full backup increases each day until the next full backup is executed


    * secure backup and restoration planning
      - sometimes data must be securely restored to its previous state as part of a recovery activity
      - this requires a secure backup and restoration strategy appropriate for the organization and minimizes the impact of unplanned downtime, regardless of the size of the organization

      - modern backup technologies range from basic tape backup systems to cloud-enabled platforms
      - cloud backup is a common backup technology
      - cloud service providers and customers have unique responsibilities
      - in cloud computing environment, backup and restoration responsibilities vary based on their cloud service model: SaaS, IaaS, and PaaS
        -- SaaS: the cloud service provider bears essentially all backup and restoration responsibilities
        -- IaaS: the customer has total responsibility
        -- PaaS: split, where generally customer is responsible for app- and data-related activities and the cloud service provider (CSP) has responsibility for OS-related duties
      - configuration data for hosts in the cloud environment should also be part of the backup plan.

      - having copies of data is one requirement for DR, but on its own, it does not ensure that business can resume in the case of a disaster
      - backup and restore procedures must be tested
      - the method and frequency of the test depend on various factors, including the backup and restore strategy selected by the organization
      - testing and restoration of hosts to validate proper functioning of the backup system should be conducted as part of the DR plan


    * data archiving
      - archiving is reference to collection of historical records retained for periods of time likely to satisfy operational needs of the organization or to meet compliance requirements
      - cost and availability considerations can affect data access procedures
      - continually changing technologies can also have a major impact on desired archiving formats
      - archived data must continue to be protected in compliance with organizational policies and regulatory requirements

      - media selection may be an important consideration depending on archiving requirements and strategies used
      - tapes may have limited life and prove to be difficult to search, but they may be appropriate for specific situations
      - optical forms of storage such as DVDs may offer some advantages but be insufficient to meet other requirements

      - keep in mind that depending on the requirements (retention and searchability of the archives), data archiving solutions offered through leading cloud service providers may prove to be advantageous


    * backup vs archiving
      - key difference between data backup and data archiving is that data backups are designed for the rapid recovery of operational data, while data archiving stores data that is no longer in day-to-day use but must be retained
      - backups are performed to preserve business continuity, and archiving is relied upon for achieving compliance

      - it is important to understand that technologies used for backup and archiving may have their own characteristics and needs in terms of retention and recovery objectives
      - backups preserve business continuity, and archives support compliance


  -- Disaster Recovery
    * closely related to the BC initiatives of the organization, and concerns formulating a plan for the recovery of IT systems and services from disruptive events
    * organizations should establish and maintain a disaster recovery plan (DRP) aligned with their mission and service delivery requirements

    * should not focus on the recovery of every IT function, system, or service, but rather on the mission-critical ones
    * it should also ensure that the recovery time objectives (RTOs) and recovery point objectives (RPOs) of the business (or lines of business) can be achieved through the adopted strategies
    * business impact analysis can be used for the purpose of establishing these objectives

    * data residency requirements
      - addressing data residency requirements should be a key consideration during DR planning, as replication of assets (db) across multiple locations must be assumed
      - disaster recovery site is likely to be geographically remote from any primary sites
      - relevant locations to be considered depend on the geographic scale of the calamity (onnettomuus) anticipated
      - this is particularly important since DR in the cloud has gained widespread acceptance


  -- Cyber Resiliency
    * according to NIST, cyber resiliency is the ability to anticipate, withstand, recover from, and adapt to adverse conditions, stresses, attacks, or compromises on systems that use or are enabled by cyber resources
    * cyber resiliency is intended to enable mission or business objectives that depend on cyber resources to be achieved in a contested cyber environment
    * cyber resiliency can refer to a property of a system, network, service, system-of-systems, mission or business function, organization, critical infrastructure sector or subsector region, or nation
    * business requirements for resiliency highlight the need for rapid recovery after of unpredictable and disastrous events

    * cloud resiliency
      - key reason that DR in the cloud has gained momentum
      - cloud resiliency represents the ability of a cloud services data center and its associated components (servers, storage) to continue operating in the event of a disruption, whether caused by equipment failure, power outage, or a natural disaster
      - given that most cloud providers have a significantly greater number of devices and redundancies in place than a standard in-house IT team, cloud resiliency should typically be far higher, offering equipment and capabilities ready to failover, multiple layers of redundancy, and enhanced exercises to test such capabilities

    * operational redundancy, survivability, and threats
      - security practitioners use various terms that are related to the concept of cyber resiliency interchangeably, but there are subtle differences
      - for instance, cyber resiliency, survivability, and redundancy are closely related
      - as described earlier, cyber resiliency a system’s ability to withstand compromises to CIA
      - if it is difficult to degrade a system or multiple vectors of attack are the only method of compromise, security practitioners might reference the system as being resilient

      - survivability and redundancy are system traits (ominaisuuksia) that characterize a cyber system
      - NIST defines survivability as the ability of a system to minimize the impact of a finite-duration disturbance by reducing the likelihood or magnitude of a disturbance
      - suppose several clustered, geographically distributed servers were used to host a high-availability web application in Turkey
      - one of the data centers in Trabzon flooded and all that location’s technical infrastructure was destroyed
      - the impact of this natural event on the web application is only slightly higher latency for users that still falls within business objectives
      - the web application is still available
      - the clustered and distributed server architecture in this scenario provides survivability by limiting the disruption to slightly higher latency
      - the architecture also makes complete denial of service highly unlikely
      - for this reason, the web application seems to have good survivability due to redundant servers
      - due to infrastructure redundancies, the system has good survivability against natural disasters and other threat actors which might create a DoS for the web application
      - if all major threats are accounted for and the system can withstand most threats, then we would consider the web application cyber resilient


  -- Erasure Coding
    * redundant array of independent disks (RAID) configurations are used to reconstruct data that may be corrupted or lost in storage by using data stored elsewhere in the array
    * erasure coding can be used to achieve the same objective - fault-tolerance
    * cloud storage is one use case for erasure coding, there are other use cases as well

    * use case
      - to provide high data availability, assurance, and performance, storage applications will often use the data dispersion (hajonta) technique
      - for this technique, each storage block is fragmented, and storage application writes each bit into different physical storage containers to achieve greater information assurance
      - this resembles the traditional RAID system, except that the applications are scattered across different physical devices

      - underlying architecture of this technology is based on erasure coding, which chunks a data object (like a file with self-describing metadata) into segments
      - each segment is encrypted, cut into 16 slices, and dispersed across an organization’s network to reside on different hard drives and servers
      - if the organization has access to only 10 of the slices, because of disk failures, for instance, the original data can still be put back together
      - if the data is generally static with few rewrites, such as media files and archive logs, creating and distributing the data is a one-time cost
      - if the data is dynamic, the erasure codes must be recreated, and the resulting data blocks redistributed


- Integration of SLOs and SLAs
  -- organizations often rely on third parties for technical and nontechnical services and components to support systems
  -- when supplier components are integrated into critical organizational infrastructure, security practitioners should ensure business units are incorporating security considerations into contractual artifacts

  -- Contracting for Operations and Maintenance Success
    * as organizational needs evolve, information technology and security requirements change, and so will the definition of corresponding roles within an enterprise
    * regardless of job role definitions and technology changes, the continuity of operations remains a steadfast organizational goal
    * further, businesses and security practitioners should ensure suppliers can and will provide components and services to the level required to maintain operations continuity

    * following diagram shows the integration of service-level agreement (SLA), service-level objective (SLO), and service-level indicator (SLI) to guarantee continuity of operations when relying on integrated supplier components

      - SLO: internal stakeholders decide a threshold for acceptable performance and security, and how to measure
      - SLI: internal and supplier stakeholders decide what is being measured to determine compliance
      - SLA: signed agreement between internal and supplier stakeholders, highlighting provider commitments

                                  SLO
                                   |
                                   |
                                   V
                    SLI ---->  Supplier  <---- SLA
                              Continuity


      - SLA
        -- represents a commitment between a service provider and one or more customers and addresses specific aspects of the service, such as responsibilities, details on the type of service, expected performance level, and requirements for reporting, resolution, and termination
        -- if the acquisitor has measurable requirements for maintenance, audits, security assessment, performance, availability, or staffing based on compliance or organizational needs, then the SLA is an opportunity to define such parameters
        -- it is better to establish expectations and reach consensus before signing a contract rather than rely on good faith
        -- properly constructed and combined with SLO and SLI create a relationship between an organization and supplier that ensures operational continuity
        -- signed agreements between two organizations wherein one is receiving a service and one is providing the service
        -- provider of the service or component agrees to ensure certain attributes (availability, uptime)
        -- normally guarantees response times when the provided service or component falls below a certain performance threshold
        -- good SLAs also include consequences for providers who fail to meet obligations

      - SLO
        -- value or range of values that describes acceptable supplier performance and is typically determined by SLI
        -- security practitioners should ensure the SLO represents the organizationally defined criteria for operational continuity when integrating the supplier deliverables
        -- if the organization requires critical infrastructure to have an availability of 99.9% and the supplier only offers 92% uptime, then it is unlikely the organization will be able to meet operating requirements provided the supplier offering is integrated into the organization’s critical infrastructure
        -- for the example of uptime, an SLO of 99.9% would be the minimum acceptable value from the supplier

      - SLI
        -- quantitative means to determine whether the supplier is compliant with the SLA commitments
        -- should directly measure the service or component; however, this is not always possible
        -- for example, if an organization acquired sw service, it might be concerned with end-user request latency
        -- however, it might be difficult or unreliable to produce such information, so the SLI may be based on server latency
        -- represents the minimum measure, and therefore the results of measurement must meet or exceed the SLI to maintain compliance with SLA


Authorization to Operate - The official management decision given by a senior organizational official to authorize operation of an information system and to explicitly accept the risk to organizational operations (including mission, functions, image, or reputation), organizational assets, individuals, other organizations, and the Nation based on the implementation of an agreed-upon set of security controls. Source: https://csrc.nist.gov/glossary/term/authorization_to_operate
Business Impact Analysis (BIA) - An analysis of an information system’s requirements, functions, and interdependencies used to characterize system contingency requirements and priorities in the event of a significant disruption. Source: https://csrc.nist.gov/glossary/term/business-impact-analysis
Certificate (Digital Certificate) - A reference to a public key certificate, as a component of the public-key infrastructure (PKI) and based on the X.509 standard. Certificate fields include information that identifies the issuer of the certificate, owner of the certificate, owner’s public key, and validity period of the certificate, among other fields.
Configuration Item - An aggregation of information system components that is designated for configuration management and treated as a single entity in the configuration management process. Source: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-53r5.pdf
Change Control Board - In the change control process, a committee of stakeholders, (comprised of management and security leadership), which reviews the change process and specific proposed changes that may affect enterprise operations. Makes final and binding decisions on implementing changes.
Configuration Management (CM) - A collection of activities focused on establishing and maintaining the integrity of information technology products and information systems, through control of processes for initializing, changing, and monitoring the configurations of those products and systems throughout the system development lifecycle. Source: https://csrc.nist.gov/publications/detail/sp/800-53/rev-5/final
Cyber Resiliency - The ability to anticipate, withstand, recover from, and adapt to adverse conditions, stresses, attacks, or compromises on systems that use or are enabled by cyber resources. Cyber resiliency is intended to enable mission or business objectives that depend on cyber resources to be achieved in a contested cyber environment.
Disaster Recovery Plan (DRP) - Management policy and procedures used to guide an enterprise response to a major loss of enterprise capability or damage to its facilities. The DRP is the second plan needed by the enterprise risk managers and is used when the enterprise must recover (at its original facilities) from a loss of capability over a period of hours or days. Source: https://csrc.nist.gov/glossary/term/disaster-recovery-plan
Dynamic Application Security Testing (DAST) - A reference to a form of application security testing method and associated tools and technologies that focus on testing the application from the outside and in the runtime environment. Primarily used for web applications, DAST is appropriate for detecting certain types of security flaws/vulnerabilities, but most importantly adequate for detecting configuration issues. DAST has its own pros and cons.
Hardening (Software) - A reference to the process of applying secure configurations (to reduce the attack surface) and locking down various software, including operating system, web server, application server, application, etc. Hardening is normally performed based on industry guidelines and benchmarks such as those provided by the Center for Internet Security (CIS).
Maximum Tolerable Downtime - The amount of time mission/business process can be disrupted without causing significant harm to the organization’s mission. Source: NIST SP 800-34 Rev. 1
Recovery Point Objective - A measure of how much data the organization can lose before the organization is no longer viable.
Recovery Time Objective (RTO) - The overall length of time an information system’s components can be in the recovery phase before negatively impacting the organization’s mission or mission/business processes. Source: https://csrc.nist.gov/Glossary/Term/Recovery-Time-Objective
Service-Level Agreement (SLA) - A contract that exists between customers and their service provider or between service providers. It records the common understanding about services, priorities, responsibilities, guarantees, warranties, etc. to be provided — collectively, the level of service. The SLA may specify the levels of availability, serviceability, performance, operation, or other attributes of the service, such as billing. In some contracts, penalties may be agreed upon in the case of noncompliance.
Service-Level Indicator - A quantitative means to determine whether the supplier is compliant with the SLA commitments.
Service Level Objective - SLAs can contain numerous service performance metrics with corresponding SLOs. The level of service can also be specified as “target” and “minimum,” which informs customers about what to expect (the minimum), while providing a measurable (average) target value that shows the level of organization performance.
Staging and Conducting Software Assurance - The level of confidence that software is free from vulnerabilities either intentionally designed in or accidentally inserted at any time during its life cycle, and that the software functions as intended. Source: https://csrc.nist.rip/external/nvlpubs.nist.gov/nistpubs/ir/2015/NIST.IR.8074v2.pdf
Survivability -The ability of a system to minimize the impact of a finite duration disturbance on value delivery (i.e., stakeholder benefit at cost), achieved through the reduction of the likelihood or magnitude of a disturbance; the satisfaction of a minimally acceptable level of value delivery during and after a disturbance; and/or a timely recovery. Source: https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-160v2r1.pdf


QUIZ


Question 1    1 / 1 point
At the conclusion of application acceptance testing, it was determined that the application should not be deployed to production. Operational constraints require the host environment for the application to enable insecure ports and service protocols or else the application will not function properly. Which risk response was exercised? (D7.1, L7.1)

A) Accepting
--> B) Avoiding
C) Transferring
D) Sharing

Correct. After a risk determination, organizations can respond to risk in a variety of ways: accepting risk, avoiding risk, mitigating risk, sharing risk, transferring risk, or a combination of these. The organization has avoided the risk, and now it needs to explore other alternatives.


Question 2    1 / 1 point
Which of the following terms best describes a set of reviewed and accepted configuration items for a production software system? (D7.2, L7.2)

--> A) Baseline
B) Configuration Item
C) Change Control Board
D) Software Configuration

Correct. A baseline, one or many configuration items, is approved by a Change Control Board through a formal process.


Question 3    1 / 1 point
Incorporated Enterprise (IE) is working on a sensitive government contract that involves national security. Due to the nature of the system, IE wants to ensure reverse engineering and exploitation of the software and subsystems during deployment will not occur to prevent unauthorized technology transfer, alteration of capability, or rapid countermeasures development. Which of the following concepts best supports IE's goal? (D7.3, L7.3, L7.4)

A) Security Controls
--> B) Anti-tamper
C) Software Assurance
D) DevSecOps

Correct. Anti-tamper prevents reverse engineering and exploitation of the software and subsystems that would result in unauthorized technology transfer, alteration of capability, and other issues.


Question 4    1 / 1 point
Fatima is the first cybersecurity engineer hired at a small software company that produces mobile apps. After receiving approval and scope from the Chief Technology Officer, she scans the production infrastructure and finds several vulnerabilities. It becomes apparent that ameliorating the security issues will be at least a six-month process. Which of the following comments is most appropriate? (D7.3, L7.5)

A) Fatima should start as soon as possible and start patching or mitigating CVEs.
B) Fatima should address the CVEs starting with the highest number to lowest.
--> C) Fatima should address the CVEs starting with the highest CVSS score to lowest.
D) Fatima should work with the teams to fix defects in the source code repository.

Correct. CVSS scores are based on base, temporal, and environmental assessments of CVEs. Fatima shouldn't randomly start. The higher the CVE number the newer the issue—that doesn't describe the organizational impact. Fixing source code is great but will not result in production infrastructure hardening.


Question 5    0 / 1 point
Which of the following is true about encryption keys? (D7.4, L7.6)

A) Encryption keys must always be stored in a key vault.
--> B) Encryption keys must be protected throughout their life cycles.
C) The public key of the sender of a message is used to encrypt the message to achieve message confidentiality.
D) The private key of the sender of a message is used to encrypt the message to achieve message confidentiality.

This option isincorrect because it suggests that the sender's private key is used to encrypt the message. In asymmetric encryption, the private key is used for decryption, not encryption. The private key is kept secret and is used by the recipient to decrypt the message encrypted with their public key.


Question 6    1 / 1 point
Softwarez R Us, also known as SRU, combines software product key and hardware serial number to create a unique installation key. SRU verifies the key value prior to activation. What is most likely SRU's primary objective?  (D7.5, L7.7)

A) Mitigate the risk of reverse engineering and exposing intellectual properties.
B) Protect against the risk of corrupting the software.
--> C) Mitigate the risk of piracy and ensure that software is legitimately licensed.
D) ransfer the risk to the software user.

Correct. Softwarez R Us is focusing on secure activation, which is done to address the risk of piracy and ensure proper licensing. Protecting against reverse engineering or corruption of the software would involve taking other actions by Softwarez R Us. Creating an installation key is not about transferring the risk to the software user.


Question 7    1 / 1 point
Which of the following is the primary objective of software hardening in production environments? (D7.5, L7.7)

A) Make it easier to identify hard-to-find security bugs during the testing phase.
B) Provide hard evidence during the forensic analysis after an incident.
--> C) Reduce the attack surface by eliminating attack vectors.
D) Isolate issues in code that lead to injection vulnerability.

Correct. You are looking for the primary objective of hardening software. When you harden software, you secure it by reducing its surface of vulnerability. Hardening software can help reduce the attack surface of the software and eliminate certain attack vectors. The primary objective of hardening is not to make it easier to find hard-to-find security bugs. Hardening is not intended to provide evidence during the forensics analysis, nor is it intended to isolate issues in code that lead to injection vulnerability.


Question 8    1 / 1 point
Which of the following statements is not true? (D7.6, L7.9)

A) Authority to operate is primarily used in the federal government when security or operational integrity is a concern.
--> B) The software QA team is the entity that has the responsibility to ensure authority to operate.
C) Authority to operate may be denied when risks are unacceptably high.
D) Authority to operate is granted for a finite period.

Correct. The software QA team is not responsible for issuing an ATO, as it represents an official management decision to authorize the operation of an information system.


Question 9    0 / 1 point
When leveraging Risk Management Framework (RMF), who approves a system to operate? Who is responsible for maintaining information and system assurance throughout the life cycle? (D7.6, L7.9)

--> A) ATO, ISSO
B) ATO, ISSM
C) AO, ISSO
D) AO, ISSM

The ISSM is responsible for overseeing the security program and managing the security risk of the system, but they do not have the authority to grant an ATO.


Question 10   1 / 1 point
Which of the following will benefit from security monitoring and logging? (D7.7, L7.10)

A) Detecting violations of security policies and standards
B) Detecting intrusion attempts
C) Policy updates
--> D) All the above

Correct. Security monitoring and logging can detect policies and standards violations and intrusion attempts. Artifacts from monitoring and logging may illuminate the need for policy updates based on observed behaviors.


Question 11   0 / 1 point
What is the primary goal of post incident activities within incident response? (D7.8, L7.11)

--> A) To improve future iterations of the incident response life cycle through lessons learned
B) To determine faulty hardware and software
C) To determine who failed to execute roles and responsibilities properly
D) To improve software assurance practices

Answers B and C do not reflect the primary goal of post incident activities within incident response because deciding blame is not objective. Actions for software assurance may result from post incident activity but are typically located much earlier in the system life cycle.


Question 12   1 / 1 point
What is the primary distinction between "incident management" and "problem management"? (D7.8, L7.11)

A) Incident management fixes the underlying issue, while problem management focuses on resolving issues as soon as possible.
--> B) Incident management refers to fixing issues as soon as possible, while problem management identifies the root cause of events and problems.
C) Incident management involves structured and methodical problem solving, while problem management focuses on quick resolution.
D) Incident management is concerned with events that significantly affect organizational assets, while problem management deals with precursor and indicator analysis.

Correct. Incident management refers to fixing issues as soon as possible, while problem management identifies the root cause of events and problems. Incident management is focused on resolving issues and restoring services as quickly as possible when incidents occur. Problem management is concerned with identifying the underlying cause of recurring incidents and addressing that cause to prevent them from happening again.


Question 13   0 / 1 point
Which of the following best describes the relationship between patch and vulnerability management? (D7.9, L7.12)

A) Patch management and vulnerability management are synonymous.
B) Vulnerability management remediates issues. Patch management remediates patches.
--> C) Vulnerability management identifies vulnerabilities. Patch management remediates vulnerabilities.
D) Patch management addresses functional software defects. Vulnerability management addresses software security defects.

Patch management primarily addresses software vulnerabilities, not functional software defects. The purpose of patch management is to deploy software updates or patches to fix security vulnerabilities. Vulnerability management, as stated in the correct answer, focuses on identifying and mitigating vulnerabilities in software systems.


Question 14   1 / 1 point
What is the main goal of vulnerability management? (D7.10, L7.13)

--> A) To reduce overall risk exposure for an organization.
B) To reduce vulnerabilities.
C) To understand patching needs.
D) To understand technical debt.

Correct. A security practitioner should understand that security exists to support the organization. Vulnerability management is about reducing potential exposure from vulnerabilities, but the main goal is to strengthen the organizational risk posture.


Question 15   1 / 1 point
Priya scanned the corporate production infrastructure for vulnerabilities two months ago. Since the scan, there have been no changes to hardware or software baselines. Which of the following statements is true? (D7.10, L7.13)

A) Priya does not need to scan for vulnerabilities until there are infrastructure changes.
--> B) Priya should scan for vulnerabilities regularly, even if there are no changes.
C) Priya should refrain from scanning for vulnerabilities in production.
D) Priya should scan for common weakness enumerations (CWEs) and common vulnerabilities enumerations (CVEs) when baseline changes occur.

Correct. Even if infrastructure does not change the list of known vulnerabilities and threat landscape may change with time. Updating tool definitions might yield new results on the same unchanged infrastructure. We are focused on CVE in production not weaknesses (code defects). Scanning should be automated and run more frequently.


Question 16   1 / 1 point
Aquene works for a technology firm in Arizona, United States that deals with modern containerized cloud application security. Which of the following security technologies should she recommend for preventing the exploitation of containerized apps? (Note that the infrastructure contains no true perimeter firewall or designated network traffic path.) (D7.11, L7.14)

A) Web Application Firewall (WAF)
--> B) Runtime Application Self-Protection (RASP)
C) Unified threat management (UTM)
D) Virtual Machine Clustering

Correct. RASP will best serve the network architecture of the described scenario. Within traditional on-premises infrastructure, Aquene may recommend firewall devices such as UTM and WAF where traffic paths are predetermined. VM clustering supports availability of VMs which doesn't align well with containerization or exploitation within the problem statement.


Question 17   1 / 1 point
Which of the following is typically used to for nonoperational data storage? (D7.12, L7.15)

A) Data Backups
--> B) Data Archives
C) Data Redundancy
D) Data Anonymization

Correct. Data archiving is normally used for compliance and will not satisfy operational requirements like data redundancy or backups.


Question 18   1 / 1 point
Which of the following phrases BEST describes business continuity plans and disaster recovery plans? (D7.12, L7.15)

A) Methods to prevent disasters
--> B) Preparations to deal with disasters
C) Redundancy and alternate sites
D) None of these

Correct. Plans do not prevent disruptions to continuity they plan to address them when they occur. Methods to prevent disasters might include implementing security controls to bolster redundancy such as alternate sites or the usage of cyber-resilient engineering practices earlier in the systems engineering life cycle.


Question 19   0 / 1 point
Charles has found some cloud service contract artifacts within his organization that were stored in a shared drive with little additional information. One artifact includes several similar pieces of information such as...

98% availability in a rolling 30-day window
Server response latency within 1.5–2.5ms
....
What is he likely looking at? (D7.13, L7.16)

A) Service-level Indicators (SLI)
--> B) Service-level Objectives (SLO)
C) Service-level Agreements (SLA)
D) Key Performance Indicators (KPI)

Service-level agreements (SLAs) are contracts that define the agreed-upon service levels between a service provider and a customer. The artifacts described in the scenario are not the SLAs themselves but rather the performance targets (SLOs) that may be included in an SLA.


Question 20   0 / 1 point
Which of the following terms best represents a commitment between a service provider and one or more customers and addresses specific aspects of the service, such as responsibilities, details on the type of service, expected performance level, and requirements for reporting, resolution, and termination? (D7.13, L7.16)

A) Non-disclosure agreement (NDA)
B) Memorandum of understanding (MOU)
C) Service-level objectives (SLO)
--> D) Service-level agreement (SLA)

Service-level objectives (SLOs) are the specific targets or goals for performance measurements in a service. While SLOs are related to service commitments, they do not encompass the comprehensive agreement and specific aspects mentioned in the question.





DOMAIN 8: Secure Software Supply Chain
--------------------------------------

- Software Supply Chain Risk Management
  -- organizations should consider implementing a standardized process to address supply chain risk and educate their acquisition workforce on threats, risk, and required security controls
  -- cyber supply-chain risk covers a wide range of issues
  -- poor information security practices by lower-tier suppliers and compromised sw or hw purchased from suppliers are two common examples

  -- Software Supply Chain
    * there are many contributing stakeholders in a typical sw supply chain
    * organizations commonly acquire apps that are built using commercial-off-the-shelf (COTS) or open-source-software (OSS) code
    * to increase dev efficiency and decrease dev time, organizations often accept heavy reliance on third-party code (libs and components) when developing apps
    * what is the genesis of the inherited source code?
    * who developed it?
    * who maintains it?
    * sw security practitioners must mitigate the risk of vulns and defects inherited from sw supply chain

  -- in essence, sw supply chain is the network of stakeholders that contribute to the content of a sw product or that have the opportunity to modify its content

  -- as more countries modernize, sw development has become a global activity
  -- the mix-ins of global sw participation and nations with divergent agendas have raised concerns about the sw supply chain
  -- originating entities who possess design and architecture knowledge are many abstraction layers away from the consumer, making them unavailable to support or verify components
  -- at each step in the sw supply chain, an organization acquires sw products or services from sw suppliers
  -- such sw products or services may support sw development, be incorporated into a product and redistributed, or resold as originally acquired

  -- risks can be introduced at any point in the supply chain and may be inherited by each subsequent acquisition
  -- for these reasons, supply chain risks should be mitigated as early as possible in the acquisition life cycle
  -- mitigations generally start with contract acquisition language that includes supply-chain risk management considerations

  -- many organizations may participate in the sw supply chain
  -- relationship between the participating organizations may be formally defined in some cases and be informal in others
  -- where the relationships are formal, requirements are defined, and contracts may be executed to govern the relationship

  -- sw supply chain is example of system-of-systems env where multiple, independently managed organizations provide technical capabilities via set of interdependent, networked systems
  -- sw supply chain assurance is defined as justified confidence that sw product functions as intended and is reliable, safe, and secure

  -- Software Supply-Chain Risk Management
    * NIST Special Publication 800-161r1 (Supply Chain Risk Management Practices for Federal Information Systems and Organizations) states:
      “Information and Communications Technology (ICT) supply chain risks are associated with an organization’s decreased visibility into, and understanding of, how the technology that they acquire is developed, integrated, and deployed. They are also associated with the processes, procedures, and practices used to assure the integrity, security, resilience, and quality of the products and services.”

    * risk management is a comprehensive process (FARM) that requires organizations to:
      - frame risk (establish the context for risk-based decisions)
      - assess risk
      - respond to risk once determined
      - monitor risk on an ongoing basis using
        -- effective organizational communications
        -- feedback loop for continuous improvement in the risk-related activities of organizations

    * steps in the risk management process (frame, assess, respond, monitor) are iterative and not inherently sequential in nature
    * risks of the supply chain must be identified, assessed, responded to, and monitored

    * identify
      - organizations need to perform due diligence on their vendors to identify the supply chain risks and integrate this process into their overall risk management processes
    * assess
      - test assumptions to understand risk posture
      - risk inputs including tolerance, threats, policy, and constraints are combined to determine the probability and impact of a supply chain compromise
      - assessments aid organizations in updating risk framing by understanding present internal and external conditions, which change over time
      - cyber focused supply chain risk assessments should be incorporated into the enterprise’s higher-level risk assessment processes
    * respond
      - risk response identifies, evaluates, decides on, and implements appropriate courses of action to accept, avoid, mitigate, share, or transfer risk

        -- risk acceptance
          * appropriate risk response when the identified risk is within the organizational risk tolerance
          * differs from ignoring risks
          * formal way to acknowledge and proceed despite the risk based organizational on opportunities and risk tolerance

        -- risk avoidance
          * appropriate risk response when the identified risk exceeds the organizational risk tolerance
          * organizations may conduct certain types of activities or employ certain types of information technologies that result in risk that is unacceptable
          * involves taking specific actions to eliminate the activities or technologies that are the basis for the risk or to revise or reposition these activities or technologies in the organizational mission/business processes to avoid the potential for unacceptable risk

        -- risk mitigation or risk reduction
          * appropriate risk response for that portion of risk that cannot be accepted, avoided, shared, or transferred
          * supply chain risk mitigations can consist of mandating that sw supplier inherits the receiver organization’s development best practices
          * at a minimum, independent verification and validation is recommended

        -- risk transference (sharing)
          * appropriate risk response when organizations desire and have the means to shift risk liability and responsibility to other organizations

    * monitor
      - changes to the supply chain can directly impact on the organization
      - through monitoring, organizations can make appropriate evaluations and assess any impact from such changes
      - subsequently, organizations may engage in a dialogue with system integrators, suppliers, and external service providers about implications and mutual obligations



  -- Cyber Supply-Chain Risk Management (C-SCRM)
    * addresses cybersecurity risks that occur through the various stages of the supply chain arising from the use of sw and services
    * efforts and maturity are unique to each business-based compliance and operational consideration
    * effective C-SCRM reduce the likelihood of supply chain compromise by enhancing organizational detection, response, and recovery from supplier system or sw component defects
    * ensures organizations understand supplier components related to critical business assets
    * when greater supplier assurance is achieved, sw that supports organizational missions can be viewed as more likely to meet mission requirements

    * implementing C-SCRM processes and controls requires investments (business resources)
    * organizations should carefully weigh C-SCRM benefits based on risk exposure from present or future supply chain implications
    * regardless, organizations are likely to have suppliers and be a supplier and cannot neglect C-SCRM
    * U.S. SECURE Technology Act (2018) highlights the importance of supply-chain security by established the Federal Acquisition Security Council (FASC)
    * FASC seeks to enhance coordination, information sharing, and risk posture in relation to supply-chain management

  -- Multitier Risk Management and C-SCRM
    * multitier (monitasoinen) risk management was discussed earlier
    * C-SCRM can be integrated seamlessly within the three levels of organizational risk management
    * C-SCRM activities are organizationally unique based on structure and mission
    * C-SCRM activities at each level should produce artifacts that relate to the production of artifacts at the other levels


  -- Cybersecurity Supply-Chain Risks
    * cybersecurity risks are inherited through suppliers, suppliers’ supply chains, and other product or service usages
    * supply chains feed business and mission outputs, but they also increase an organization’s exposure to threats and vulnerabilities through an enlarged attack surface
    * disruptions in cybersecurity supply chain CIA can create long-lasting negative effects
    * theft of sw, intellectual property, or customer data may not only disrupt business operations, but also cripple market reputation


  -- Managing Third-Party Components
    * risks associated with third-party components (TPCs) have been discussed previously
    * this topic is revisited here to highlight the importance of managing third-party components in the context of the software supply chain

    * when a new security vuln is reported for a TPC, security teams are faced with the challenge of determining whether the TPC is included in their sw
    * if included, are the organization’s sw or customer’s sw impacted by the specific vulnerability?
    * addressing the challenges associated with using third-party components requires a robust process that is integrated into the organization’s SDLC
    * management of TPCs should begin as early as possible in the SDLC
    * organizations should define and adopt a process for managing the security risk of TPCs that fits into an organization’s existing SDLC

    * to manage TPCs, an organization should:
      - maintain a list of TPCs
      - assess security risks from TPCs
      - mitigate or accept risks arising from vulnerable TPCs
      - monitor changes to ensure that the risk profile remains acceptable over time


  -- Software Bill of Materials (SBOM)
    * according to NIST, SBOM is a formal record containing the details and supply chain relationships of various components used in building sw
    * developers and vendors often create products by assembling existing open-source and commercial sw components
    * SBOM enumerates these components in a product, creating greater supplier transparency
    * there are certain minimum elements for an effective SBOM that include data fields, processes, and support for automation:

      - data fields
        -- effective SBOM is consistent and presents sw component information in a digestible format
        -- contain baseline component information for tracking
        -- this information should map easily to license and vulnerability resources
        -- data fields may include:
          * supplier name
          * supplier’s original software name
          * supplier’s version identifier
          * unique identifiers
          * dependencies (component X is included in software Y)
          * SBOM author’s name and information
          * time and date of SBOM creation

      - practices and processes
        -- SBOM standard formatting is not enough to increase transparency for TPCs
        -- SBOM must be integrated into contractual requirements in the early stage of SDLC
        -- SBOM should be integrated into the implementation phase of SDLC by examining third-party dependencies as part of the development process
        -- later in sustainment, SBOM should be maintained and used to identify and address vulns within an organization’s sw infrastructure

      - automation support
        -- automatic generation of SBOM and SBOM machine-readability are important factors that when properly considered allow for SBOM scaling and sharing across enterprises
        -- SBOM data consistency through predictable data formatting standards are crucial
        -- there are several data formats with basic compatibility between the data fields described earlier:
          * Software Package Data eXchange (SPDX)
          * CycloneDX
          * Software Identification (SWID) tags



- Security of Third-Party Software
  -- organizations frequently go to great lengths to assure their own code’s security yet make assumptions about the security of third-party sw
  -- apps developed internally often rely on open-source and third-party libraries and components
  -- security verification of these components is critical

  -- OWASP Software Component Verification Standard
    * tracing all the changes, releases, modifications, packaging, and distribution that a component has undergone (determining its pedigree) is not an easy task
    * SCVS can help identify and reduce risk in a software supply chain
    * attacks on the software supply chain can vary in complexity
    * securing code repositories and the build environment can help mitigate some risks, as can code signing
    * in contrast, using insecure communication protocols and P2P applications can introduce security risks

    * SCVS is community-driven effort to establish a framework for identifying activities, controls, and best practices, helping in identifying and reducing risk in sw supply chain
    * SCVS has the following goals:

      - develop a common taxonomy of activities, controls, and best-practices that can reduce risk in sw supply chain
      - devise (suunnitella) a path to baseline and mature sw supply-chain vigilance (valppaus)

    * SCVS provides a standardized way to assess supply chain transparency provided by contract or outsourced sw developers, based on the documentation and metadata present in the supplier's sw development workflow and/or provided with sw deliverables
    * specifically, the provision of SBOM can be requested or required by sw customers to establish supply chain visibility and differentiate suppliers

    * because of the tiered and topical structure of SCVS, it can be used for analysis of alternatives and/or used in whole or in part to evaluate proposals for procurement
    * SCVS controls at different levels can be used to qualify eligibility for procurement, or as an element of numerical scoring of proposals


  -- Interdiction (kielto) Mitigation
    * risks across the supply chain must be identified, assessed, and managed
    * one of those risks is intentional supply chain disruption
    * for example, cyberattacks on the sw supply chain networks, which would obviously be intentional, could cause a supply chain disruption

    * sw supply-chain attack
      - attacks on the sw supply chain can vary in complexity
      - simple attack could be one that targets a patch site and introduces malware files with the hope that the user downloads the malicious sw instead of a legitimate patch
      - more sophisticated attacks could aim to introduce malicious logic into the source code prior to the code being digitally signed

      - according to NIST:
        “Software supply chain attacks are particularly bothersome and insidious because they violate the basic and assumed trust between software provider and consumer. Customers have been correctly conditioned to buy and install software only from trusted sources and to download and use patches or updates only from authorized vendor sites. Now, customers must be wary of performing those basic, proper and prudent cybersecurity tasks when purchasing software and maintaining systems, since even authorized resources may be compromised. Attackers have broken this trust by surreptitiously infecting software with malware in the development and distribution process in ways virtually impossible to detect. In these instances, attackers have successfully compromised the software development cycle and inserted malware before the code has been compiled and signed – therefore, creating a package that includes malware undetectable by typical customers and anti-virus, anti-malware programs. In other instances, attackers have disrupted distribution channels by placing dummy code, updates and patches on sites that customers use to obtain software releases and, ironically, security updates."


  -- Certification and Assessment
    * organizations may opt to assess a potential supplier by requesting SBOM or other artifacts, or by conducting an analysis via SCVS or other methods
    * presented in order of rigor (kurinalaisuus), there are many acceptable means of validation that include
      - periodic reassessment such as self-attestation
      - site visits
      - third-party assessments
      - certifications

    * level of rigor should align with the criticality of the sw or service being acquired with respect to the inheriting organization
    * suppliers should be given a higher level of trust if they have been certified using third-party assessments when compared with self-attestation

    * sw developers, system engineering integrators, and service providers should embrace reuse of applicable assessment and certification artifacts to support C-SCRM when available
    * some examples might include supplier ISO 27001 certification or CMMC maturity level
    * for assessing cloud sw suppliers, the acquisition process may include auditing of Consensus Assessment Initiate Questionnaire (CAIQ) artifacts generated as part of a Cloud Controls Matrix (CCM) effort by the supplier
    * artifact reuse is efficient and aids both acquisition and supplier to realize cost savings
    * security practitioners should help organizations identify and include security assessment considerations early in the acquisition process


Checking Third-Party Libraries
Install ScanCode using PyPI. From inside Visual Studio Code, using the terminal, execute pip install scancode-toolkit
Download an abridged copy of the ScanCode samples. Located at https://github.com/avenatti/ISC2-CSSLP/tree/main/SCA/JGroups  (i.e., entire “JGroups folder”).
Run ScanCode helper function. From inside Visual Studio Code, using the terminal, execute scancode -h to display options for executing ScanCode.
Check JGroups for license considerations and output as HTML. From inside Visual Studio Code, using the terminal, navigate to the folder containing JGroups and execute scancode -l JGroups --html LicenseResults.html



- Pedigree (historia/sukupuu) and Provenance (alkuperä)
  -- “Modern software is assembled using third-party and open-source components, glued together in complex and unique ways, and integrated with original code to provide the desired functionality. A component’s pedigree (sometimes referred to as ‘provenance’) refers to the traceability of all changes (i.e., commits), releases, modifications, packaging, and distribution across the entire supply chain”
    * writes Steve Springett in the OWASP report, Component Analysis
    * in physical supply chains, this is referred to as the chain of custody
    * obtaining a component’s pedigree may involve a mixture of automation across multiple systems and suppliers, along with legal and verifiable supporting documentation
    * component's pedigree includes all supporting documentation of lineage and the attributes that make a component unique

  -- Secure Transfer
    * transferring files securely in the supply chain requires secure communication protocols
    * Transmission Control Protocol/Internet Protocol (TCP/IP) stack contains many insecure protocols at various layers, such as File Transfer Protocol (FTP) and Telnet
      - you should avoid using these protocols to transmit sensitive content
      - instead, alternatives such as Secure Shell (SSH) file transfer or Secure File Transfer Protocol (SFTP) should be used

    * alternatives to insecure protocols should provide confidentiality mechanism for otherwise plaintext transmission (encryption) and authenticating clients and servers
    * example of protocols intended for secure remote access is IPsec, which is the name for a suite of protocols that rely on public-key technology for securing communication

    * leading general purpose secure web communication protocol is TLS protocol, which is based on SSL
    * TLS is widely used to provide secure channels for confidential TCP/IP communication on the web

    * always follow best practices regarding the usage of protocols: use the latest versions and consider the identified known vulns


  -- System Sharing
    * involves multiple individuals, often with separate roles and permissions, sharing organizational infrastructure such as a computer or network printer
    * confidentiality issues may arise on shared computer systems when a user with higher privileges accesses organizational data, such as source code stored in a repository, and stores a local copy using sw such as Git
    * if the local storage is accessible by any user, then the next user to access the machine will have undue access to the organization's intellectual property
    * the same issue can arise from organizational printers storing recent print media in memory that is accessible for reprint by anyone with physical access to the device

    * file-sharing technology risks
      - file sharing involves using technology that allows internet users to share files that are housed on their individual computers
      - peer-to-peer (P2P) apps, such as those used to share music files, are some of the most common forms of file-sharing technology
      - however, P2P applications introduce security risks that may put your information or computer in jeopardy

      - work-from-home has extended the need for information sharing across more complex networking situations, which extends to transversing the internet
      - in this work setup, you are unable to print and hand-deliver a sensitive document to a coworker
      - getting information to flow securely while supporting business needs in a distributed environment is a challenging prospect

      - many ad hoc file exchange methods have major security issues
      - some fail to provide confidentiality via encryption, even though files are traversing untrusted paths on the internet
      - unsecured file exchanges are at risk of man-in-the-middle and sniffing attacks
      - storing files on unknown servers might result in undesirable data sharing, tampering, or retention

    * improving file-sharing security
      - there are many viable solutions for facilitating secure file-sharing across the internet
      - all solutions should seek to maintain organizationally defined confidentiality and integrity of files from the entire path between sender and receiver
      - below is a summary of NIST general guidance for securing file exchanges:

        -- organizations should identify user data access requirements to exchange files
          * needs should include defining senders, receivers, and sensitivity of the data
        -- after defining needs, organizations should provide training and solutions to address the identified file exchange needs
          * this will reduce the risk that users will seek ad hoc means of file sharing
        -- organizations should leverage encryption that meets organizational goals for confidentiality and integrity using well known industry encryption standards
        -- organizations should monitor file sharing for compliance and adoption
        -- organizations should ensure incident response includes sensitive data leak responses


  -- Code Repository Security
    * code repository is a file-based storage structure for sw source code and metadata pertaining to that code
    * structure can be centralized (file archive on a local server) or distributed (on a web-based repository)
    * key element of a code repository is that it enforces both access control and version control
    * access control is typically either role based or by individual identification
    * in both cases, the three core principles of confidentiality, integrity, and nonrepudiation with respect to the source code need to be enforced

    * specific operations will be derived from the code repository requirements, as will the level of access control to be performed
    * type of access control may depend on the capabilities of the code repository

      - does it support project-level roles?
      - is there a default or preferred version-control system?
      - does it support local and remote user bases?
      - does it include integrated development environment support?
      - is there support for authentication systems, such as OpenID or SSH keys?
      - does it support additional security plug-ins and tools?

    * code repository must be sufficiently secured to address the risk of unauthorized access and modification of the code base by malicious subjects, resulting from compromise of access credentials or breach of the underlying service


  -- Build Environment Security
    * historically, term “build” referred to compiling the source into binary code, assembling and packaging sw and its various dependencies, and preparing it for release and deployment
    * realistically, the “build process” may be complex and involve various tools and technologies depending on the nature of the sw
    * consistent build requires the proper build environment, comprising the right structure and elements
    * it must include not only the source-code repositories, compilers, and linkers, but also libraries, configuration files, build-automation scripts, and test suites

    * given the advancements in tools and technologies, some tools may actually be a collection of tools that provide a large array of functionality and even allow the build to be done from within developers’ integrated development environments (IDEs)

    * most organizations that engage in sw development have development, test, and production environments, so build deliverables go through development and QA before they enter production
    * organizations must ensure security and integrity of each environment

    * although it is true that it’s more efficient to automate functions such as building code, running tests, and deployment, the build and deployment tooling should also include security considerations for risks (risk of compromise in code integrity) that are relevant at this stage


  -- Cryptographically Hashed, Digitally Signed Components
    * according to the Information Technology Laboratory Bulletin article:
      “Protecting Software Integrity Through Code Signing,” “An effective and common method of protecting software is to apply a digital signature to the code. When securely implemented, digitally signing code provides both data integrity to prove that the code was not modified, and source authentication to identify who was in control of the code at the time it was signed. Verifying the signature assures the recipient that the code came from the source that signed it, and that it has not been modified in transit.”

    * security considerations for code signing solutions to protect the sw supply chain are beyond the scope of this course
    * still, it must be noted that code signing provides assurance that sw is authentic and has not been tampered with during the distribution and maintenance phases, and the verifier can validate these properties at runtime



- Supplier Security Requirements in the Acquisition Process
  -- supply chain risks can be introduced at any point in the supply chain and may be inherited by each subsequent acquirer
  -- therefore, consideration of supply chain risks should begin as early as possible in the acquisition life cycle
  -- during the life cycle of a contract, suppliers are often required to show they meet the required level of information security for services they provide

  -- Vulnerability and Incident Notification, Response, Coordination, and Reporting
    * organizations must ensure that acquired sw is reliable, resilient, recoverable, and relatively free from defects, either intentional or accidental
    * acquisitions should seek assurance artifacts from suppliers regarding vuln and incident management practices
    * organization reviewing the security practices of its vendors should look at vendor continuity planning, including incident response plans, to ensure that notification and reporting provisions meet the inheriting organization’s business requirements


  -- Security Information and Event Management (SIEM)
    * managing security operations involves the use of tools that collect information about the IT environment from many disparate sources to better examine the overall security of the organization and streamline security efforts
    * these tools are generally known as SIEM solutions

    * there is no formal industry standard that defines what a SIEM is or the functions that they perform
    * SIEM is a marketing term, used by vendors to label products and services that offer a range of capabilities for organizations looking to integrate the many different sources of security-oriented information throughout their systems
    * many different products offer these services, or similar-sounding sets of services, under a variety of names and acronyms such as “SEM” or “SIM”
    * you will need to closely read the fine print to separate the SIEM (or SEIM) required from the ones on offer in the marketplace

    * as these shorter acronyms suggest, a security event manager (SEM) provides real-time monitoring of events by correlating and analyzing data from different security sensors
    * security information managers (SIMs) provide long-term storage and management of such event data
    * SIMs were traditionally used for non-real time investigation and analysis, while SEMs were part of the real-time incident detection and response process

    * general idea of SIEM solution is to gather log data from various sources across the enterprise to better understand potential security concerns and apportion resources accordingly
    * some common functions of SIEM solutions include the following:

      - aggregation
        -- SIEM tool gathers information from across the environment
        -- this offers a centralized repository of security data and allows analysts to have a single interface with which to perform their duties
        -- SIEM might gather log data from:
          * firewalls
          * IDS/IPS systems
          * IT performance monitoring tools
          * network devices (routers/switches/gateways)
          * individual hosts/endpoints
          * anti-malware solutions

      - normalization
        -- SIEM tools can often collect different types of information from different types of sources and present the data in a meaningful, standardized way, making analysis simplified
        -- analysts can use the SIEM tool instead of repeating various log review actions on multiple systems

      - correlation
        -- SIEM may be able to mathematically assign weight and probability to various activities throughout the enterprise as a means of automatically calculating the probability that a given stream of log information is an actual attack, whether the attack affects more than host/location/network, and the likelihood and significance of input

      - secure storage
        -- because log data is enormously valuable (to organization and to attackers) for many reasons and purposes, SIEM tools often offer ability to archive data in a secure manner

      - analysis
        -- SIEM solutions perform automated analyses, using scripts and heuristics

      - reporting
        -- SIEM solutions often offer reporting tools for distilling current and historical depictions of the activity in your environment

      - real-time monitoring
        -- SIEM provides real-time monitoring and threat detection across the organization’s infrastructure, which can enable rapid responses to potential data breaches


  -- Maintenance and Support Structure
    * maintenance and support structure of sw vendor is another consideration during acquisition
    * organization must identify proper channels and methods of receiving notifications about updates and end-of-maintenance announcements for elements within the entire sw stack

    * software maintenance
      - activities involved in sw maintenance can generally be grouped into four categories
        -- corrective
        -- adaptive
        -- perfecting
        -- preventive

      - bug fixing and troubleshooting activities are considered corrective
      - modifying and updating the sw to keep it relevant are adaptive activities
      - perfecting refers to maintenance activities undertaken to keep the system functioning properly in the long term
      - preventive activities are intended to keep bugs and glitches from occurring in the future

    * community vs. commercial
      - there are advantages to both open source and commercial sw models, each with its own strengths, challenges, and trade-offs
      - the models are not exclusive and may coexist

      - in the case of commercial sw, the commercial entity often provides support, training, updates and other similar services needed by customers
      - with respect to open-source sw, it should be noted that GNU General Public License (GPL) is common, and according to the Business Software Alliance:
        “Charging fees for system setup, system management, support, maintenance, and other related services is permitted under the GPL. It is on this basis that commercial support services for Linux—which is licensed under the GPL—are offered by companies and used as one of their revenue sources.


  -- Licensing
    * sw can be sold (transfer of all control) or licensed (transfer of permission)
    * licensing terms do not change intellectual property rights; they are contractual terms

    * several types of sw licenses exist, and penalties for noncompliance can be harsh
    * commercial, shareware, freeware, and creative commons are examples of different types of licenses
    * commercial sw licensing typically consists of a master agreement and an end-user license agreement (EULA)
    * whereas the master agreement addresses the general terms, EULA addresses the specifics

    * common open-source license models include public domain, permissive, and copyleft (also known as restrictive) licenses

      - public domain licenses
        -- sw can be used and modified by anyone with no restrictions -> hence, it is the most permissive type of software license

      - permissive licenses
        -- some minimal requirements with respect to modification or redistribution
        -- apache style or BSD style are examples

      - copyleft licenses
        -- a.k.a restrictive licenses, mean that any sw created or modified under that license must be distributed under the same license terms
        -- allows redistribution (fee/free), but cannot impose any constraints further than those already enforced by parent GPL license, such as NDA or contract
        -- only restriction on the use of the GLP license is that two licensees (lisenssinhaltija) must use different names for the license
        -- GNU General Public License (GNU GPL) is an example


  -- Software Security License Considerations
    * sw supply-chain considerations vary based on the license type of sw, such as commercial off-the-shelf (COTS), government off-the-shelf (GOTS), and open-source software (OSS)
    * situations may vary even within the abovementioned licensing types
    * the example of OSS is detailed below

      - OSS is sw with the source code available to the public
      - OSS has two main inferred licensing conditions: namely, permissive and copyleft
      - both have a similar cost structure, with no cost associated with usage
      - permissive sw license includes free use with minimal requirements dictating redistribution
      - copyleft sw license also allows free use and reproduction, but on the condition that the source code and derivative sw products remain publicly available
      - this small difference may have large negative impact on organization that modifies copyleft OSS code and erroneously hopes to keep the derivative product confidential
      - alternatively, if a derivative copyleft source is procured from a supplier, the source could potentially be requested


  -- Security Track Record
    * organization cannot know everything about suppliers and their products, but it will need to know enough to measure the trustworthiness of a supplier to make informed risk decision
    * organizations should review the security track record of potential vendors

    * to evaluate third-party providers, organizations may rely on various methods including:

      - proprietary and internally developed security due diligence questionnaires
      - standardized assessment tools and reports (Standardized Information Gathering (SIG) questionnaire)
      - independent third-party audit reports (SOC 2, Type 2 audit report)
      - combination of all of the above
      - the use of other methods would depend on the nature of the services provided by third parties


  -- Right to Audit
    * audit can help an organization understand the capabilities and policies of its supporting vendors
    * the right to audit is a key control that can be included in a vendor contract
    * including a right-to-audit clause in a contract does not mean the organization must perform an audit; it merely reserves the right to perform an audit if needed
    * right to audit is not solely for high-risk relationships
      - low-risk proposition can quickly turn into high-risk one as the nature of the relationship changes and a vendor starts offering different types of services for the organization

    * responsibility models
      - responsibility is an important consideration when establishing or auditing a supply chain
      - contractual obligations between acquisitions and suppliers will determine responsibility for compliance and security
      - contractual agreements will also determine a supplier’s responsibility to provide security artifacts such as SBOM documentation
      - additionally, explicit contract language can also empower an organization to audit suppliers for security compliance

      - with most cloud providers, there is a concept of shared responsibility
      - shared model can lower the operational security burden for cloud service providers (CSP) and customers
      - security practitioners should carefully review the cloud services selected by an organization to understand services, IT integration, and compliance
      - in a shared model, security practitioners need to understand the dividing line between vendor inherited security, shared security, and customer security considerations



- Contractual Requirements
  -- ownership of intellectual properties, code escrow, liabilities and warranties, and service-level agreements must be addressed when acquiring sw

  -- OWASP Secure Software Contract Annex
    * OWASP Application Security Verification Standard (ASVS) provides a basis for specifying app security verification requirements in contracts
    * OWASP provides a secure sw contract annex, which is intended to frame negotiation points for sw suppliers and clients
    * it captures important contractual terms and conditions related to the security of the sw to be developed or delivered

    * OWASP Secure Software Contract Annex clarifies the security-related rights and obligations of all the parties to sw development relationship
    * the annex has distinct sections for addressing various relevant topics, including:

      - life cycle activities
      - security requirement areas
      - development environment
      - libraries, frameworks and products
      - security reviews
      - security issue management

    * as part of the acquisition process, parties’ requirements and obligations that may be captured and addressed through contracts can include
      - intellectual property (IP) ownership
      - code escrow
      - liability
      - warranty
      - service-level agreements
      - licensing


  -- Intellectual Property (IP) Ownership
    * when development of sw is outsourced, intellectual property (IP) rights and ownership are often addressed using contract language
    * various ownership options may be available depending on the outsourcing arrangements and the objectives
    * possibilities include sole ownership by the sw developer, sole ownership by the client, or other arrangements that meet the parties’ requirements

    * ownership of IP is perhaps the first of two critical concerns in offshore outsourcing
    * whether the outsourced work is expected to take place domestically or outside the enterprises’ national borders, it is essential to identify, account for and clarify ownership related issues of IP assets improved or created during the relationship
    * more often than not, many companies overlook or pay inadequate attention to this very important aspect

    * disclosure of confidential information and trade secrets is another concern
    * if the ability of the partner to safeguard confidential information of commercial value cannot be ensured, the risks associated with offshore outsourcing may outweigh the perceived benefits of such arrangements

    * code escrow
      - there are usually three parties involved in a sw escrow, or source code escrow, which include:
        -- acquirer or licensee that is purchasing the sw
        -- publisher or licensor that develops the sw
        -- neutral third party that holds the copies of source code as insurance

      - part of acquisition of sw, acquirer and publisher (developer) can agree that a copy of the source code (+ other relevant items) be given to a neutral third party: an escrow agent
      - the agreement would then specify under which conditions that agent would be allowed to release the source code
      - for example, one of the conditions might be the publisher’s bankruptcy or another reason for going out of business
      - primarily initiated by the licensee for protection and to ensure maintenance and business continuity would be possible in the event of bankruptcy or breach of licensor
      - the terms of the escrow should also address when and how the items in the escrow are updated by the sw publisher (developer)

    * liability
      - outsourcing sw development is commonplace
      - when an organization outsources its sw development, it is imperative that there be protection mechanisms in place
      - it is critically important and necessary that contracts are there to cover any liability arising from the outsourcing provider
      - if possible, the outsourced functions should be treated as a supplier relationship and therefore require security to be integrated into the product life cycle
      - assurances should be demonstrated prior to acceptance of the product from the supplier

    * warranty
      - as part of negotiating sw contracts, certain obligations of the sw developer or licensor may be captured as part of the warranty
      - terms and conditions of the warranty may include:

        -- the exact nature of the items that the developer or licensor warrants
        -- remedies available to the sw acquirer or licensee in case of any breach of the warranty
        -- the scope and duration of the warranty



  -- Service-Level Agreements (SLAs)
    * NIST defines SLAs as a commitment between service provider and one or more customers that addresses specific aspects of the service, such as responsibilities, details on the type of service, expected performance level, and requirements for reporting, resolution, and termination

    * supplier trustworthiness
      - one indicator of sw product’s likely trustworthiness is that of its supplier
      - unfortunately, for commercial and open-source software, it is almost impossible for a buyer to know the traceability of the product’s chain of custody (provenance) throughout its life cycle with any meaningful level of confidence
      - the nature of the global sw industry and its open-source counterpart is that sw supply chains are complex and convoluted, sometimes to the extent that original vendors and developers cannot be certain of all companies, individuals, or countries involved in the production, staging, or delivery of their sw

      - when considering acquisition from a new commercial or open-source supplier, always assume that the supplier is potentially malicious until an acceptable level of confidence in the supplier’s trustworthiness can be attained

      - to be acceptable, the level of confidence needs to be commensurate (oikeassa suhteessa) with the required level of confidence in the sw itself, given criticality of the system, sensitivity of the data it will handle, and exposure of its environment to potential threats
      - for example, a higher level of confidence is needed for a supplier of critical sw than for a supplier of general-purpose (business, office) sw



  -- Agreements Between Customers and Service Providers
    * think of a service-level agreement (SLA) as a legally binding agreement between a customer and a service provider
    * SLA defines the agreed-upon level of performance and compensation or penalty between the provider and the customer
    * having an SLA defined and in place does not mean that the provider will always be in compliance with the SLA
    * the customer can gain assurance through reviews, assessments, and monitoring
    * for instance, customer should ask to review the security docs of 3rd-party system and then perform assessment to determine compliance with specific control framework or regulation
    * the assessment gives customers an opportunity to view the providers as they really are
    * customers should ensure the assessment or inspection covers the information security and privacy areas that are deemed most important

    * to clarify goals and assure that service providers and service consumers are on the same page, SLOs, SLRs, and even SLIs may be established
    * these terms are related, and a brief discussion of a few is presented below

      - service-level objective (SLO)
        -- within the SLA, SLO may be used to specify metrics that reflect a promise made to a customer (uptime, response time)
        -- SLOs set customer expectations, and they require the service provider to live up to them
        -- simplicity and clarity are important when specifying SLOs

      - service-level requirement (SLR)
        -- contains the requirements for a service from the client viewpoint
        -- it defines:
          * detailed service-level targets
          * mutual responsibilities
          * other requirements specific to a certain group of customers

        -- SLR document is used to create the SLA that defines:
          * type of service provided and service-level targets
          * metrics for monitoring the process and service-level reporting
          * responsibilities of both the customer and provider
          * steps for reporting issues with the service
          * response and issue resolution time frame
          * penalties for noncompliance with requirements

      - service-level reports
        -- give insight into a service provider’s ability to deliver the agreed-upon service quality
        -- compares agreed-to and actually achieved service levels, and includes information on the usage of services, ongoing measures for service improvement, and any exceptional events




Cloud Controls Matrix (CCM) - A tool for the systematic assessment of a cloud implementation that provides guidance on which security controls should be implemented by which actor within the cloud supply chain. Source: https://cloudsecurityalliance.org/research/cloud-controls-matrix/
Commercial Off-the-Shelf (COTS): A reference to pre-packaged software applications that are designed and developed for general use by individual consumers or businesses.
Consensus Assessment Initiate Questionnaire (CAIQ) - Formerly a separate tool for assessment of cloud implementation, this is now part of the Cloud Controls Matrix (CCM).
Copyleft - A term used to describe a type of licensing arrangement for software and creative works that aims to ensure the continued freedom of use, modification, and distribution with the condition that any derived materials have the same conditions for use.
Cybersecurity Supply-Chain Risk Management (C-SCRM) - A risk management strategy that addresses cybersecurity risks that occur through the various stages of the supply chain arising from the use of software and services.
ISO/IEC 27001 - The best-known standard for information security management systems. Provides guidance for establishing, implementing, maintaining, and continually improving an information security management system. Source: https://www.iso.org/standard/27001
Open-Source Software (OSS) - A reference to software that is in contrast to proprietary software in terms of the availability of its source code to users. It also provides the opportunity to copy, modify and distribute derivatives of the original program.
Provenance - A reference to the origin and license of third-party components included and used in the software.
Risk Acceptance - Determining that the potential benefits of a business function outweigh the possible risk impact/likelihood and performing that business function with no other action.
Risk Avoidance - Determining that the impact and/or likelihood of a specific risk is too great to be offset by the potential benefits and not performing a certain business function because of that determination.
Risk Mitigation - Putting security controls in place to attenuate the possible impact and/or likelihood of a specific risk.
Risk Transference (Sharing) - Paying an external party to accept the financial impact of a given risk.
Software Bill of Materials (SBOM) - A formal record containing the details and supply chain relationships of various components used in building software. Source: EO 14028 Section 10(j)
Third-Party Components - A reusable system or software component developed to be distributed by a supplier for integration into an acquisitor system.




Question 1    1 / 1 point
Which of the following most adequately describes supply chain concerns? (D8.1, L8.1)

Question options:

--> A) Organizations now rely on other organizations to provide critical products and services, yet they often lack visibility into their supply ecosystems.
B) Organizations are buying various IT products from too many suppliers, and interoperability challenges require enormous effort on the part of the organization.
C) Organizations rely significantly on a handful of key suppliers that lack contingency plans for the continuity of their operations.
D) Direct attacks on the supply chain may be successful in introducing malware on patch sites, where these patches are downloaded and applied by organizations.

Correct. Choice A describes the essence and the root cause of many concerns with the supply chain, which is the lack of visibility into the supply ecosystem.


Question 2    0 / 1 point
Why should supply chain risks be mitigated as early as possible in the acquisition life cycle? (D8.1, L8.1)

Question options:

A) To ensure software products function as intended and are reliable, safe, and secure.
B) To define formal relationships and execute contracts between participating organizations.
--> C) To prevent the insertion of malicious code into software products.
D) To increase the probability of acquiring software products from trusted suppliers.

Option A is incorrect as it is a general statement about software quality but does not specifically address supply chain risks.


Question 3    1 / 1 point
Which risk management activity must be conducted before an organization can adequately respond to software supply chain risk? (D8.1, L8.2)

Question options:

A) Monitor risks
B) Provide a means for continuous feedback
--> C) Assess risk
D) None of these

Correct. Once the organization is scoped, the risk must be assessed before it can be ameliorated.


Question 4    1 / 1 point
Lakeisha works as a security analyst at a medium-sized business. She has identified a software application risk which, if realized, will result in a one-time loss of $500,000 in U.S. currency. There is no insurance available to cover the risk because the application is very old. In fact, updating the application will require a complete rewrite at an anticipated cost of $5 million. The application makes the business $2 million annually. What course of action is the business likely to take? (D8.1, L8.2)

Question options:

A) Ignore the risk
B) Avoid the risk
--> C) Accept the risk
D) Transfer the risk

Correct. Lakeisha and the organization are likely to accept the risk of losing $500,000 to make $2 million.


Question 5    1 / 1 point
Why is managing third-party components (TPCs) important in the context of the software supply chain? (D8.1, L8.2)

Question options:

A) To prevent disruptions in cybersecurity supply chain CIA.
B) To enhance the organization's software development life cycle (SDLC).
C) To ensure compliance with the organization's existing SDLC.
--> D) To minimize the risk of cyber threats and vulnerabilities.

Correct. Managing third-party components (TPCs) in the software supply chain is important to minimize the risk of cyber threats and vulnerabilities.


Question 6    1 / 1 point
Which of the following is LEAST relevant to supply chain confidence? (D8.2, L8.3)

Question options:

--> A) Gated check-in
B) Software Bill of Materials (SBOM)
C) OWASP Software Component Verification Standard (SCVS)
D) Contract Language

Correct. Gated check-ins are generally associated with in-house development efforts more than ensuring trustworthy supplier source.


Question 7    0 / 1 point
Which of the following strategies is likely to reduce potential exposure and control risk for the software supply chain? (D8.2, L8.3)

Question options:

--> A) Selecting vendors based on vulnerability scan results
B) Selecting multiple vendors
C) Selecting as few vendors as possible
D) Selecting vendors based on mission

Selecting multiple vendors may increase redundancy and strengthen the supply chain, but it does little to limit exposure. C and D are not correlated to limiting the attack surface introduced by the software supply chain.


Question 8    1 / 1 point
Which of the following security risks are commonly associated with peer-to-peer (P2P) applications? (D8.3, L8.4)

Question options:

--> A) Malicious code exposure
B) OSS license exposure
C) SQL injections
D) Broken authentication

Correct. Malware infection and disclosure of sensitive information are among the common risks associated with P2P applications.


Question 9    1 / 1 point
Which of the following terms BEST describes tracing the history of software supply chain? (D8.3, L8.4)

Question options:

A) Software artifacts
--> B) Software provenance (alkuperä)
C) Security attestation (todistaminen, vahvistaminen)
D) Software security risk

Correct. Software provenance and pedigree refer to the history of a software application including both development and distribution. Review key terms as required.


Question 10   1 / 1 point
Why is it important to ensure security and integrity in each environment of the software development life cycle? (D8.3, L8.5)

Question options:

A) To automate functions such as building code, running tests, and deployment.
B) To enforce access control and version control in the code repository.
--> C) To prevent unauthorized access and modification of the code base.
D) To manage the risk of compromise in code integrity during the build process.

Correct. It is important to ensure security and integrity in each environment of the software development lifecycle to prevent unauthorized access and modification of the code base.


Question 11   1 / 1 point
An effective and common method of protecting software is to apply a digital signature to software. When implemented correctly, digitally signing code accomplishes which security goal? (D8.3, L8.6)

Question options:

A) Provides for the confidentiality and integrity of the code
B) Provides for the confidentiality of the code
C) Provides for the integrity of the code
--> D) Provides for the origin and integrity of the code

Correct. Code signing is used to guarantee authenticity and integrity of software applications by the creator. Code signing is not associated with confidentiality mechanisms.


Question 12   1 / 1 point
Why do many organizations purchase systems that are more vulnerable than they should be when acquiring commercial off-the-shelf (COTS) software? (D8.4, L8.7)

Question options:

A) Lack of cost-effectiveness
B) Inadequate features
--> C) Security being an afterthought
D) Scalability and performance concerns

Correct. When acquiring COTS software, organizations often prioritize cost and features, neglecting security considerations. This can lead to purchasing systems that are more vulnerable than they should be.


Question 13   0 / 1 point
What type of audit may include the direct testing of security controls? (D8.4, L8.8)

Question options:

--> A) Software quality assurance audit
B) Software compliance audit
C) Software licensing audit
D) Security policy audit

Options B, C, and D are incorrect as they refer to different types of audits that focus on compliance, licensing, and security policies, respectively.


Question 14   1 / 1 point
Which of the following provides the GREATEST guarantee for safeguarding software intellectual property? (D8.4, L8.8)

Question options:

A) Leveraging copyleft source code repositories
B) Outsourcing the development and negotiating the intellectual property ownership
C) Obtaining shareware or freeware software license
--> D) Developing the software in-house

Correct. Outsourcing the development to a third party and negotiating the IP rights may result in adequate protections, but the strongest answer is to conduct in-house development activities. In practice, some organizations may outsource some layers of development such as the user interface while keeping more sensitive layers such as integration and business logic in-house.


Question 15   1 / 1 point
Which of the following are security controls in software acquisitions? (D8.4, L8.8)

Question options:

A) Establishing security track record of the suppliers
B) Tracing the pedigree of software products
C) Requiring assurance cases from software suppliers
--> D) All of the above

Correct. Establishing sufficient basis for trusting suppliers, tracing the pedigree of software products, and requiring assurance cases from software suppliers are all control considerations during software acquisition.


Question 16   1 / 1 point
Which of the following is true of "right-to-audit" provisions in contracts? (D8.4, L8.8)

Question options:

A) Should only be a consideration for high-risk vendors and suppliers.
B) Only for vendors and suppliers that are not ISO 27001 certified.
C) Forfeit the customer's right to conduct audits. Suppliers self-audit if needed.
--> D) None of these

Correct. The right to audit is a key control that can be included in a contract. Including a right to audit clause in a contract does not require the organization to perform the audit; it merely reserves the right to perform the audit if needed. It is also important to understand that the right to audit is not strictly for high-risk relationships, and that a low-risk proposition can quickly turn into a high-risk one as the nature of the relationship changes and a vendor starts offering different types of services for the organization.


Question 17   1 / 1 point
What are the categories of activities involved in software maintenance? (D8.4, D8.8)

Question options:

A) Corrective, preventive, commercial, and adaptive
B) Bug fixing, modifying, perfecting, and compliance
--> C) Corrective, adaptive, perfecting, and preventive
D) Troubleshooting, updating, commercial, and preventive

Correct. The activities involved in software maintenance can be grouped into four categories: corrective (bug fixing and troubleshooting), adaptive (modifying and updating to keep it relevant), perfecting (keeping the system functioning properly over the long term), and preventive (keeping bugs and glitches from occurring in the future).


Question 18   0 / 1 point
What does a copyleft license require for software distribution? (D8.4, D8.8)

Question options:

A)  The software must be distributed for a fee.
--> B) The software must remain publicly available.
C) The software can only be used for non-commercial purposes.
D) The software cannot be modified or redistributed.

A copyleft license allows for redistribution, either for a fee or free of charge, but the source code and derivative software products must remain publicly available.


Question 19   1 / 1 point
Which of the following best describes a service-level agreement (SLA)? (D8.5, L8.9)

Question options:

A) A definition of responsibilities for the service provider to adhere to.
B) An authentication protocol put in place for accessing web services.
C) A handshake agreement that is not legally binding.
--> D) A commitment between a service provider and the customer that defines the agreed-upon level of performance and compensation or penalty.

Correct. An SLA is a commitment between a service provider and the customer that defines the agreed-upon level of performance and compensation or penalty.


Question 20   1 / 1 point
What is one of the purposes of the OWASP Secure Software Contract Annex? (D8.5, L8.9)

Question options:

A) To outline the ownership options for intellectual property (IP) rights in software development outsourcing.
B) To establish liability coverage for potential risks associated with offshore outsourcing.
C) To define the terms and conditions of code escrow agreements in software acquisition.
--> D) To clarify the security-related rights and obligations of parties involved in software development contracts.

Correct. The purpose of the OWASP Secure Software Contract Annex is to clarify the security-related rights and obligations of parties involved in software development contracts.






Course Objectives
-----------------
Define core security objectives for software development.
Describe the information security triad and explain the main mechanisms of confidentiality, integrity, and availability of information.
Characterize the relationship between information security and data privacy.
Identify regulatory considerations that impact software security.
Explain how security methods mitigate vulnerabilitiesthrough access controls.
Describe the purpose and function of multiple layers of protection in software security.
Describe how security culture and practices impact data privacy and security.
Explore security in predictive and adaptive methodologies for software development.
Describe the incorporation of software security practices into the SDLC processes.
Identify the DevOps and DevSecOps.
Recognize security configuration standards and benchmarks.
Describe security-focused configuration management processes.
Determine security standards for software weaknesses and vulnerabilities.
Explain OWASP’s Software Assurance Maturity Model (OpenSAMM) and Building Security in Maturity Model (BSIMM).
Define Software Security Milestones and Checkpoints within a DevSecOps Pipeline.
Interpret the System Security Plan.
Recognize security-relevant documentation.
Identify the metrics in software development.
Elaborate software decommissioning policy and processes.
Explain security reporting mechanisms within DevSecOps.
Describe risk assessment and risk management.
Review the implantation of secure operation processes.
Identify security considerations within the container life cycle.
Describe requirements management.
Recognize functional and nonfunctional requirements.
Explain the impact of security-focused stories in SCRUM/SCRUM-like methodologies.
Describe sources for software security requirements.
Analyze security policies and their supporting elements as internal sources for security requirements.
Explain compliance requirements and recognize laws, regulations and industry standards as external sources for security requirements.
Discuss security standards and frameworks.
Describe data governance and ownership.
Describe data classification and security labeling and marking.
Recognize structured and unstructured data types. 
Describe the data life cycle.
Identify privacy laws and regulations that aim to mitigate privacy risk.
Discuss data anonymization and enumerate various approaches for anonymization.
Explain user consent, data retention, and data disposition in the context of privacy.
Recognize implications of cross-border data transfer and restrictions for the transfer of personal data.
Describe user and software data access provisions.
Describe misuse and abuse cases and their relevance to known attack patterns.
Describe Security Requirements Traceability Matrix (STRM).
Identify third-party vendor security requirements.
Describe architecture and security-relevant design patterns.
Recognize the security criteria of interfaces design.
Compare and distinguish various authentication and authorization mechanisms.
Describe credential management.
Identify the principles and tools used in network security.
Describe the methods used to maintain database security.
Identify the threat modeling process, tools, and methodologies.
Indicate the process of attack surface evaluation and management.
Discuss threat intelligence and sources for cyberthreat information.
Describe the architecture risk assessment process.
Recognize nonfunctional security properties and constraints.
Identify secure operational architecture considerations.
Identify the features of secure coding standards.
Describe different approaches for implementing security in managed applications. 
Identify common flaws in software and corresponding mitigation strategies.
Explain common secure coding practices.
Identify methods of data protection in transit and at rest.
Identify software weaknesses listed in the most common vulnerability lists and databases.
Describe the function and methods of software assurance tools.
Describe categorization of controls by type and by functionality.
Identify controls to prevent common web application vulnerabilities.
Describe the process for defining a security strategy that includes considerations for software risks.
Identify the risks associated with using third-party and open-source components and libraries.
Describe the various integration categories and their relevance to software security.
Describe the build automation process.
Explain the techniques used in software assurance.
Identify common security testing techniques.   
Describe the testing environment.  
Identify organization's software security standards and guidelines.  
Explain the benefits of the crowdsourced security and bug bounty program.
Identify the guidelines for security testing.
Identify various security test cases.
Recognize the importance of creating misuse and abuse cases.
Describe documentation verification and validation. 
Explain the structure and goals of OWASP’s Application Security Verification Standard (ASVS).
Describe undocumented functionality and source code.
Explain the security implications of test results. 
Differentiate the types of test results.
Describe the process of tracking security defects. 
Explain risk scoring systems and the Common Vulnerability Scoring System (CVSS).
Explain the generation of test data and ramifications of using production data.
Describe the process of verification and validation testing.
Describe operational risk analysis in the context of ISO 31000 series.
Describe Security Configuration Management (SecCM).
Identify the elements of the Secure Continuous Integration and Continuous Delivery (CI/CD) pipeline.
Explain the application security toolchain.
Identify the steps to identify app vulnerabilities.
Compare and distinguish the common methods to store and manage security data.
Describe secure installation processes and methods.
Explain secure software activation mechanisms.
Identify the steps and methods in the Authorization to Operate (ATO) process.
Explain how to perform information security continuous monitoring.
Describe the phases involved in an incident response plan.
Relate the patch management process to overall software security practices.
Describe the methods and tools used for vulnerability management.
Describe how controls contribute to protecting applications during runtime.
Compare and distinguish how business continuity and disaster recovery plans support continuity of operations.
Explain and differentiate service-level agreements (SLA), service-level objectives (SLO), and service-level indicators (SLI) in order to achieve supplier continuity.
Describe the software supply chain.
Review the process of software supply-chain risk management.
Explain security risks associated with third-party software.
Associate the risks with peer-to-peer applications and file sharing.
Interpret code repository and environment security.
Express cryptographically hashed, digitally signed components.
Identify security requirements and principlesin software acquisition.
Elaborate critical considerations for software acquisition.
Explain contractual requirements for software acquisition.
